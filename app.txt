This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  tjs/
    backends/
      onnx.js
    ops/
      registry.js
    utils/
      audio.js
      constants.js
      core.js
      data-structures.js
      devices.js
      dtypes.js
      generic.js
      hub.js
      image.js
      maths.js
      tensor.js
      video.js
    env.js
  App.jsx
  audio-utils.js
  f5-tts.js
  index.css
  main.jsx
index.html
package.json
README.md
vite.config.js
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/tjs/backends/onnx.js">
  1: /**
  2:  * @file Handler file for choosing the correct version of ONNX Runtime, based on the environment.
  3:  * Ideally, we could import the `onnxruntime-web` and `onnxruntime-node` packages only when needed,
  4:  * but dynamic imports don't seem to work with the current webpack version and/or configuration.
  5:  * This is possibly due to the experimental nature of top-level await statements.
  6:  * So, we just import both packages, and use the appropriate one based on the environment:
  7:  *   - When running in node, we use `onnxruntime-node`.
  8:  *   - When running in the browser, we use `onnxruntime-web` (`onnxruntime-node` is not bundled).
  9:  * 
 10:  * This module is not directly exported, but can be accessed through the environment variables:
 11:  * ```javascript
 12:  * import { env } from '@huggingface/transformers';
 13:  * console.log(env.backends.onnx);
 14:  * ```
 15:  * 
 16:  * @module backends/onnx
 17:  */
 18: 
 19: import { env, apis } from '../env.js';
 20: 
 21: // NOTE: Import order matters here. We need to import `onnxruntime-node` before `onnxruntime-web`.
 22: // In either case, we select the default export if it exists, otherwise we use the named export.
 23: import * as ONNX_NODE from 'onnxruntime-node';
 24: import * as ONNX_WEB from 'onnxruntime-web';
 25: 
 26: export { Tensor } from 'onnxruntime-common';
 27: 
 28: /**
 29:  * @typedef {import('onnxruntime-common').InferenceSession.ExecutionProviderConfig} ONNXExecutionProviders
 30:  */
 31: 
 32: /** @type {Record<import("../utils/devices.js").DeviceType, ONNXExecutionProviders>} */
 33: const DEVICE_TO_EXECUTION_PROVIDER_MAPPING = Object.freeze({
 34:     auto: null, // Auto-detect based on device and environment
 35:     gpu: null, // Auto-detect GPU
 36:     cpu: 'cpu', // CPU
 37:     wasm: 'wasm', // WebAssembly
 38:     webgpu: 'webgpu', // WebGPU
 39:     cuda: 'cuda', // CUDA
 40:     dml: 'dml', // DirectML
 41: 
 42:     webnn: { name: 'webnn', deviceType: 'cpu' }, // WebNN (default)
 43:     'webnn-npu': { name: 'webnn', deviceType: 'npu' }, // WebNN NPU
 44:     'webnn-gpu': { name: 'webnn', deviceType: 'gpu' }, // WebNN GPU
 45:     'webnn-cpu': { name: 'webnn', deviceType: 'cpu' }, // WebNN CPU
 46: });
 47: 
 48: /** 
 49:  * The list of supported devices, sorted by priority/performance.
 50:  * @type {import("../utils/devices.js").DeviceType[]}
 51:  */
 52: const supportedDevices = [];
 53: 
 54: /** @type {ONNXExecutionProviders[]} */
 55: let defaultDevices;
 56: let ONNX;
 57: const ORT_SYMBOL = Symbol.for('onnxruntime');
 58: 
 59: if (ORT_SYMBOL in globalThis) {
 60:     // If the JS runtime exposes their own ONNX runtime, use it
 61:     ONNX = globalThis[ORT_SYMBOL];
 62: 
 63: } else if (apis.IS_NODE_ENV) {
 64:     ONNX = ONNX_NODE.default ?? ONNX_NODE;
 65: 
 66:     // Updated as of ONNX Runtime 1.20.1
 67:     // The following table lists the supported versions of ONNX Runtime Node.js binding provided with pre-built binaries.
 68:     // | EPs/Platforms | Windows x64 | Windows arm64 | Linux x64         | Linux arm64 | MacOS x64 | MacOS arm64 |
 69:     // | ------------- | ----------- | ------------- | ----------------- | ----------- | --------- | ----------- |
 70:     // | CPU           | ✔️          | ✔️            | ✔️                | ✔️          | ✔️        | ✔️          |
 71:     // | DirectML      | ✔️          | ✔️            | ❌                | ❌          | ❌        | ❌          |
 72:     // | CUDA          | ❌          | ❌            | ✔️ (CUDA v11.8)   | ❌          | ❌        | ❌          |
 73:     switch (process.platform) {
 74:         case 'win32': // Windows x64 and Windows arm64
 75:             supportedDevices.push('dml');
 76:             break;
 77:         case 'linux': // Linux x64 and Linux arm64
 78:             if (process.arch === 'x64') {
 79:                 supportedDevices.push('cuda');
 80:             }
 81:             break;
 82:         case 'darwin': // MacOS x64 and MacOS arm64
 83:             break;
 84:     }
 85: 
 86:     supportedDevices.push('cpu');
 87:     defaultDevices = ['cpu'];
 88: } else {
 89:     ONNX = ONNX_WEB;
 90: 
 91:     if (apis.IS_WEBNN_AVAILABLE) {
 92:         // TODO: Only push supported providers (depending on available hardware)
 93:         supportedDevices.push('webnn-npu', 'webnn-gpu', 'webnn-cpu', 'webnn');
 94:     }
 95: 
 96:     if (apis.IS_WEBGPU_AVAILABLE) {
 97:         supportedDevices.push('webgpu');
 98:     }
 99: 
100:     supportedDevices.push('wasm');
101:     defaultDevices = ['wasm'];
102: }
103: 
104: // @ts-ignore
105: const InferenceSession = ONNX.InferenceSession;
106: 
107: /**
108:  * Map a device to the execution providers to use for the given device.
109:  * @param {import("../utils/devices.js").DeviceType|"auto"|null} [device=null] (Optional) The device to run the inference on.
110:  * @returns {ONNXExecutionProviders[]} The execution providers to use for the given device.
111:  */
112: export function deviceToExecutionProviders(device = null) {
113:     // Use the default execution providers if the user hasn't specified anything
114:     if (!device) return defaultDevices;
115: 
116:     // Handle overloaded cases
117:     switch (device) {
118:         case "auto":
119:             return supportedDevices;
120:         case "gpu":
121:             return supportedDevices.filter(x =>
122:                 ["webgpu", "cuda", "dml", "webnn-gpu"].includes(x),
123:             );
124:     }
125: 
126:     if (supportedDevices.includes(device)) {
127:         return [DEVICE_TO_EXECUTION_PROVIDER_MAPPING[device] ?? device];
128:     }
129: 
130:     throw new Error(`Unsupported device: "${device}". Should be one of: ${supportedDevices.join(', ')}.`)
131: }
132: 
133: 
134: /**
135:  * To prevent multiple calls to `initWasm()`, we store the first call in a Promise
136:  * that is resolved when the first InferenceSession is created. Subsequent calls
137:  * will wait for this Promise to resolve before creating their own InferenceSession.
138:  * @type {Promise<any>|null}
139:  */
140: let wasmInitPromise = null;
141: 
142: /**
143:  * Create an ONNX inference session.
144:  * @param {Uint8Array|string} buffer_or_path The ONNX model buffer or path.
145:  * @param {import('onnxruntime-common').InferenceSession.SessionOptions} session_options ONNX inference session options.
146:  * @param {Object} session_config ONNX inference session configuration.
147:  * @returns {Promise<import('onnxruntime-common').InferenceSession & { config: Object}>} The ONNX inference session.
148:  */
149: export async function createInferenceSession(buffer_or_path, session_options, session_config) {
150:     if (wasmInitPromise) {
151:         // A previous session has already initialized the WASM runtime
152:         // so we wait for it to resolve before creating this new session.
153:         await wasmInitPromise;
154:     }
155: 
156:     const sessionPromise = InferenceSession.create(buffer_or_path, session_options);
157:     wasmInitPromise ??= sessionPromise;
158:     const session = await sessionPromise;
159:     session.config = session_config;
160:     return session;
161: }
162: 
163: /**
164:  * Check if an object is an ONNX tensor.
165:  * @param {any} x The object to check
166:  * @returns {boolean} Whether the object is an ONNX tensor.
167:  */
168: export function isONNXTensor(x) {
169:     return x instanceof ONNX.Tensor;
170: }
171: 
172: /** @type {import('onnxruntime-common').Env} */
173: // @ts-ignore
174: const ONNX_ENV = ONNX?.env;
175: if (ONNX_ENV?.wasm) {
176:     // Initialize wasm backend with suitable default settings.
177: 
178:     // (Optional) Set path to wasm files. This will override the default path search behavior of onnxruntime-web.
179:     // By default, we only do this if we are not in a service worker and the wasmPaths are not already set.
180:     if (
181:         // @ts-ignore Cannot find name 'ServiceWorkerGlobalScope'.ts(2304)
182:         !(typeof ServiceWorkerGlobalScope !== 'undefined' && self instanceof ServiceWorkerGlobalScope)
183:         && !ONNX_ENV.wasm.wasmPaths
184:     ) {
185:         ONNX_ENV.wasm.wasmPaths = `https://cdn.jsdelivr.net/npm/@huggingface/transformers@${env.version}/dist/`;
186:     }
187: 
188:     // TODO: Add support for loading WASM files from cached buffer when we upgrade to onnxruntime-web@1.19.0
189:     // https://github.com/microsoft/onnxruntime/pull/21534
190: 
191:     // Users may wish to proxy the WASM backend to prevent the UI from freezing,
192:     // However, this is not necessary when using WebGPU, so we default to false.
193:     ONNX_ENV.wasm.proxy = false;
194: }
195: 
196: if (ONNX_ENV?.webgpu) {
197:     ONNX_ENV.webgpu.powerPreference = 'high-performance';
198: }
199: 
200: /**
201:  * Check if ONNX's WASM backend is being proxied.
202:  * @returns {boolean} Whether ONNX's WASM backend is being proxied.
203:  */
204: export function isONNXProxy() {
205:     // TODO: Update this when allowing non-WASM backends.
206:     return ONNX_ENV?.wasm?.proxy;
207: }
208: 
209: // Expose ONNX environment variables to `env.backends.onnx`
210: env.backends.onnx = ONNX_ENV;
</file>

<file path="src/tjs/ops/registry.js">
  1: import { createInferenceSession, isONNXProxy } from "../backends/onnx.js";
  2: import { Tensor } from "../utils/tensor.js";
  3: import { apis } from "../env.js";
  4: 
  5: const IS_WEB_ENV = apis.IS_BROWSER_ENV || apis.IS_WEBWORKER_ENV;
  6: /**
  7:  * Asynchronously creates a wrapper function for running an ONNX inference session.
  8:  *
  9:  * @param {number[]} session_bytes The session data in bytes.
 10:  * @param {import('onnxruntime-common').InferenceSession.SessionOptions} session_options The options for the ONNX session.
 11:  * @template {string | [string] | string[]} T
 12:  * @param {T} names The name(s) of the output tensor(s).
 13:  * 
 14:  * @returns {Promise<function(Record<string, Tensor>): Promise<T extends string ? Tensor : T extends string[] ? { [K in keyof T]: Tensor } : never>>}
 15:  * The wrapper function for running the ONNX inference session.
 16:  */
 17: const wrap = async (session_bytes, session_options, names) => {
 18:     const session = await createInferenceSession(
 19:         new Uint8Array(session_bytes), session_options,
 20:     );
 21: 
 22:     /** @type {Promise<any>} */
 23:     let chain = Promise.resolve();
 24: 
 25:     return /** @type {any} */(async (/** @type {Record<string, Tensor>} */ inputs) => {
 26:         const proxied = isONNXProxy();
 27:         const ortFeed = Object.fromEntries(Object.entries(inputs).map(([k, v]) => [k, (proxied ? v.clone() : v).ort_tensor]));
 28: 
 29:         // When running in-browser via WASM, we need to chain calls to session.run to avoid "Error: Session already started"
 30:         const outputs = await (chain = IS_WEB_ENV ? chain.then(() => session.run(ortFeed)) : session.run(ortFeed));
 31: 
 32:         if (Array.isArray(names)) {
 33:             return names.map((n) => new Tensor(outputs[n]));
 34:         } else {
 35:             return new Tensor(outputs[/** @type {string} */(names)]);
 36:         }
 37:     })
 38: }
 39: 
 40: // In-memory registry of initialized ONNX operators
 41: export class TensorOpRegistry {
 42:     static session_options = {
 43:         // TODO: Allow for multiple execution providers
 44:         // executionProviders: ['webgpu'],
 45:     };
 46: 
 47:     static get nearest_interpolate_4d() {
 48:         if (!this._nearest_interpolate_4d) {
 49:             this._nearest_interpolate_4d = wrap(
 50:                 [8, 10, 18, 0, 58, 129, 1, 10, 41, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 18, 10, 4, 109, 111, 100, 101, 34, 7, 110, 101, 97, 114, 101, 115, 116, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 21],
 51:                 this.session_options,
 52:                 'y',
 53:             );
 54:         }
 55:         return this._nearest_interpolate_4d;
 56:     }
 57:     static get bilinear_interpolate_4d() {
 58:         if (!this._bilinear_interpolate_4d) {
 59:             this._bilinear_interpolate_4d = wrap(
 60:                 [8, 9, 18, 0, 58, 128, 1, 10, 40, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 17, 10, 4, 109, 111, 100, 101, 34, 6, 108, 105, 110, 101, 97, 114, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
 61:                 this.session_options,
 62:                 'y',
 63:             );
 64:         }
 65:         return this._bilinear_interpolate_4d;
 66:     }
 67: 
 68:     static get bicubic_interpolate_4d() {
 69:         if (!this._bicubic_interpolate_4d) {
 70:             this._bicubic_interpolate_4d = wrap(
 71:                 [8, 9, 18, 0, 58, 127, 10, 39, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 16, 10, 4, 109, 111, 100, 101, 34, 5, 99, 117, 98, 105, 99, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
 72:                 this.session_options,
 73:                 'y',
 74:             );
 75:         }
 76:         return this._bicubic_interpolate_4d;
 77:     }
 78: 
 79:     static get matmul() {
 80:         if (!this._matmul) {
 81:             this._matmul = wrap(
 82:                 [8, 9, 18, 0, 58, 55, 10, 17, 10, 1, 97, 10, 1, 98, 18, 1, 99, 34, 6, 77, 97, 116, 77, 117, 108, 18, 1, 114, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 98, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 99, 18, 4, 10, 2, 8, 1, 66, 2, 16, 20],
 83:                 this.session_options,
 84:                 'c',
 85:             );
 86:         }
 87:         return this._matmul;
 88:     }
 89: 
 90:     static get stft() {
 91:         if (!this._stft) {
 92:             this._stft = wrap(
 93:                 [8, 7, 18, 0, 58, 148, 1, 10, 38, 10, 1, 115, 10, 1, 106, 10, 1, 119, 10, 1, 108, 18, 1, 111, 34, 4, 83, 84, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 115, 90, 26, 10, 1, 115, 18, 21, 10, 19, 8, 1, 18, 15, 10, 3, 18, 1, 98, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 106, 18, 6, 10, 4, 8, 7, 18, 0, 90, 16, 10, 1, 119, 18, 11, 10, 9, 8, 1, 18, 5, 10, 3, 18, 1, 119, 90, 11, 10, 1, 108, 18, 6, 10, 4, 8, 7, 18, 0, 98, 31, 10, 1, 111, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 102, 10, 3, 18, 1, 100, 10, 3, 18, 1, 99, 66, 2, 16, 17],
 94:                 this.session_options,
 95:                 'o',
 96:             )
 97:         }
 98:         return this._stft;
 99:     }
100: 
101:     static get rfft() {
102:         if (!this._rfft) {
103:             this._rfft = wrap(
104:                 [8, 9, 18, 0, 58, 97, 10, 33, 10, 1, 120, 10, 0, 10, 1, 97, 18, 1, 121, 34, 3, 68, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 100, 90, 21, 10, 1, 120, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 97, 18, 6, 10, 4, 8, 7, 18, 0, 98, 21, 10, 1, 121, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 66, 2, 16, 20],
105:                 this.session_options,
106:                 'y',
107:             )
108:         }
109:         return this._rfft;
110:     }
111: 
112:     static get top_k() {
113:         if (!this._top_k) {
114:             this._top_k = wrap(
115:                 [8, 10, 18, 0, 58, 73, 10, 18, 10, 1, 120, 10, 1, 107, 18, 1, 118, 18, 1, 105, 34, 4, 84, 111, 112, 75, 18, 1, 116, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 15, 10, 1, 107, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 118, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 105, 18, 4, 10, 2, 8, 7, 66, 2, 16, 21],
116:                 this.session_options,
117:                 [ /* Values */ 'v', /* Indices */ 'i']
118:             )
119:         }
120:         return this._top_k;
121:     }
122: 
123:     static get slice() {
124:         if (!this._slice) {
125:             this._slice = wrap(
126:                 [8, 7, 18, 0, 58, 96, 10, 25, 10, 1, 120, 10, 1, 115, 10, 1, 101, 10, 1, 97, 10, 1, 116, 18, 1, 121, 34, 5, 83, 108, 105, 99, 101, 18, 1, 114, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 115, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 101, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 116, 18, 4, 10, 2, 8, 7, 98, 9, 10, 1, 121, 18, 4, 10, 2, 8, 1, 66, 2, 16, 13],
127:                 this.session_options,
128:                 'y',
129:             )
130:         }
131:         return this._slice;
132:     }
133: }
</file>

<file path="src/tjs/utils/audio.js">
  1: /**
  2:  * @file Helper module for audio processing. 
  3:  * 
  4:  * These functions and classes are only used internally, 
  5:  * meaning an end-user shouldn't need to access anything here.
  6:  * 
  7:  * @module utils/audio
  8:  */
  9: 
 10: import {
 11:     getFile,
 12: } from './hub.js';
 13: import { FFT, max } from './maths.js';
 14: import {
 15:     calculateReflectOffset, saveBlob,
 16: } from './core.js';
 17: import { apis } from '../env.js';
 18: import { Tensor, matmul } from './tensor.js';
 19: import fs from 'node:fs';
 20: 
 21: /**
 22:  * Helper function to read audio from a path/URL.
 23:  * @param {string|URL} url The path/URL to load the audio from.
 24:  * @param {number} sampling_rate The sampling rate to use when decoding the audio.
 25:  * @returns {Promise<Float32Array>} The decoded audio as a `Float32Array`.
 26:  */
 27: export async function read_audio(url, sampling_rate) {
 28:     if (typeof AudioContext === 'undefined') {
 29:         // Running in node or an environment without AudioContext
 30:         throw Error(
 31:             "Unable to load audio from path/URL since `AudioContext` is not available in your environment. " +
 32:             "Instead, audio data should be passed directly to the pipeline/processor. " +
 33:             "For more information and some example code, see https://huggingface.co/docs/transformers.js/guides/node-audio-processing."
 34:         )
 35:     }
 36: 
 37:     const response = await (await getFile(url)).arrayBuffer();
 38:     const audioCTX = new AudioContext({ sampleRate: sampling_rate });
 39:     if (typeof sampling_rate === 'undefined') {
 40:         console.warn(`No sampling rate provided, using default of ${audioCTX.sampleRate}Hz.`)
 41:     }
 42:     const decoded = await audioCTX.decodeAudioData(response);
 43: 
 44:     /** @type {Float32Array} */
 45:     let audio;
 46: 
 47:     // We now replicate HuggingFace's `ffmpeg_read` method:
 48:     if (decoded.numberOfChannels === 2) {
 49:         // When downmixing a stereo audio file to mono using the -ac 1 option in FFmpeg,
 50:         // the audio signal is summed across both channels to create a single mono channel.
 51:         // However, if the audio is at full scale (i.e. the highest possible volume level),
 52:         // the summing of the two channels can cause the audio signal to clip or distort.
 53: 
 54:         // To prevent this clipping, FFmpeg applies a scaling factor of 1/sqrt(2) (~ 0.707)
 55:         // to the audio signal before summing the two channels. This scaling factor ensures
 56:         // that the combined audio signal will not exceed the maximum possible level, even
 57:         // if both channels are at full scale.
 58: 
 59:         // After applying this scaling factor, the audio signal from both channels is summed
 60:         // to create a single mono channel. It's worth noting that this scaling factor is
 61:         // only applied when downmixing stereo audio to mono using the -ac 1 option in FFmpeg.
 62:         // If you're using a different downmixing method, or if you're not downmixing the
 63:         // audio at all, this scaling factor may not be needed.
 64:         const SCALING_FACTOR = Math.sqrt(2);
 65: 
 66:         const left = decoded.getChannelData(0);
 67:         const right = decoded.getChannelData(1);
 68: 
 69:         audio = new Float32Array(left.length);
 70:         for (let i = 0; i < decoded.length; ++i) {
 71:             audio[i] = SCALING_FACTOR * (left[i] + right[i]) / 2;
 72:         }
 73: 
 74:     } else {
 75:         // If the audio is not stereo, we can just use the first channel:
 76:         audio = decoded.getChannelData(0);
 77:     }
 78: 
 79:     return audio;
 80: }
 81: 
 82: /**
 83:  * Helper function to generate windows that are special cases of the generalized cosine window.
 84:  * See https://www.mathworks.com/help/signal/ug/generalized-cosine-windows.html for more information.
 85:  * @param {number} M Number of points in the output window. If zero or less, an empty array is returned.
 86:  * @param {number} a_0 Offset for the generalized cosine window.
 87:  * @returns {Float64Array} The generated window.
 88:  */
 89: function generalized_cosine_window(M, a_0) {
 90:     if (M < 1) {
 91:         return new Float64Array();
 92:     }
 93:     if (M === 1) {
 94:         return new Float64Array([1]);
 95:     }
 96: 
 97:     const a_1 = 1 - a_0;
 98:     const factor = 2 * Math.PI / (M - 1);
 99: 
100:     const cos_vals = new Float64Array(M);
101:     for (let i = 0; i < M; ++i) {
102:         cos_vals[i] = a_0 - a_1 * Math.cos(i * factor);
103:     }
104:     return cos_vals;
105: }
106: 
107: /**
108:  * Generates a Hanning window of length M.
109:  * See https://numpy.org/doc/stable/reference/generated/numpy.hanning.html for more information.
110:  *
111:  * @param {number} M The length of the Hanning window to generate.
112:  * @returns {Float64Array} The generated Hanning window.
113:  */
114: export function hanning(M) {
115:     return generalized_cosine_window(M, 0.5);
116: }
117: 
118: 
119: /**
120:  * Generates a Hamming window of length M.
121:  * See https://numpy.org/doc/stable/reference/generated/numpy.hamming.html for more information.
122:  *
123:  * @param {number} M The length of the Hamming window to generate.
124:  * @returns {Float64Array} The generated Hamming window.
125:  */
126: export function hamming(M) {
127:     return generalized_cosine_window(M, 0.54);
128: }
129: 
130: 
131: const HERTZ_TO_MEL_MAPPING = {
132:     "htk": (/** @type {number} */ freq) => 2595.0 * Math.log10(1.0 + (freq / 700.0)),
133:     "kaldi": (/** @type {number} */ freq) => 1127.0 * Math.log(1.0 + (freq / 700.0)),
134:     "slaney": (/** @type {number} */ freq, min_log_hertz = 1000.0, min_log_mel = 15.0, logstep = 27.0 / Math.log(6.4)) =>
135:         freq >= min_log_hertz
136:             ? min_log_mel + Math.log(freq / min_log_hertz) * logstep
137:             : 3.0 * freq / 200.0,
138: }
139: 
140: /**
141:  * @template {Float32Array|Float64Array|number} T 
142:  * @param {T} freq 
143:  * @param {string} [mel_scale]
144:  * @returns {T}
145:  */
146: function hertz_to_mel(freq, mel_scale = "htk") {
147:     const fn = HERTZ_TO_MEL_MAPPING[mel_scale];
148:     if (!fn) {
149:         throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
150:     }
151: 
152:     // @ts-expect-error ts(2322)
153:     return typeof freq === 'number' ? fn(freq) : freq.map(x => fn(x));
154: }
155: 
156: const MEL_TO_HERTZ_MAPPING = {
157:     "htk": (/** @type {number} */ mels) => 700.0 * (10.0 ** (mels / 2595.0) - 1.0),
158:     "kaldi": (/** @type {number} */ mels) => 700.0 * (Math.exp(mels / 1127.0) - 1.0),
159:     "slaney": (/** @type {number} */ mels, min_log_hertz = 1000.0, min_log_mel = 15.0, logstep = Math.log(6.4) / 27.0) => mels >= min_log_mel
160:         ? min_log_hertz * Math.exp(logstep * (mels - min_log_mel))
161:         : 200.0 * mels / 3.0,
162: }
163: 
164: /**
165:  * @template {Float32Array|Float64Array|number} T 
166:  * @param {T} mels 
167:  * @param {string} [mel_scale]
168:  * @returns {T}
169:  */
170: function mel_to_hertz(mels, mel_scale = "htk") {
171:     const fn = MEL_TO_HERTZ_MAPPING[mel_scale];
172:     if (!fn) {
173:         throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
174:     }
175: 
176:     // @ts-expect-error ts(2322)
177:     return typeof mels === 'number' ? fn(mels) : mels.map(x => fn(x));
178: }
179: 
180: /**
181: * Creates a triangular filter bank.
182: *
183: * Adapted from torchaudio and librosa.
184: *
185: * @param {Float64Array} fft_freqs Discrete frequencies of the FFT bins in Hz, of shape `(num_frequency_bins,)`.
186: * @param {Float64Array} filter_freqs Center frequencies of the triangular filters to create, in Hz, of shape `(num_mel_filters,)`.
187: * @returns {number[][]} of shape `(num_frequency_bins, num_mel_filters)`.
188: */
189: function _create_triangular_filter_bank(fft_freqs, filter_freqs) {
190:     const filter_diff = Float64Array.from(
191:         { length: filter_freqs.length - 1 },
192:         (_, i) => filter_freqs[i + 1] - filter_freqs[i]
193:     );
194: 
195:     const slopes = Array.from({
196:         length: fft_freqs.length
197:     }, () => new Array(filter_freqs.length));
198: 
199:     for (let j = 0; j < fft_freqs.length; ++j) {
200:         const slope = slopes[j];
201:         for (let i = 0; i < filter_freqs.length; ++i) {
202:             slope[i] = filter_freqs[i] - fft_freqs[j];
203:         }
204:     }
205: 
206:     const numFreqs = filter_freqs.length - 2;
207:     const ret = Array.from({ length: numFreqs }, () => new Array(fft_freqs.length));
208: 
209:     for (let j = 0; j < fft_freqs.length; ++j) { // 201
210:         const slope = slopes[j];
211:         for (let i = 0; i < numFreqs; ++i) { // 80
212:             const down = -slope[i] / filter_diff[i];
213:             const up = slope[i + 2] / filter_diff[i + 1];
214:             ret[i][j] = Math.max(0, Math.min(down, up));
215:         }
216:     }
217:     return ret;
218: }
219: 
220: /**
221:  * Return evenly spaced numbers over a specified interval.
222:  * @param {number} start The starting value of the sequence.
223:  * @param {number} end The end value of the sequence.
224:  * @param {number} num Number of samples to generate.
225:  * @returns `num` evenly spaced samples, calculated over the interval `[start, stop]`.
226:  */
227: function linspace(start, end, num) {
228:     const step = (end - start) / (num - 1);
229:     return Float64Array.from({ length: num }, (_, i) => start + step * i);
230: }
231: 
232: /**
233:  * Creates a frequency bin conversion matrix used to obtain a mel spectrogram. This is called a *mel filter bank*, and
234:  * various implementation exist, which differ in the number of filters, the shape of the filters, the way the filters
235:  * are spaced, the bandwidth of the filters, and the manner in which the spectrum is warped. The goal of these
236:  * features is to approximate the non-linear human perception of the variation in pitch with respect to the frequency.
237:  * @param {number} num_frequency_bins Number of frequency bins (should be the same as `n_fft // 2 + 1`
238:  * where `n_fft` is the size of the Fourier Transform used to compute the spectrogram).
239:  * @param {number} num_mel_filters Number of mel filters to generate.
240:  * @param {number} min_frequency Lowest frequency of interest in Hz.
241:  * @param {number} max_frequency Highest frequency of interest in Hz. This should not exceed `sampling_rate / 2`.
242:  * @param {number} sampling_rate Sample rate of the audio waveform.
243:  * @param {string} [norm] If `"slaney"`, divide the triangular mel weights by the width of the mel band (area normalization).
244:  * @param {string} [mel_scale] The mel frequency scale to use, `"htk"` or `"slaney"`.
245:  * @param {boolean} [triangularize_in_mel_space] If this option is enabled, the triangular filter is applied in mel space rather than frequency space.
246:  * This should be set to `true` in order to get the same results as `torchaudio` when computing mel filters.
247:  * @returns {number[][]} Triangular filter bank matrix, which is a 2D array of shape (`num_frequency_bins`, `num_mel_filters`).
248:  * This is a projection matrix to go from a spectrogram to a mel spectrogram.
249:  */
250: export function mel_filter_bank(
251:     num_frequency_bins,
252:     num_mel_filters,
253:     min_frequency,
254:     max_frequency,
255:     sampling_rate,
256:     norm = null,
257:     mel_scale = "htk",
258:     triangularize_in_mel_space = false,
259: ) {
260:     if (norm !== null && norm !== "slaney") {
261:         throw new Error('norm must be one of null or "slaney"');
262:     }
263: 
264:     if (num_frequency_bins < 2) {
265:         throw new Error(`Require num_frequency_bins: ${num_frequency_bins} >= 2`);
266:     }
267: 
268:     if (min_frequency > max_frequency) {
269:         throw new Error(`Require min_frequency: ${min_frequency} <= max_frequency: ${max_frequency}`);
270:     }
271: 
272:     const mel_min = hertz_to_mel(min_frequency, mel_scale);
273:     const mel_max = hertz_to_mel(max_frequency, mel_scale);
274:     const mel_freqs = linspace(mel_min, mel_max, num_mel_filters + 2);
275: 
276:     let filter_freqs = mel_to_hertz(mel_freqs, mel_scale);
277:     let fft_freqs; // frequencies of FFT bins in Hz
278: 
279:     if (triangularize_in_mel_space) {
280:         const fft_bin_width = sampling_rate / ((num_frequency_bins - 1) * 2);
281:         fft_freqs = hertz_to_mel(Float64Array.from({ length: num_frequency_bins }, (_, i) => i * fft_bin_width), mel_scale);
282:         filter_freqs = mel_freqs;
283:     } else {
284:         fft_freqs = linspace(0, Math.floor(sampling_rate / 2), num_frequency_bins);
285:     }
286: 
287:     const mel_filters = _create_triangular_filter_bank(fft_freqs, filter_freqs);
288: 
289:     if (norm !== null && norm === "slaney") {
290:         // Slaney-style mel is scaled to be approx constant energy per channel
291:         for (let i = 0; i < num_mel_filters; ++i) {
292:             const filter = mel_filters[i];
293:             const enorm = 2.0 / (filter_freqs[i + 2] - filter_freqs[i]);
294:             for (let j = 0; j < num_frequency_bins; ++j) {
295:                 // Apply this enorm to all frequency bins
296:                 filter[j] *= enorm;
297:             }
298:         }
299:     }
300: 
301:     // TODO warn if there is a zero row
302: 
303:     return mel_filters;
304: 
305: }
306: 
307: /**
308:  * @template {Float32Array|Float64Array} T
309:  * Pads an array with a reflected version of itself on both ends.
310:  * @param {T} array The array to pad.
311:  * @param {number} left The amount of padding to add to the left.
312:  * @param {number} right The amount of padding to add to the right.
313:  * @returns {T} The padded array.
314:  */
315: function padReflect(array, left, right) {
316:     // @ts-ignore
317:     const padded = new array.constructor(array.length + left + right);
318:     const w = array.length - 1;
319: 
320:     for (let i = 0; i < array.length; ++i) {
321:         padded[left + i] = array[i];
322:     }
323: 
324:     for (let i = 1; i <= left; ++i) {
325:         padded[left - i] = array[calculateReflectOffset(i, w)];
326:     }
327: 
328:     for (let i = 1; i <= right; ++i) {
329:         padded[w + left + i] = array[calculateReflectOffset(w - i, w)];
330:     }
331: 
332:     return padded;
333: }
334: 
335: /**
336:  * Helper function to compute `amplitude_to_db` and `power_to_db`.
337:  * @template {Float32Array|Float64Array} T
338:  * @param {T} spectrogram 
339:  * @param {number} factor 
340:  * @param {number} reference 
341:  * @param {number} min_value 
342:  * @param {number} db_range 
343:  * @returns {T}
344:  */
345: function _db_conversion_helper(spectrogram, factor, reference, min_value, db_range) {
346:     if (reference <= 0) {
347:         throw new Error('reference must be greater than zero');
348:     }
349: 
350:     if (min_value <= 0) {
351:         throw new Error('min_value must be greater than zero');
352:     }
353: 
354:     reference = Math.max(min_value, reference);
355: 
356:     const logReference = Math.log10(reference);
357:     for (let i = 0; i < spectrogram.length; ++i) {
358:         spectrogram[i] = factor * Math.log10(Math.max(min_value, spectrogram[i]) - logReference)
359:     }
360: 
361:     if (db_range !== null) {
362:         if (db_range <= 0) {
363:             throw new Error('db_range must be greater than zero');
364:         }
365:         const maxValue = max(spectrogram)[0] - db_range;
366:         for (let i = 0; i < spectrogram.length; ++i) {
367:             spectrogram[i] = Math.max(spectrogram[i], maxValue);
368:         }
369:     }
370: 
371:     return spectrogram;
372: }
373: 
374: /**
375:  * Converts an amplitude spectrogram to the decibel scale. This computes `20 * log10(spectrogram / reference)`,
376:  * using basic logarithm properties for numerical stability. NOTE: Operates in-place.
377:  * 
378:  * The motivation behind applying the log function on the (mel) spectrogram is that humans do not hear loudness on a
379:  * linear scale. Generally to double the perceived volume of a sound we need to put 8 times as much energy into it.
380:  * This means that large variations in energy may not sound all that different if the sound is loud to begin with.
381:  * This compression operation makes the (mel) spectrogram features match more closely what humans actually hear.
382:  * 
383:  * @template {Float32Array|Float64Array} T
384:  * @param {T} spectrogram The input amplitude (mel) spectrogram.
385:  * @param {number} [reference=1.0] Sets the input spectrogram value that corresponds to 0 dB.
386:  * For example, use `np.max(spectrogram)` to set the loudest part to 0 dB. Must be greater than zero.
387:  * @param {number} [min_value=1e-5] The spectrogram will be clipped to this minimum value before conversion to decibels,
388:  * to avoid taking `log(0)`. The default of `1e-5` corresponds to a minimum of -100 dB. Must be greater than zero.
389:  * @param {number} [db_range=null] Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the
390:  * difference between the peak value and the smallest value will never be more than 80 dB. Must be greater than zero.
391:  * @returns {T} The modified spectrogram in decibels.
392:  */
393: function amplitude_to_db(spectrogram, reference = 1.0, min_value = 1e-5, db_range = null) {
394:     return _db_conversion_helper(spectrogram, 20.0, reference, min_value, db_range);
395: }
396: 
397: /**
398:  * Converts a power spectrogram to the decibel scale. This computes `10 * log10(spectrogram / reference)`,
399:  * using basic logarithm properties for numerical stability. NOTE: Operates in-place.
400:  * 
401:  * The motivation behind applying the log function on the (mel) spectrogram is that humans do not hear loudness on a
402:  * linear scale. Generally to double the perceived volume of a sound we need to put 8 times as much energy into it.
403:  * This means that large variations in energy may not sound all that different if the sound is loud to begin with.
404:  * This compression operation makes the (mel) spectrogram features match more closely what humans actually hear.
405:  * 
406:  * Based on the implementation of `librosa.power_to_db`.
407:  * 
408:  * @template {Float32Array|Float64Array} T
409:  * @param {T} spectrogram The input power (mel) spectrogram. Note that a power spectrogram has the amplitudes squared!
410:  * @param {number} [reference=1.0] Sets the input spectrogram value that corresponds to 0 dB.
411:  * For example, use `np.max(spectrogram)` to set the loudest part to 0 dB. Must be greater than zero.
412:  * @param {number} [min_value=1e-10] The spectrogram will be clipped to this minimum value before conversion to decibels,
413:  * to avoid taking `log(0)`. The default of `1e-10` corresponds to a minimum of -100 dB. Must be greater than zero.
414:  * @param {number} [db_range=null] Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the
415:  * difference between the peak value and the smallest value will never be more than 80 dB. Must be greater than zero.
416:  * @returns {T} The modified spectrogram in decibels.
417:  */
418: function power_to_db(spectrogram, reference = 1.0, min_value = 1e-10, db_range = null) {
419:     return _db_conversion_helper(spectrogram, 10.0, reference, min_value, db_range);
420: }
421: 
422: /**
423:  * Calculates a spectrogram over one waveform using the Short-Time Fourier Transform.
424:  * 
425:  * This function can create the following kinds of spectrograms:
426:  *   - amplitude spectrogram (`power = 1.0`)
427:  *   - power spectrogram (`power = 2.0`)
428:  *   - complex-valued spectrogram (`power = None`)
429:  *   - log spectrogram (use `log_mel` argument)
430:  *   - mel spectrogram (provide `mel_filters`)
431:  *   - log-mel spectrogram (provide `mel_filters` and `log_mel`)
432:  *
433:  * In this implementation, the window is assumed to be zero-padded to have the same size as the analysis frame.
434:  * A padded window can be obtained from `window_function()`. The FFT input buffer may be larger than the analysis frame, 
435:  * typically the next power of two.
436:  * 
437:  * @param {Float32Array|Float64Array} waveform The input waveform of shape `(length,)`. This must be a single real-valued, mono waveform.
438:  * @param {Float32Array|Float64Array} window The windowing function to apply of shape `(frame_length,)`, including zero-padding if necessary. The actual window length may be
439:  * shorter than `frame_length`, but we're assuming the array has already been zero-padded.
440:  * @param {number} frame_length The length of the analysis frames in samples (a.k.a., `fft_length`).
441:  * @param {number} hop_length The stride between successive analysis frames in samples.
442:  * @param {Object} options
443:  * @param {number} [options.fft_length=null] The size of the FFT buffer in samples. This determines how many frequency bins the spectrogram will have.
444:  * For optimal speed, this should be a power of two. If `null`, uses `frame_length`.
445:  * @param {number} [options.power=1.0] If 1.0, returns the amplitude spectrogram. If 2.0, returns the power spectrogram. If `null`, returns complex numbers.
446:  * @param {boolean} [options.center=true] Whether to pad the waveform so that frame `t` is centered around time `t * hop_length`. If `false`, frame
447:  * `t` will start at time `t * hop_length`.
448:  * @param {string} [options.pad_mode="reflect"] Padding mode used when `center` is `true`. Possible values are: `"constant"` (pad with zeros),
449:  * `"edge"` (pad with edge values), `"reflect"` (pads with mirrored values).
450:  * @param {boolean} [options.onesided=true] If `true`, only computes the positive frequencies and returns a spectrogram containing `fft_length // 2 + 1`
451:  * frequency bins. If `false`, also computes the negative frequencies and returns `fft_length` frequency bins.
452:  * @param {number} [options.preemphasis=null] Coefficient for a low-pass filter that applies pre-emphasis before the DFT.
453:  * @param {boolean} [options.preemphasis_htk_flavor=true] Whether to apply the pre-emphasis filter in the HTK flavor.
454:  * @param {number[][]} [options.mel_filters=null] The mel filter bank of shape `(num_freq_bins, num_mel_filters)`.
455:  * If supplied, applies this filter bank to create a mel spectrogram.
456:  * @param {number} [options.mel_floor=1e-10] Minimum value of mel frequency banks.
457:  * @param {string} [options.log_mel=null] How to convert the spectrogram to log scale. Possible options are:
458:  * `null` (don't convert), `"log"` (take the natural logarithm) `"log10"` (take the base-10 logarithm), `"dB"` (convert to decibels).
459:  * Can only be used when `power` is not `null`.
460:  * @param {number} [options.reference=1.0] Sets the input spectrogram value that corresponds to 0 dB. For example, use `max(spectrogram)[0]` to set
461:  * the loudest part to 0 dB. Must be greater than zero.
462:  * @param {number} [options.min_value=1e-10] The spectrogram will be clipped to this minimum value before conversion to decibels, to avoid taking `log(0)`.
463:  * For a power spectrogram, the default of `1e-10` corresponds to a minimum of -100 dB. For an amplitude spectrogram, the value `1e-5` corresponds to -100 dB.
464:  * Must be greater than zero.
465:  * @param {number} [options.db_range=null] Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the difference between the
466:  * peak value and the smallest value will never be more than 80 dB. Must be greater than zero.
467:  * @param {boolean} [options.remove_dc_offset=null] Subtract mean from waveform on each frame, applied before pre-emphasis. This should be set to `true` in
468:  * order to get the same results as `torchaudio.compliance.kaldi.fbank` when computing mel filters.
469:  * @param {number} [options.max_num_frames=null] If provided, limits the number of frames to compute to this value.
470:  * @param {number} [options.min_num_frames=null] If provided, ensures the number of frames to compute is at least this value.
471:  * @param {boolean} [options.do_pad=true] If `true`, pads the output spectrogram to have `max_num_frames` frames.
472:  * @param {boolean} [options.transpose=false] If `true`, the returned spectrogram will have shape `(num_frames, num_frequency_bins/num_mel_filters)`. If `false`, the returned spectrogram will have shape `(num_frequency_bins/num_mel_filters, num_frames)`.
473:  * @returns {Promise<Tensor>} Spectrogram of shape `(num_frequency_bins, length)` (regular spectrogram) or shape `(num_mel_filters, length)` (mel spectrogram).
474:  */
475: export async function spectrogram(
476:     waveform,
477:     window,
478:     frame_length,
479:     hop_length,
480:     {
481:         fft_length = null,
482:         power = 1.0,
483:         center = true,
484:         pad_mode = "reflect",
485:         onesided = true,
486:         preemphasis = null,
487:         preemphasis_htk_flavor = true,
488:         mel_filters = null,
489:         mel_floor = 1e-10,
490:         log_mel = null,
491:         reference = 1.0,
492:         min_value = 1e-10,
493:         db_range = null,
494:         remove_dc_offset = null,
495: 
496:         // Custom parameters for efficiency reasons
497:         min_num_frames = null,
498:         max_num_frames = null,
499:         do_pad = true,
500:         transpose = false,
501:     } = {}
502: ) {
503:     const window_length = window.length;
504:     if (fft_length === null) {
505:         fft_length = frame_length;
506:     }
507:     if (frame_length > fft_length) {
508:         throw Error(`frame_length (${frame_length}) may not be larger than fft_length (${fft_length})`)
509:     }
510: 
511:     if (window_length !== frame_length) {
512:         throw new Error(`Length of the window (${window_length}) must equal frame_length (${frame_length})`);
513:     }
514: 
515:     if (hop_length <= 0) {
516:         throw new Error("hop_length must be greater than zero");
517:     }
518: 
519:     if (power === null && mel_filters !== null) {
520:         throw new Error(
521:             "You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram. " +
522:             "Specify `power` to fix this issue."
523:         );
524:     }
525: 
526:     if (!preemphasis_htk_flavor) {
527:         throw new Error(
528:             "`preemphasis_htk_flavor=false` is not currently supported."
529:         );
530:     }
531: 
532:     if (center) {
533:         if (pad_mode !== 'reflect') {
534:             throw new Error(`pad_mode="${pad_mode}" not implemented yet.`)
535:         }
536:         const half_window = Math.floor((fft_length - 1) / 2) + 1;
537:         waveform = padReflect(waveform, half_window, half_window);
538:     }
539: 
540:     // split waveform into frames of frame_length size
541:     let num_frames = Math.floor(1 + Math.floor((waveform.length - frame_length) / hop_length))
542:     if (min_num_frames !== null && num_frames < min_num_frames) {
543:         num_frames = min_num_frames
544:     }
545:     const num_frequency_bins = onesided ? Math.floor(fft_length / 2) + 1 : fft_length
546: 
547:     let d1 = num_frames;
548:     let d1Max = num_frames;
549: 
550:     // If maximum number of frames is provided, we must either pad or truncate
551:     if (max_num_frames !== null) {
552:         if (max_num_frames > num_frames) { // input is too short, so we pad
553:             if (do_pad) {
554:                 d1Max = max_num_frames;
555:             }
556:         } else { // input is too long, so we truncate
557:             d1Max = d1 = max_num_frames;
558:         }
559:     }
560: 
561:     // Preallocate arrays to store output.
562:     const fft = new FFT(fft_length);
563:     const inputBuffer = new Float64Array(fft_length);
564:     const outputBuffer = new Float64Array(fft.outputBufferSize);
565:     const transposedMagnitudeData = new Float32Array(num_frequency_bins * d1Max);
566: 
567:     for (let i = 0; i < d1; ++i) {
568:         // Populate buffer with waveform data
569:         const offset = i * hop_length;
570:         const buffer_size = Math.min(waveform.length - offset, frame_length);
571:         if (buffer_size !== frame_length) {
572:             // The full buffer is not needed, so we need to reset it (avoid overflow from previous iterations)
573:             // NOTE: We don't need to reset the buffer if it's full since we overwrite the first
574:             // `frame_length` values and the rest (`fft_length - frame_length`) remains zero.
575:             inputBuffer.fill(0, 0, frame_length);
576:         }
577: 
578:         for (let j = 0; j < buffer_size; ++j) {
579:             inputBuffer[j] = waveform[offset + j];
580:         }
581: 
582:         if (remove_dc_offset) {
583:             let sum = 0;
584:             for (let j = 0; j < buffer_size; ++j) {
585:                 sum += inputBuffer[j];
586:             }
587:             const mean = sum / buffer_size;
588:             for (let j = 0; j < buffer_size; ++j) {
589:                 inputBuffer[j] -= mean;
590:             }
591:         }
592: 
593:         if (preemphasis !== null) {
594:             // Done in reverse to avoid copies and destructive modification
595:             for (let j = buffer_size - 1; j >= 1; --j) {
596:                 inputBuffer[j] -= preemphasis * inputBuffer[j - 1];
597:             }
598:             inputBuffer[0] *= 1 - preemphasis;
599:         }
600: 
601:         // Apply window function
602:         for (let j = 0; j < window.length; ++j) {
603:             inputBuffer[j] *= window[j];
604:         }
605: 
606:         fft.realTransform(outputBuffer, inputBuffer);
607: 
608:         // compute magnitudes
609:         for (let j = 0; j < num_frequency_bins; ++j) {
610:             const j2 = j << 1;
611: 
612:             // NOTE: We transpose the data here to avoid doing it later
613:             transposedMagnitudeData[j * d1Max + i] = outputBuffer[j2] ** 2 + outputBuffer[j2 + 1] ** 2;
614:         }
615:     }
616: 
617:     if (power !== null && power !== 2) {
618:         // slight optimization to not sqrt
619:         const pow = power / 2; // we use 2 since we already squared
620:         for (let i = 0; i < transposedMagnitudeData.length; ++i) {
621:             transposedMagnitudeData[i] **= pow;
622:         }
623:     }
624: 
625:     // TODO: What if `mel_filters` is null?
626:     const num_mel_filters = mel_filters.length;
627: 
628:     // Perform matrix muliplication:
629:     // mel_spec = mel_filters @ magnitudes.T
630:     //  - mel_filters.shape=(80, 201)
631:     //  - magnitudes.shape=(3000, 201) => magnitudes.T.shape=(201, 3000)
632:     //  - mel_spec.shape=(80, 3000)
633:     let mel_spec = await matmul(
634:         // TODO: Make `mel_filters` a Tensor during initialization
635:         new Tensor('float32', mel_filters.flat(), [num_mel_filters, num_frequency_bins]),
636:         new Tensor('float32', transposedMagnitudeData, [num_frequency_bins, d1Max]),
637:     );
638:     if (transpose) {
639:         mel_spec = mel_spec.transpose(1, 0);
640:     }
641: 
642:     const mel_spec_data = /** @type {Float32Array} */(mel_spec.data);
643:     for (let i = 0; i < mel_spec_data.length; ++i) {
644:         mel_spec_data[i] = Math.max(mel_floor, mel_spec_data[i]);
645:     }
646: 
647:     if (power !== null && log_mel !== null) {
648:         const o = Math.min(mel_spec_data.length, d1 * num_mel_filters);
649:         // NOTE: operates in-place
650:         switch (log_mel) {
651:             case 'log':
652:                 for (let i = 0; i < o; ++i) {
653:                     mel_spec_data[i] = Math.log(mel_spec_data[i]);
654:                 }
655:                 break;
656:             case 'log10':
657:                 for (let i = 0; i < o; ++i) {
658:                     mel_spec_data[i] = Math.log10(mel_spec_data[i]);
659:                 }
660:                 break;
661:             case 'dB':
662:                 if (power === 1.0) {
663:                     amplitude_to_db(mel_spec_data, reference, min_value, db_range);
664:                 } else if (power === 2.0) {
665:                     power_to_db(mel_spec_data, reference, min_value, db_range);
666:                 } else {
667:                     throw new Error(`Cannot use log_mel option '${log_mel}' with power ${power}`)
668:                 }
669:                 break;
670:             default:
671:                 throw new Error(`log_mel must be one of null, 'log', 'log10' or 'dB'. Got '${log_mel}'`);
672:         }
673:     }
674: 
675:     return mel_spec;
676: }
677: 
678: /**
679:  * Returns an array containing the specified window.
680:  * @param {number} window_length The length of the window in samples.
681:  * @param {string} name The name of the window function.
682:  * @param {Object} options Additional options.
683:  * @param {boolean} [options.periodic=true] Whether the window is periodic or symmetric.
684:  * @param {number} [options.frame_length=null] The length of the analysis frames in samples.
685:  * Provide a value for `frame_length` if the window is smaller than the frame length, so that it will be zero-padded.
686:  * @param {boolean} [options.center=true] Whether to center the window inside the FFT buffer. Only used when `frame_length` is provided.
687:  * @returns {Float64Array} The window of shape `(window_length,)` or `(frame_length,)`.
688:  */
689: export function window_function(window_length, name, {
690:     periodic = true,
691:     frame_length = null,
692:     center = true,
693: } = {}) {
694:     const length = periodic ? window_length + 1 : window_length;
695:     let window;
696:     switch (name) {
697:         case 'boxcar':
698:             window = new Float64Array(length).fill(1.0);
699:             break;
700:         case 'hann':
701:         case 'hann_window':
702:             window = hanning(length);
703:             break;
704:         case 'hamming':
705:             window = hamming(length);
706:             break;
707:         case 'povey':
708:             window = hanning(length).map(x => Math.pow(x, 0.85));
709:             break;
710:         default:
711:             throw new Error(`Unknown window type ${name}.`);
712:     }
713:     if (periodic) {
714:         window = window.subarray(0, window_length);
715:     }
716:     if (frame_length === null) {
717:         return window;
718:     }
719:     if (window_length > frame_length) {
720:         throw new Error(`Length of the window (${window_length}) may not be larger than frame_length (${frame_length})`);
721:     }
722: 
723:     return window;
724: }
725: 
726: /**
727:  * Encode audio data to a WAV file.
728:  * WAV file specs : https://en.wikipedia.org/wiki/WAV#WAV_File_header
729:  * 
730:  * Adapted from https://www.npmjs.com/package/audiobuffer-to-wav
731:  * @param {Float32Array} samples The audio samples.
732:  * @param {number} rate The sample rate.
733:  * @returns {ArrayBuffer} The WAV audio buffer.
734:  */
735: function encodeWAV(samples, rate) {
736:     let offset = 44;
737:     const buffer = new ArrayBuffer(offset + samples.length * 4);
738:     const view = new DataView(buffer);
739: 
740:     /* RIFF identifier */
741:     writeString(view, 0, "RIFF");
742:     /* RIFF chunk length */
743:     view.setUint32(4, 36 + samples.length * 4, true);
744:     /* RIFF type */
745:     writeString(view, 8, "WAVE");
746:     /* format chunk identifier */
747:     writeString(view, 12, "fmt ");
748:     /* format chunk length */
749:     view.setUint32(16, 16, true);
750:     /* sample format (raw) */
751:     view.setUint16(20, 3, true);
752:     /* channel count */
753:     view.setUint16(22, 1, true);
754:     /* sample rate */
755:     view.setUint32(24, rate, true);
756:     /* byte rate (sample rate * block align) */
757:     view.setUint32(28, rate * 4, true);
758:     /* block align (channel count * bytes per sample) */
759:     view.setUint16(32, 4, true);
760:     /* bits per sample */
761:     view.setUint16(34, 32, true);
762:     /* data chunk identifier */
763:     writeString(view, 36, "data");
764:     /* data chunk length */
765:     view.setUint32(40, samples.length * 4, true);
766: 
767:     for (let i = 0; i < samples.length; ++i, offset += 4) {
768:         view.setFloat32(offset, samples[i], true);
769:     }
770: 
771:     return buffer;
772: }
773: 
774: function writeString(view, offset, string) {
775:     for (let i = 0; i < string.length; ++i) {
776:         view.setUint8(offset + i, string.charCodeAt(i));
777:     }
778: }
779: 
780: 
781: export class RawAudio {
782: 
783:     /**
784:      * Create a new `RawAudio` object.
785:      * @param {Float32Array} audio Audio data
786:      * @param {number} sampling_rate Sampling rate of the audio data
787:      */
788:     constructor(audio, sampling_rate) {
789:         this.audio = audio
790:         this.sampling_rate = sampling_rate
791:     }
792: 
793:     /**
794:      * Convert the audio to a wav file buffer.
795:      * @returns {ArrayBuffer} The WAV file.
796:      */
797:     toWav() {
798:         return encodeWAV(this.audio, this.sampling_rate)
799:     }
800: 
801:     /**
802:      * Convert the audio to a blob.
803:      * @returns {Blob}
804:      */
805:     toBlob() {
806:         const wav = this.toWav();
807:         const blob = new Blob([wav], { type: 'audio/wav' });
808:         return blob;
809:     }
810: 
811:     /**
812:      * Save the audio to a wav file.
813:      * @param {string} path
814:      */
815:     async save(path) {
816:         let fn;
817: 
818:         if (apis.IS_BROWSER_ENV) {
819:             if (apis.IS_WEBWORKER_ENV) {
820:                 throw new Error('Unable to save a file from a Web Worker.')
821:             }
822:             fn = saveBlob;
823:         } else if (apis.IS_FS_AVAILABLE) {
824:             fn = async (/** @type {string} */ path, /** @type {Blob} */ blob) => {
825:                 let buffer = await blob.arrayBuffer();
826:                 fs.writeFileSync(path, Buffer.from(buffer));
827:             }
828:         } else {
829:             throw new Error('Unable to save because filesystem is disabled in this environment.')
830:         }
831: 
832:         await fn(path, this.toBlob())
833:     }
834: }
</file>

<file path="src/tjs/utils/constants.js">
1: export const GITHUB_ISSUE_URL = 'https://github.com/huggingface/transformers.js/issues/new/choose';
2: 
3: export const CONFIG_NAME = "config.json"
4: export const FEATURE_EXTRACTOR_NAME = "preprocessor_config.json"
5: export const IMAGE_PROCESSOR_NAME = FEATURE_EXTRACTOR_NAME
6: export const PROCESSOR_NAME = "processor_config.json"
7: export const CHAT_TEMPLATE_NAME = "chat_template.jinja"
8: export const GENERATION_CONFIG_NAME = "generation_config.json"
</file>

<file path="src/tjs/utils/core.js">
  1: /**
  2:  * @file Core utility functions/classes for Transformers.js.
  3:  *
  4:  * These are only used internally, meaning an end-user shouldn't
  5:  * need to access anything here.
  6:  *
  7:  * @module utils/core
  8:  */
  9: 
 10: /**
 11:  * @typedef {Object} InitiateProgressInfo
 12:  * @property {'initiate'} status
 13:  * @property {string} name The model id or directory path.
 14:  * @property {string} file The name of the file.
 15:  */
 16: 
 17: /**
 18:  * @typedef {Object} DownloadProgressInfo
 19:  * @property {'download'} status
 20:  * @property {string} name The model id or directory path.
 21:  * @property {string} file The name of the file.
 22:  */
 23: 
 24: /**
 25:  * @typedef {Object} ProgressStatusInfo
 26:  * @property {'progress'} status
 27:  * @property {string} name The model id or directory path.
 28:  * @property {string} file The name of the file.
 29:  * @property {number} progress A number between 0 and 100.
 30:  * @property {number} loaded The number of bytes loaded.
 31:  * @property {number} total The total number of bytes to be loaded.
 32:  */
 33: 
 34: /**
 35:  * @typedef {Object} DoneProgressInfo
 36:  * @property {'done'} status
 37:  * @property {string} name The model id or directory path.
 38:  * @property {string} file The name of the file.
 39:  */
 40: 
 41: /**
 42:  * @typedef {Object} ReadyProgressInfo
 43:  * @property {'ready'} status
 44:  * @property {string} task The loaded task.
 45:  * @property {string} model The loaded model.
 46:  */
 47: 
 48: /**
 49:  * @typedef {InitiateProgressInfo | DownloadProgressInfo | ProgressStatusInfo | DoneProgressInfo | ReadyProgressInfo} ProgressInfo
 50:  */
 51: 
 52: /**
 53:  * A callback function that is called with progress information.
 54:  * @callback ProgressCallback
 55:  * @param {ProgressInfo} progressInfo
 56:  * @returns {void}
 57:  */
 58: 
 59: /**
 60:  * Helper function to dispatch progress callbacks.
 61:  *
 62:  * @param {ProgressCallback | null | undefined} progress_callback The progress callback function to dispatch.
 63:  * @param {ProgressInfo} data The data to pass to the progress callback function.
 64:  * @returns {void}
 65:  * @private
 66:  */
 67: export function dispatchCallback(progress_callback, data) {
 68:     if (progress_callback) progress_callback(data);
 69: }
 70: 
 71: /**
 72:  * Reverses the keys and values of an object.
 73:  *
 74:  * @param {Object} data The object to reverse.
 75:  * @returns {Object} The reversed object.
 76:  * @see https://ultimatecourses.com/blog/reverse-object-keys-and-values-in-javascript
 77:  */
 78: export function reverseDictionary(data) {
 79:     // https://ultimatecourses.com/blog/reverse-object-keys-and-values-in-javascript
 80:     return Object.fromEntries(Object.entries(data).map(([key, value]) => [value, key]));
 81: }
 82: 
 83: /**
 84:  * Escapes regular expression special characters from a string by replacing them with their escaped counterparts.
 85:  *
 86:  * @param {string} string The string to escape.
 87:  * @returns {string} The escaped string.
 88:  */
 89: export function escapeRegExp(string) {
 90:     return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'); // $& means the whole matched string
 91: }
 92: 
 93: /**
 94:  * Check if a value is a typed array.
 95:  * @param {*} val The value to check.
 96:  * @returns {boolean} True if the value is a `TypedArray`, false otherwise.
 97:  *
 98:  * Adapted from https://stackoverflow.com/a/71091338/13989043
 99:  */
100: export function isTypedArray(val) {
101:     return val?.prototype?.__proto__?.constructor?.name === 'TypedArray';
102: }
103: 
104: 
105: /**
106:  * Check if a value is an integer.
107:  * @param {*} x The value to check.
108:  * @returns {boolean} True if the value is a string, false otherwise.
109:  */
110: export function isIntegralNumber(x) {
111:     return Number.isInteger(x) || typeof x === 'bigint'
112: }
113: 
114: /**
115:  * Determine if a provided width or height is nullish.
116:  * @param {*} x The value to check.
117:  * @returns {boolean} True if the value is `null`, `undefined` or `-1`, false otherwise.
118:  */
119: export function isNullishDimension(x) {
120:     return x === null || x === undefined || x === -1;
121: }
122: 
123: /**
124:  * Calculates the dimensions of a nested array.
125:  *
126:  * @param {any[]} arr The nested array to calculate dimensions for.
127:  * @returns {number[]} An array containing the dimensions of the input array.
128:  */
129: export function calculateDimensions(arr) {
130:     const dimensions = [];
131:     let current = arr;
132:     while (Array.isArray(current)) {
133:         dimensions.push(current.length);
134:         current = current[0];
135:     }
136:     return dimensions;
137: }
138: 
139: /**
140:  * Replicate python's .pop() method for objects.
141:  * @param {Object} obj The object to pop from.
142:  * @param {string} key The key to pop.
143:  * @param {*} defaultValue The default value to return if the key does not exist.
144:  * @returns {*} The value of the popped key.
145:  * @throws {Error} If the key does not exist and no default value is provided.
146:  */
147: export function pop(obj, key, defaultValue = undefined) {
148:     const value = obj[key];
149:     if (value !== undefined) {
150:         delete obj[key];
151:         return value;
152:     }
153:     if (defaultValue === undefined) {
154:         throw Error(`Key ${key} does not exist in object.`)
155:     }
156:     return defaultValue;
157: }
158: 
159: /**
160:  * Efficiently merge arrays, creating a new copy.
161:  * Adapted from https://stackoverflow.com/a/6768642/13989043
162:  * @param  {Array[]} arrs Arrays to merge.
163:  * @returns {Array} The merged array.
164:  */
165: export function mergeArrays(...arrs) {
166:     return Array.prototype.concat.apply([], arrs);
167: }
168: 
169: /**
170:  * Compute the Cartesian product of given arrays
171:  * @param {...Array} a Arrays to compute the product
172:  * @returns {Array} Returns the computed Cartesian product as an array
173:  * @private
174:  */
175: export function product(...a) {
176:     // Cartesian product of items
177:     // Adapted from https://stackoverflow.com/a/43053803
178:     return a.reduce((a, b) => a.flatMap(d => b.map(e => [d, e])));
179: }
180: 
181: /**
182:  * Calculates the index offset for a given index and window size.
183:  * @param {number} i The index.
184:  * @param {number} w The window size.
185:  * @returns {number} The index offset.
186:  */
187: export function calculateReflectOffset(i, w) {
188:     return Math.abs((i + w) % (2 * w) - w);
189: }
190: 
191: /**
192:  * Save blob file on the web.
193:  * @param {string} path The path to save the blob to
194:  * @param {Blob} blob The blob to save
195:  */
196: export function saveBlob(path, blob){
197:     // Convert the canvas content to a data URL
198:     const dataURL = URL.createObjectURL(blob);
199: 
200:     // Create an anchor element with the data URL as the href attribute
201:     const downloadLink = document.createElement('a');
202:     downloadLink.href = dataURL;
203: 
204:     // Set the download attribute to specify the desired filename for the downloaded image
205:     downloadLink.download = path;
206: 
207:     // Trigger the download
208:     downloadLink.click();
209: 
210:     // Clean up: remove the anchor element from the DOM
211:     downloadLink.remove();
212: 
213:     // Revoke the Object URL to free up memory
214:     URL.revokeObjectURL(dataURL);
215: }
216: 
217: /**
218:  *
219:  * @param {Object} o
220:  * @param {string[]} props
221:  * @returns {Object}
222:  */
223: export function pick(o, props) {
224:     return Object.assign(
225:         {},
226:         ...props.map((prop) => {
227:             if (o[prop] !== undefined) {
228:                 return { [prop]: o[prop] };
229:             }
230:         })
231:     );
232: }
233: 
234: /**
235:  * Calculate the length of a string, taking multi-byte characters into account.
236:  * This mimics the behavior of Python's `len` function.
237:  * @param {string} s The string to calculate the length of.
238:  * @returns {number} The length of the string.
239:  */
240: export function len(s) {
241:     let length = 0;
242:     for (const c of s) ++length;
243:     return length;
244: }
245: 
246: /**
247:  * Count the occurrences of a value in an array or string.
248:  * This mimics the behavior of Python's `count` method.
249:  * @param {any[]|string} arr The array or string to search.
250:  * @param {any} value The value to count.
251:  */
252: export function count(arr, value) {
253:     let count = 0;
254:     for (const v of arr) {
255:         if (v === value) ++count;
256:     }
257:     return count;
258: }
</file>

<file path="src/tjs/utils/data-structures.js">
  1: /**
  2:  * @file Custom data structures.
  3:  * 
  4:  * These are only used internally, meaning an end-user shouldn't
  5:  * need to access anything here.
  6:  * 
  7:  * @module utils/data-structures
  8:  */
  9: 
 10: 
 11: /**
 12:  * Efficient Heap-based Implementation of a Priority Queue.
 13:  * It uses an array-based binary heap, where the root is at index `0`, and the
 14:  * children of node `i` are located at indices `2i + 1` and `2i + 2`, respectively.
 15:  * 
 16:  * Adapted from the following sources:
 17:  * - https://stackoverflow.com/a/42919752/13989043 (original)
 18:  * - https://github.com/belladoreai/llama-tokenizer-js (minor improvements)
 19:  */
 20: export class PriorityQueue {
 21: 
 22:     /**
 23:      * Create a new PriorityQueue.
 24:      * @param {function(any, any): boolean} comparator Comparator function to determine priority. Defaults to a MaxHeap.
 25:      */
 26:     constructor(comparator = (a, b) => a > b, maxSize = Infinity) {
 27:         this._heap = [];
 28:         this._comparator = comparator;
 29:         this._maxSize = maxSize;
 30:     }
 31: 
 32:     /**
 33:      * The size of the queue
 34:      */
 35:     get size() {
 36:         return this._heap.length;
 37:     }
 38: 
 39:     /**
 40:      * Check if the queue is empty.
 41:      * @returns {boolean} `true` if the queue is empty, `false` otherwise.
 42:      */
 43:     isEmpty() {
 44:         return this.size === 0;
 45:     }
 46: 
 47:     /**
 48:      * Return the element with the highest priority in the queue.
 49:      * @returns {any} The highest priority element in the queue.
 50:      */
 51:     peek() {
 52:         return this._heap[0];
 53:     }
 54: 
 55:     /**
 56:      * Add one or more elements to the queue.
 57:      * @param  {...any} values The values to push into the queue.
 58:      * @returns {number} The new size of the queue.
 59:      */
 60:     push(...values) {
 61:         return this.extend(values);
 62:     }
 63: 
 64:     /**
 65:      * Add multiple elements to the queue.
 66:      * @param {any[]} values The values to push into the queue.
 67:      * @returns {number} The new size of the queue.
 68:      */
 69:     extend(values) {
 70:         for (const value of values) {
 71:             if (this.size < this._maxSize) {
 72:                 this._heap.push(value);
 73:                 this._siftUp();
 74:             } else {
 75:                 // Get index of value with the lowest priority
 76:                 const smallest = this._smallest();
 77: 
 78:                 // If the new value has higher priority than the smallest value in the heap
 79:                 // then replace the smallest value with the new value and update the heap
 80:                 if (this._comparator(value, this._heap[smallest])) {
 81:                     this._heap[smallest] = value;
 82:                     this._siftUpFrom(smallest);
 83:                 }
 84:             }
 85:         }
 86:         return this.size;
 87:     }
 88: 
 89:     /**
 90:      * Remove and return the element with the highest priority in the queue.
 91:      * @returns {any} The element with the highest priority in the queue.
 92:      */
 93:     pop() {
 94:         const poppedValue = this.peek();
 95:         const bottom = this.size - 1;
 96:         if (bottom > 0) {
 97:             this._swap(0, bottom);
 98:         }
 99:         this._heap.pop();
100:         this._siftDown();
101:         return poppedValue;
102:     }
103: 
104:     /**
105:      * Replace the element with the highest priority in the queue with a new value.
106:      * @param {*} value The new value.
107:      * @returns {*} The replaced value.
108:      */
109:     replace(value) {
110:         const replacedValue = this.peek();
111:         this._heap[0] = value;
112:         this._siftDown();
113:         return replacedValue;
114:     }
115: 
116:     /**
117:      * Compute the index for the parent of the node at index `i`.
118:      * @param {number} i The index of the node to get the parent of.
119:      * @returns {number} The index of the parent node.
120:      * @private
121:      */
122:     _parent(i) {
123:         return ((i + 1) >>> 1) - 1;
124:     }
125: 
126:     /**
127:      * Compute the index for the left child of the node at index `i`.
128:      * @param {number} i The index of the node to get the left child of.
129:      * @returns {number} The index of the left child.
130:      * @private
131:      */
132:     _left(i) {
133:         return (i << 1) + 1;
134:     }
135: 
136:     /**
137:      * Compute the index for the right child of the node at index `i`.
138:      * @param {number} i The index of the node to get the right child of.
139:      * @returns {number} The index of the right child.
140:      * @private
141:      */
142:     _right(i) {
143:         return (i + 1) << 1;
144:     }
145: 
146:     /**
147:      * Check if the element at index `i` is greater than the element at index `j`.
148:      * @param {number} i The index of the first element to compare.
149:      * @param {number} j The index of the second element to compare.
150:      * @returns {boolean} `true` if the element at index `i` is greater than the element at index `j`, `false` otherwise.
151:      * @private
152:      */
153:     _greater(i, j) {
154:         return this._comparator(this._heap[i], this._heap[j]);
155:     }
156: 
157:     /**
158:      * Swap the elements at indices `i` and `j`.
159:      * @param {number} i The index of the first element to swap.
160:      * @param {number} j The index of the second element to swap.
161:      * @private
162:      */
163:     _swap(i, j) {
164:         const temp = this._heap[i];
165:         this._heap[i] = this._heap[j];
166:         this._heap[j] = temp;
167:     }
168: 
169:     /**
170:      * Maintain the heap property by updating positions in the heap,
171:      * starting at the last element and moving up the heap.
172:      * @private
173:      */
174:     _siftUp() {
175:         this._siftUpFrom(this.size - 1);
176:     }
177: 
178:     /**
179:      * Helper function to sift up from a given node.
180:      * @param {number} node The index of the node to start sifting up from.
181:      */
182:     _siftUpFrom(node) {
183:         while (node > 0 && this._greater(node, this._parent(node))) {
184:             this._swap(node, this._parent(node));
185:             node = this._parent(node);
186:         }
187:     }
188: 
189:     /**
190:      * Maintain the heap property by updating positions in the heap,
191:      * starting at the first element and moving down the heap.
192:      * @private
193:      */
194:     _siftDown() {
195:         let node = 0;
196:         while (
197:             (this._left(node) < this.size && this._greater(this._left(node), node)) ||
198:             (this._right(node) < this.size && this._greater(this._right(node), node))
199:         ) {
200:             const maxChild = (this._right(node) < this.size && this._greater(this._right(node), this._left(node)))
201:                 ? this._right(node)
202:                 : this._left(node);
203:             this._swap(node, maxChild);
204:             node = maxChild;
205:         }
206:     }
207: 
208:     /**
209:      * Get the index of the smallest element in the heap. Since we use an array-based heap,
210:      * the index can be computed without needing to traverse the heap.
211:      * @private
212:      */
213:     _smallest() {
214:         return (2 ** (Math.floor(Math.log2(this.size))) - 1);
215:     }
216: }
217: 
218: /**
219:  * A trie structure to efficiently store and search for strings.
220:  */
221: export class CharTrie {
222:     constructor() {
223:         this.root = CharTrieNode.default();
224:     }
225: 
226:     /**
227:      * Adds one or more `texts` to the trie.
228:      * @param {string[]} texts The strings to add to the trie.
229:      */
230:     extend(texts) {
231:         for (const text of texts) {
232:             this.push(text);
233:         }
234:     }
235: 
236:     /**
237:      * Adds text to the trie.
238:      * @param {string} text The string to add to the trie.
239:      */
240:     push(text) {
241:         let node = this.root;
242:         for (const ch of text) {
243:             let child = node.children.get(ch);
244:             if (child === undefined) {
245:                 child = CharTrieNode.default();
246:                 node.children.set(ch, child);
247:             }
248:             node = child;
249:         }
250:         node.isLeaf = true;
251:     }
252: 
253:     /**
254:      * Searches the trie for all strings with a common prefix of `text`.
255:      * @param {string} text The common prefix to search for.
256:      * @yields {string} Each string in the trie that has `text` as a prefix.
257:      */
258:     *commonPrefixSearch(text) {
259:         let node = this.root;
260:         if (node === undefined) return;
261: 
262:         let prefix = "";
263:         for (const ch of text) {
264:             prefix += ch;
265:             node = node.children.get(ch);
266:             if (node === undefined) return;
267:             if (node.isLeaf) {
268:                 yield prefix;
269:             }
270:         }
271:     }
272: }
273: 
274: /**
275:  * Represents a node in a character trie.
276:  */
277: class CharTrieNode {
278:     /**
279:      * Create a new CharTrieNode.
280:      * @param {boolean} isLeaf Whether the node is a leaf node or not.
281:      * @param {Map<string, CharTrieNode>} children A map containing the node's children, where the key is a character and the value is a `CharTrieNode`.
282:      */
283:     constructor(isLeaf, children) {
284:         this.isLeaf = isLeaf;
285:         this.children = children;
286:     }
287: 
288:     /**
289:      * Returns a new `CharTrieNode` instance with default values.
290:      * @returns {CharTrieNode} A new `CharTrieNode` instance with `isLeaf` set to `false` and an empty `children` map.
291:      */
292:     static default() {
293:         return new CharTrieNode(false, new Map());
294:     }
295: }
296: 
297: /**
298:  * A lattice data structure to be used for tokenization.
299:  */
300: export class TokenLattice {
301:     /**
302:      * Creates a new TokenLattice instance.
303:      *
304:      * @param {string} sentence The input sentence to be tokenized.
305:      * @param {number} bosTokenId The beginning-of-sequence token ID.
306:      * @param {number} eosTokenId The end-of-sequence token ID.
307:      */
308:     constructor(sentence, bosTokenId, eosTokenId) {
309:         this.chars = Array.from(sentence);
310:         this.len = this.chars.length;
311:         this.bosTokenId = bosTokenId;
312:         this.eosTokenId = eosTokenId;
313:         this.nodes = [];
314:         this.beginNodes = Array.from({ length: this.len + 1 }, () => []);
315:         this.endNodes = Array.from({ length: this.len + 1 }, () => []);
316: 
317:         const bos = new TokenLatticeNode(this.bosTokenId, 0, 0, 0, 0.0);
318:         const eos = new TokenLatticeNode(this.eosTokenId, 1, this.len, 0, 0.0);
319:         this.nodes.push(bos.clone());
320:         this.nodes.push(eos.clone());
321:         this.beginNodes[this.len].push(eos);
322:         this.endNodes[0].push(bos);
323:     }
324: 
325:     /**
326:      * Inserts a new token node into the token lattice.
327:      *
328:      * @param {number} pos The starting position of the token.
329:      * @param {number} length The length of the token.
330:      * @param {number} score The score of the token.
331:      * @param {number} tokenId The token ID of the token.
332:      */
333:     insert(pos, length, score, tokenId) {
334:         const nodeId = this.nodes.length;
335:         const node = new TokenLatticeNode(tokenId, nodeId, pos, length, score);
336:         this.beginNodes[pos].push(node);
337:         this.endNodes[pos + length].push(node);
338:         this.nodes.push(node);
339:     }
340: 
341:     /**
342:      * Implements the Viterbi algorithm to compute the most likely sequence of tokens.
343:      *
344:      * @returns {TokenLatticeNode[]} The most likely sequence of tokens.
345:      */
346:     viterbi() {
347:         const len = this.len;
348:         let pos = 0;
349:         while (pos <= len) {
350:             if (this.beginNodes[pos].length == 0) {
351:                 return [];
352:             }
353:             for (let rnode of this.beginNodes[pos]) {
354:                 rnode.prev = null;
355:                 let bestScore = 0.0;
356:                 let bestNode = null;
357:                 for (let lnode of this.endNodes[pos]) {
358:                     const score = lnode.backtraceScore + rnode.score;
359:                     if (bestNode === null || score > bestScore) {
360:                         bestNode = lnode.clone();
361:                         bestScore = score;
362:                     }
363:                 }
364: 
365:                 if (bestNode !== null) {
366:                     rnode.prev = bestNode;
367:                     rnode.backtraceScore = bestScore;
368:                 } else {
369:                     return [];
370:                 }
371:             }
372:             ++pos;
373:         }
374: 
375:         const results = [];
376:         const root = this.beginNodes[len][0];
377:         const prev = root.prev;
378:         if (prev === null) {
379:             return [];
380:         }
381: 
382:         let node = prev.clone();
383:         while (node.prev !== null) {
384:             results.push(node.clone());
385:             const n = node.clone();
386:             node = n.prev.clone();
387:         }
388: 
389:         results.reverse();
390:         return results;
391:     }
392: 
393:     /**
394:      * @param {TokenLatticeNode} node
395:      * @returns {string} The array of nodes representing the most likely sequence of tokens.
396:      */
397:     piece(node) {
398:         return this.chars.slice(node.pos, node.pos + node.length).join('');
399:     }
400: 
401:     /**
402:      * @returns {string[]} The most likely sequence of tokens.
403:      */
404:     tokens() {
405:         const nodes = this.viterbi();
406:         return nodes.map(x => this.piece(x));
407:     }
408: 
409:     /**
410:      * @returns {number[]} The most likely sequence of token ids.
411:      */
412:     tokenIds() {
413:         const nodes = this.viterbi();
414:         return nodes.map(x => x.tokenId);
415:     }
416: }
417: class TokenLatticeNode {
418:     /**
419:      * Represents a node in a token lattice for a given sentence.
420:      * @param {number} tokenId The ID of the token associated with this node.
421:      * @param {number} nodeId The ID of this node.
422:      * @param {number} pos The starting position of the token in the sentence.
423:      * @param {number} length The length of the token.
424:      * @param {number} score The score associated with the token.
425:      */
426:     constructor(tokenId, nodeId, pos, length, score) {
427:         this.tokenId = tokenId;
428:         this.nodeId = nodeId;
429:         this.pos = pos;
430:         this.length = length;
431:         this.score = score;
432:         this.prev = null;
433:         this.backtraceScore = 0.0;
434:     }
435: 
436:     /**
437:      * Returns a clone of this node.
438:      * @returns {TokenLatticeNode} A clone of this node.
439:      */
440:     clone() {
441:         const n = new TokenLatticeNode(this.tokenId, this.nodeId, this.pos, this.length, this.score);
442:         n.prev = this.prev;
443:         n.backtraceScore = this.backtraceScore;
444:         return n;
445:     }
446: }
447: 
448: /**
449:  * A data structure which uses a trie to split a string into tokens based on a dictionary.
450:  * It can also use a regular expression to preprocess the input text before splitting.
451:  * 
452:  * NOTE: To ensure multi-byte characters are handled correctly, we operate at byte-level instead of character-level.
453:  */
454: export class DictionarySplitter {
455:     /**
456:      * @param {string[]} dictionary The dictionary of words to use for splitting.
457:      */
458:     constructor(dictionary) {
459:         this.trie = this._buildTrie(dictionary);
460:     }
461: 
462:     /**
463:      * Builds a trie from the given dictionary.
464:      * @param {string[]} dictionary The dictionary of words to build the trie from.
465:      * @returns {Object} The root node of the trie.
466:      * @private
467:      */
468:     _buildTrie(dictionary) {
469:         const trie = Object.create(null);
470:         for (const word of dictionary) {
471:             let node = trie;
472:             for (let i = 0; i < word.length; ++i) {
473:                 node = (node[word[i]] ??= Object.create(null));
474:             }
475:             node.end = word;
476:         }
477:         return trie;
478:     }
479: 
480:     /**
481:      * Splits the input text into tokens based on the dictionary.
482:      * @param {string} text The input text to split.
483:      * @returns {string[]} An array of tokens.
484:      */
485:     split(text) {
486:         const result = [];
487:         const n = text.length;
488:         let start = 0;
489:         let i = 0;
490: 
491:         while (i < n) {
492:             let node = this.trie;
493:             let match = null;
494:             let j = i;
495: 
496:             while (j < n && (node = node[text[j]])) {
497:                 if (node.end) {
498:                     // Always keep the last (i.e., longest) match.
499:                     match = node.end;
500:                 }
501:                 ++j;
502:             }
503: 
504:             if (match) {
505:                 if (i > start) {
506:                     result.push(text.slice(start, i));
507:                 }
508:                 result.push(match);
509:                 i += match.length;
510:                 start = i;
511:             } else {
512:                 ++i;
513:             }
514:         }
515:         if (start < n) {
516:             result.push(text.slice(start));
517:         }
518:         return result;
519:     }
520: }
521: 
522: /**
523: * A simple Least Recently Used (LRU) cache implementation in JavaScript.
524: * This cache stores key-value pairs and evicts the least recently used item
525: * when the capacity is exceeded.
526: */
527: export class LRUCache {
528:     /**
529:      * Creates an LRUCache instance.
530:      * @param {number} capacity The maximum number of items the cache can hold.
531:      */
532:     constructor(capacity) {
533:         this.capacity = capacity;
534:         this.cache = new Map();
535:     }
536: 
537:     /**
538:      * Retrieves the value associated with the given key and marks the key as recently used.
539:      * @param {any} key The key to retrieve.
540:      * @returns {any} The value associated with the key, or undefined if the key does not exist.
541:      */
542:     get(key) {
543:         if (!this.cache.has(key)) return undefined;
544:         const value = this.cache.get(key);
545:         this.cache.delete(key);
546:         this.cache.set(key, value);
547:         return value;
548:     }
549: 
550:     /**
551:      * Inserts or updates the key-value pair in the cache.
552:      * If the key already exists, it is updated and marked as recently used.
553:      * If the cache exceeds its capacity, the least recently used item is evicted.
554:      * @param {any} key The key to add or update.
555:      * @param {any} value The value to associate with the key.
556:      */
557:     put(key, value) {
558:         if (this.cache.has(key)) {
559:             this.cache.delete(key);
560:         }
561:         this.cache.set(key, value);
562:         if (this.cache.size > this.capacity) {
563:             this.cache.delete(this.cache.keys().next().value);
564:         }
565:     }
566: 
567:     /**
568:      * Clears the cache.
569:      */
570:     clear() {
571:         this.cache.clear();
572:     }
573: }
</file>

<file path="src/tjs/utils/devices.js">
 1: /**
 2:  * The list of devices supported by Transformers.js
 3:  */
 4: export const DEVICE_TYPES = Object.freeze({
 5:     auto: 'auto', // Auto-detect based on device and environment
 6:     gpu: 'gpu', // Auto-detect GPU
 7:     cpu: 'cpu', // CPU
 8:     wasm: 'wasm', // WebAssembly
 9:     webgpu: 'webgpu', // WebGPU
10:     cuda: 'cuda', // CUDA
11:     dml: 'dml', // DirectML
12: 
13:     webnn: 'webnn', // WebNN (default)
14:     'webnn-npu': 'webnn-npu', // WebNN NPU
15:     'webnn-gpu': 'webnn-gpu', // WebNN GPU
16:     'webnn-cpu': 'webnn-cpu', // WebNN CPU
17: });
18: 
19: /**
20:  * @typedef {keyof typeof DEVICE_TYPES} DeviceType
21:  */
</file>

<file path="src/tjs/utils/dtypes.js">
 1: /// <reference types="@webgpu/types" />
 2: 
 3: import { apis } from "../env.js";
 4: 
 5: import { DEVICE_TYPES } from "./devices.js";
 6: 
 7: // TODO: Use the adapter from `env.backends.onnx.webgpu.adapter` to check for `shader-f16` support,
 8: // when available in https://github.com/microsoft/onnxruntime/pull/19940.
 9: // For more information, see https://github.com/microsoft/onnxruntime/pull/19857#issuecomment-1999984753
10: 
11: /**
12:  * Checks if WebGPU fp16 support is available in the current environment.
13:  */
14: export const isWebGpuFp16Supported = (function () {
15:     /** @type {boolean} */
16:     let cachedResult;
17: 
18:     return async function () {
19:         if (cachedResult === undefined) {
20:             if (!apis.IS_WEBGPU_AVAILABLE) {
21:                 cachedResult = false;
22:             } else {
23:                 try {
24:                     const adapter = await navigator.gpu.requestAdapter();
25:                     cachedResult = adapter.features.has('shader-f16');
26:                 } catch (e) {
27:                     cachedResult = false;
28:                 }
29:             }
30:         }
31:         return cachedResult;
32:     };
33: })();
34: 
35: export const DATA_TYPES = Object.freeze({
36:     auto: 'auto', // Auto-detect based on environment
37:     fp32: 'fp32',
38:     fp16: 'fp16',
39:     q8: 'q8',
40:     int8: 'int8',
41:     uint8: 'uint8',
42:     q4: 'q4',
43:     bnb4: 'bnb4',
44:     q4f16: 'q4f16', // fp16 model with int4 block weight quantization
45: });
46: /** @typedef {keyof typeof DATA_TYPES} DataType */
47: 
48: export const DEFAULT_DEVICE_DTYPE_MAPPING = Object.freeze({
49:     // NOTE: If not specified, will default to fp32
50:     [DEVICE_TYPES.wasm]: DATA_TYPES.q8,
51: });
52: 
53: /** @type {Record<Exclude<DataType, "auto">, string>} */
54: export const DEFAULT_DTYPE_SUFFIX_MAPPING = Object.freeze({
55:     [DATA_TYPES.fp32]: '',
56:     [DATA_TYPES.fp16]: '_fp16',
57:     [DATA_TYPES.int8]: '_int8',
58:     [DATA_TYPES.uint8]: '_uint8',
59:     [DATA_TYPES.q8]: '_quantized',
60:     [DATA_TYPES.q4]: '_q4',
61:     [DATA_TYPES.q4f16]: '_q4f16',
62:     [DATA_TYPES.bnb4]: '_bnb4',
63: });
</file>

<file path="src/tjs/utils/generic.js">
 1: /**
 2:  * A base class for creating callable objects.
 3:  * See [here](https://stackoverflow.com/q/76073890) for more information.
 4:  * 
 5:  * @type {new () => {(...args: any[]): any, _call(...args: any[]): any}}
 6:  */
 7: export const Callable = /** @type {any} */ (class {
 8:     /**
 9:     * Creates a new instance of the Callable class.
10:     */
11:     constructor() {
12:         /**
13:          * Creates a closure that delegates to a private method '_call' with the given arguments.
14:          * @type {any}
15:          * @param {...any} args Zero or more arguments to pass to the '_call' method.
16:          * @returns {*} The result of calling the '_call' method.
17:          */
18:         let closure = function (...args) {
19:             return closure._call(...args)
20:         }
21:         return Object.setPrototypeOf(closure, new.target.prototype)
22:     }
23: 
24:     /**
25:      * This method should be implemented in subclasses to provide the
26:      * functionality of the callable object.
27:      *
28:      * @param {any[]} args
29:      * @throws {Error} If the subclass does not implement the `_call` method.
30:      */
31:     _call(...args) {
32:         throw Error('Must implement _call method in subclass')
33:     }
34: });
</file>

<file path="src/tjs/utils/hub.js">
  1: /**
  2:  * @file Utility functions to interact with the Hugging Face Hub (https://huggingface.co/models)
  3:  *
  4:  * @module utils/hub
  5:  */
  6: 
  7: import fs from 'node:fs';
  8: import path from 'node:path';
  9: 
 10: import { apis, env } from '../env.js';
 11: import { dispatchCallback } from './core.js';
 12: 
 13: /**
 14:  * @typedef {boolean|number} ExternalData Whether to load the model using the external data format (used for models >= 2GB in size).
 15:  * If `true`, the model will be loaded using the external data format.
 16:  * If a number, this many chunks will be loaded using the external data format (of the form: "model.onnx_data[_{chunk_number}]").
 17:  */
 18: export const MAX_EXTERNAL_DATA_CHUNKS = 100;
 19: 
 20: /**
 21:  * @typedef {Object} PretrainedOptions Options for loading a pretrained model.
 22:  * @property {import('./core.js').ProgressCallback} [progress_callback=null] If specified, this function will be called during model construction, to provide the user with progress updates.
 23:  * @property {import('../configs.js').PretrainedConfig} [config=null] Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:
 24:  * - The model is a model provided by the library (loaded with the *model id* string of a pretrained model).
 25:  * - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.
 26:  * @property {string} [cache_dir=null] Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.
 27:  * @property {boolean} [local_files_only=false] Whether or not to only look at local files (e.g., not try downloading the model).
 28:  * @property {string} [revision='main'] The specific model version to use. It can be a branch name, a tag name, or a commit id,
 29:  * since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.
 30:  * NOTE: This setting is ignored for local requests.
 31:  */
 32: 
 33: /**
 34:  * @typedef {Object} ModelSpecificPretrainedOptions Options for loading a pretrained model.
 35:  * @property {string} [subfolder='onnx'] In case the relevant files are located inside a subfolder of the model repo on huggingface.co,
 36:  * you can specify the folder name here.
 37:  * @property {string} [model_file_name=null] If specified, load the model with this name (excluding the .onnx suffix). Currently only valid for encoder- or decoder-only models.
 38:  * @property {import("./devices.js").DeviceType|Record<string, import("./devices.js").DeviceType>} [device=null] The device to run the model on. If not specified, the device will be chosen from the environment settings.
 39:  * @property {import("./dtypes.js").DataType|Record<string, import("./dtypes.js").DataType>} [dtype=null] The data type to use for the model. If not specified, the data type will be chosen from the environment settings.
 40:  * @property {ExternalData|Record<string, ExternalData>} [use_external_data_format=false] Whether to load the model using the external data format (used for models >= 2GB in size).
 41:  * @property {import('onnxruntime-common').InferenceSession.SessionOptions} [session_options] (Optional) User-specified session options passed to the runtime. If not provided, suitable defaults will be chosen.
 42:  */
 43: 
 44: /**
 45:  * @typedef {PretrainedOptions & ModelSpecificPretrainedOptions} PretrainedModelOptions Options for loading a pretrained model.
 46:  */
 47: 
 48: /**
 49:  * Mapping from file extensions to MIME types.
 50:  */
 51: const CONTENT_TYPE_MAP = {
 52:     'txt': 'text/plain',
 53:     'html': 'text/html',
 54:     'css': 'text/css',
 55:     'js': 'text/javascript',
 56:     'json': 'application/json',
 57:     'png': 'image/png',
 58:     'jpg': 'image/jpeg',
 59:     'jpeg': 'image/jpeg',
 60:     'gif': 'image/gif',
 61: }
 62: class FileResponse {
 63: 
 64:     /**
 65:      * Creates a new `FileResponse` object.
 66:      * @param {string} filePath
 67:      */
 68:     constructor(filePath) {
 69:         this.filePath = filePath;
 70:         this.headers = new Headers();
 71: 
 72:         this.exists = fs.existsSync(filePath);
 73:         if (this.exists) {
 74:             this.status = 200;
 75:             this.statusText = 'OK';
 76: 
 77:             let stats = fs.statSync(filePath);
 78:             this.headers.set('content-length', stats.size.toString());
 79: 
 80:             this.updateContentType();
 81: 
 82:             const stream = fs.createReadStream(filePath);
 83:             this.body = new ReadableStream({
 84:                 start(controller) {
 85:                     stream.on('data', (chunk) => controller.enqueue(chunk));
 86:                     stream.on('end', () => controller.close());
 87:                     stream.on('error', (err) => controller.error(err));
 88:                 },
 89:                 cancel() {
 90:                     stream.destroy();
 91:                 }
 92:             });
 93:         } else {
 94:             this.status = 404;
 95:             this.statusText = 'Not Found';
 96:             this.body = null;
 97:         }
 98:     }
 99: 
100:     /**
101:      * Updates the 'content-type' header property of the response based on the extension of
102:      * the file specified by the filePath property of the current object.
103:      * @returns {void}
104:      */
105:     updateContentType() {
106:         // Set content-type header based on file extension
107:         const extension = this.filePath.toString().split('.').pop().toLowerCase();
108:         this.headers.set('content-type', CONTENT_TYPE_MAP[extension] ?? 'application/octet-stream');
109:     }
110: 
111:     /**
112:      * Clone the current FileResponse object.
113:      * @returns {FileResponse} A new FileResponse object with the same properties as the current object.
114:      */
115:     clone() {
116:         let response = new FileResponse(this.filePath);
117:         response.exists = this.exists;
118:         response.status = this.status;
119:         response.statusText = this.statusText;
120:         response.headers = new Headers(this.headers);
121:         return response;
122:     }
123: 
124:     /**
125:      * Reads the contents of the file specified by the filePath property and returns a Promise that
126:      * resolves with an ArrayBuffer containing the file's contents.
127:      * @returns {Promise<ArrayBuffer>} A Promise that resolves with an ArrayBuffer containing the file's contents.
128:      * @throws {Error} If the file cannot be read.
129:      */
130:     async arrayBuffer() {
131:         const data = await fs.promises.readFile(this.filePath);
132:         return /** @type {ArrayBuffer} */ (data.buffer);
133:     }
134: 
135:     /**
136:      * Reads the contents of the file specified by the filePath property and returns a Promise that
137:      * resolves with a Blob containing the file's contents.
138:      * @returns {Promise<Blob>} A Promise that resolves with a Blob containing the file's contents.
139:      * @throws {Error} If the file cannot be read.
140:      */
141:     async blob() {
142:         const data = await fs.promises.readFile(this.filePath);
143:         return new Blob([data], { type: this.headers.get('content-type') });
144:     }
145: 
146:     /**
147:      * Reads the contents of the file specified by the filePath property and returns a Promise that
148:      * resolves with a string containing the file's contents.
149:      * @returns {Promise<string>} A Promise that resolves with a string containing the file's contents.
150:      * @throws {Error} If the file cannot be read.
151:      */
152:     async text() {
153:         const data = await fs.promises.readFile(this.filePath, 'utf8');
154:         return data;
155:     }
156: 
157:     /**
158:      * Reads the contents of the file specified by the filePath property and returns a Promise that
159:      * resolves with a parsed JavaScript object containing the file's contents.
160:      *
161:      * @returns {Promise<Object>} A Promise that resolves with a parsed JavaScript object containing the file's contents.
162:      * @throws {Error} If the file cannot be read.
163:      */
164:     async json() {
165:         return JSON.parse(await this.text());
166:     }
167: }
168: 
169: /**
170:  * Determines whether the given string is a valid URL.
171:  * @param {string|URL} string The string to test for validity as an URL.
172:  * @param {string[]} [protocols=null] A list of valid protocols. If specified, the protocol must be in this list.
173:  * @param {string[]} [validHosts=null] A list of valid hostnames. If specified, the URL's hostname must be in this list.
174:  * @returns {boolean} True if the string is a valid URL, false otherwise.
175:  */
176: function isValidUrl(string, protocols = null, validHosts = null) {
177:     let url;
178:     try {
179:         url = new URL(string);
180:     } catch (_) {
181:         return false;
182:     }
183:     if (protocols && !protocols.includes(url.protocol)) {
184:         return false;
185:     }
186:     if (validHosts && !validHosts.includes(url.hostname)) {
187:         return false;
188:     }
189:     return true;
190: }
191: 
192: const REPO_ID_REGEX = /^(\b[\w\-.]+\b\/)?\b[\w\-.]{1,96}\b$/;
193: 
194: /**
195:  * Tests whether a string is a valid Hugging Face model ID or not.
196:  * Adapted from https://github.com/huggingface/huggingface_hub/blob/6378820ebb03f071988a96c7f3268f5bdf8f9449/src/huggingface_hub/utils/_validators.py#L119-L170
197:  *
198:  * @param {string} string The string to test
199:  * @returns {boolean} True if the string is a valid model ID, false otherwise.
200:  */
201: function isValidHfModelId(string) {
202:     if (!REPO_ID_REGEX.test(string)) return false;
203:     if (string.includes("..") || string.includes("--")) return false;
204:     if (string.endsWith(".git") || string.endsWith(".ipynb")) return false;
205:     return true;
206: }
207: 
208: /**
209:  * Helper function to get a file, using either the Fetch API or FileSystem API.
210:  *
211:  * @param {URL|string} urlOrPath The URL/path of the file to get.
212:  * @returns {Promise<FileResponse|Response>} A promise that resolves to a FileResponse object (if the file is retrieved using the FileSystem API), or a Response object (if the file is retrieved using the Fetch API).
213:  */
214: export async function getFile(urlOrPath) {
215: 
216:     if (env.useFS && !isValidUrl(urlOrPath, ["http:", "https:", "blob:"])) {
217:         return new FileResponse(
218:           urlOrPath instanceof URL
219:             ? urlOrPath.protocol === "file:"
220:               ? urlOrPath.pathname
221:               : urlOrPath.toString()
222:             : urlOrPath,
223:         );
224:     } else if (typeof process !== 'undefined' && process?.release?.name === 'node') {
225:         const IS_CI = !!process.env?.TESTING_REMOTELY;
226:         const version = env.version;
227: 
228:         const headers = new Headers();
229:         headers.set('User-Agent', `transformers.js/${version}; is_ci/${IS_CI};`);
230: 
231:         // Check whether we are making a request to the Hugging Face Hub.
232:         const isHFURL = isValidUrl(urlOrPath, ['http:', 'https:'], ['huggingface.co', 'hf.co']);
233:         if (isHFURL) {
234:             // If an access token is present in the environment variables,
235:             // we add it to the request headers.
236:             // NOTE: We keep `HF_ACCESS_TOKEN` for backwards compatibility (as a fallback).
237:             const token = process.env?.HF_TOKEN ?? process.env?.HF_ACCESS_TOKEN;
238:             if (token) {
239:                 headers.set('Authorization', `Bearer ${token}`);
240:             }
241:         }
242:         return fetch(urlOrPath, { headers });
243:     } else {
244:         // Running in a browser-environment, so we use default headers
245:         // NOTE: We do not allow passing authorization headers in the browser,
246:         // since this would require exposing the token to the client.
247:         return fetch(urlOrPath);
248:     }
249: }
250: 
251: const ERROR_MAPPING = {
252:     // 4xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses)
253:     400: 'Bad request error occurred while trying to load file',
254:     401: 'Unauthorized access to file',
255:     403: 'Forbidden access to file',
256:     404: 'Could not locate file',
257:     408: 'Request timeout error occurred while trying to load file',
258: 
259:     // 5xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses)
260:     500: 'Internal server error error occurred while trying to load file',
261:     502: 'Bad gateway error occurred while trying to load file',
262:     503: 'Service unavailable error occurred while trying to load file',
263:     504: 'Gateway timeout error occurred while trying to load file',
264: }
265: /**
266:  * Helper method to handle fatal errors that occur while trying to load a file from the Hugging Face Hub.
267:  * @param {number} status The HTTP status code of the error.
268:  * @param {string} remoteURL The URL of the file that could not be loaded.
269:  * @param {boolean} fatal Whether to raise an error if the file could not be loaded.
270:  * @returns {null} Returns `null` if `fatal = true`.
271:  * @throws {Error} If `fatal = false`.
272:  */
273: function handleError(status, remoteURL, fatal) {
274:     if (!fatal) {
275:         // File was not loaded correctly, but it is optional.
276:         // TODO in future, cache the response?
277:         return null;
278:     }
279: 
280:     const message = ERROR_MAPPING[status] ?? `Error (${status}) occurred while trying to load file`;
281:     throw Error(`${message}: "${remoteURL}".`);
282: }
283: 
284: class FileCache {
285:     /**
286:      * Instantiate a `FileCache` object.
287:      * @param {string} path
288:      */
289:     constructor(path) {
290:         this.path = path;
291:     }
292: 
293:     /**
294:      * Checks whether the given request is in the cache.
295:      * @param {string} request
296:      * @returns {Promise<FileResponse | undefined>}
297:      */
298:     async match(request) {
299: 
300:         let filePath = path.join(this.path, request);
301:         let file = new FileResponse(filePath);
302: 
303:         if (file.exists) {
304:             return file;
305:         } else {
306:             return undefined;
307:         }
308:     }
309: 
310:     /**
311:      * Adds the given response to the cache.
312:      * @param {string} request
313:      * @param {Response} response
314:      * @param {(data: {progress: number, loaded: number, total: number}) => void} [progress_callback] Optional.
315:      * The function to call with progress updates
316:      * @returns {Promise<void>}
317:      */
318:     async put(request, response, progress_callback = undefined) {
319:         let filePath = path.join(this.path, request);
320: 
321:         try {
322:             const contentLength = response.headers.get('Content-Length');
323:             const total = parseInt(contentLength ?? '0');
324:             let loaded = 0;
325: 
326:             await fs.promises.mkdir(path.dirname(filePath), { recursive: true });
327:             const fileStream = fs.createWriteStream(filePath);
328:             const reader = response.body.getReader();
329: 
330:             while (true) {
331:                 const { done, value } = await reader.read();
332:                 if (done) {
333:                     break;
334:                 }
335: 
336:                 await new Promise((resolve, reject) => {
337:                     fileStream.write(value, (err) => {
338:                         if (err) {
339:                             reject(err);
340:                             return;
341:                         }
342:                         resolve();
343:                     });
344:                 });
345: 
346:                 loaded += value.length;
347:                 const progress = total ? (loaded / total) * 100 : 0;
348: 
349:                 progress_callback?.({ progress, loaded, total });
350:             }
351: 
352:             fileStream.close();
353:         } catch (error) {
354:             // Clean up the file if an error occurred during download
355:             try {
356:                 await fs.promises.unlink(filePath);
357:             } catch { }
358:             throw error;
359:         }
360:     }
361: 
362:     // TODO add the rest?
363:     // addAll(requests: RequestInfo[]): Promise<void>;
364:     // delete(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<boolean>;
365:     // keys(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Request>>;
366:     // match(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<Response | undefined>;
367:     // matchAll(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Response>>;
368: }
369: 
370: /**
371:  *
372:  * @param {FileCache|Cache} cache The cache to search
373:  * @param {string[]} names The names of the item to search for
374:  * @returns {Promise<FileResponse|Response|undefined>} The item from the cache, or undefined if not found.
375:  */
376: async function tryCache(cache, ...names) {
377:     for (let name of names) {
378:         try {
379:             let result = await cache.match(name);
380:             if (result) return result;
381:         } catch (e) {
382:             continue;
383:         }
384:     }
385:     return undefined;
386: }
387: 
388: /**
389:  * Retrieves a file from either a remote URL using the Fetch API or from the local file system using the FileSystem API.
390:  * If the filesystem is available and `env.useCache = true`, the file will be downloaded and cached.
391:  *
392:  * @param {string} path_or_repo_id This can be either:
393:  * - a string, the *model id* of a model repo on huggingface.co.
394:  * - a path to a *directory* potentially containing the file.
395:  * @param {string} filename The name of the file to locate in `path_or_repo`.
396:  * @param {boolean} [fatal=true] Whether to throw an error if the file is not found.
397:  * @param {PretrainedOptions} [options] An object containing optional parameters.
398:  * @param {boolean} [return_path=false] Whether to return the path of the file instead of the file content.
399:  *
400:  * @throws Will throw an error if the file is not found and `fatal` is true.
401:  * @returns {Promise<string|Uint8Array>} A Promise that resolves with the file content as a Uint8Array if `return_path` is false, or the file path as a string if `return_path` is true.
402:  */
403: export async function getModelFile(path_or_repo_id, filename, fatal = true, options = {}, return_path = false) {
404: 
405:     if (!env.allowLocalModels) {
406:         // User has disabled local models, so we just make sure other settings are correct.
407: 
408:         if (options.local_files_only) {
409:             throw Error("Invalid configuration detected: local models are disabled (`env.allowLocalModels=false`) but you have requested to only use local models (`local_files_only=true`).")
410:         } else if (!env.allowRemoteModels) {
411:             throw Error("Invalid configuration detected: both local and remote models are disabled. Fix by setting `env.allowLocalModels` or `env.allowRemoteModels` to `true`.")
412:         }
413:     }
414: 
415:     // Initiate file retrieval
416:     dispatchCallback(options.progress_callback, {
417:         status: 'initiate',
418:         name: path_or_repo_id,
419:         file: filename
420:     })
421: 
422:     // First, check if the a caching backend is available
423:     // If no caching mechanism available, will download the file every time
424:     let cache;
425:     if (!cache && env.useCustomCache) {
426:         // Allow the user to specify a custom cache system.
427:         if (!env.customCache) {
428:             throw Error('`env.useCustomCache=true`, but `env.customCache` is not defined.')
429:         }
430: 
431:         // Check that the required methods are defined:
432:         if (!env.customCache.match || !env.customCache.put) {
433:             throw new Error(
434:                 "`env.customCache` must be an object which implements the `match` and `put` functions of the Web Cache API. " +
435:                 "For more information, see https://developer.mozilla.org/en-US/docs/Web/API/Cache"
436:             )
437:         }
438:         cache = env.customCache;
439:     }
440: 
441:     if (!cache && env.useBrowserCache) {
442:         if (typeof caches === 'undefined') {
443:             throw Error('Browser cache is not available in this environment.')
444:         }
445:         try {
446:             // In some cases, the browser cache may be visible, but not accessible due to security restrictions.
447:             // For example, when running an application in an iframe, if a user attempts to load the page in
448:             // incognito mode, the following error is thrown: `DOMException: Failed to execute 'open' on 'CacheStorage':
449:             // An attempt was made to break through the security policy of the user agent.`
450:             // So, instead of crashing, we just ignore the error and continue without using the cache.
451:             cache = await caches.open('transformers-cache');
452:         } catch (e) {
453:             console.warn('An error occurred while opening the browser cache:', e);
454:         }
455:     }
456: 
457:     if (!cache && env.useFSCache) {
458:         if (!apis.IS_FS_AVAILABLE) {
459:             throw Error('File System Cache is not available in this environment.');
460:         }
461: 
462:         // If `cache_dir` is not specified, use the default cache directory
463:         cache = new FileCache(options.cache_dir ?? env.cacheDir);
464:     }
465: 
466:     const revision = options.revision ?? 'main';
467:     const requestURL = pathJoin(path_or_repo_id, filename);
468: 
469:     const validModelId = isValidHfModelId(path_or_repo_id);
470:     const localPath = validModelId
471:         ? pathJoin(env.localModelPath, requestURL)
472:         : requestURL;
473:     const remoteURL = pathJoin(
474:         env.remoteHost,
475:         env.remotePathTemplate
476:             .replaceAll('{model}', path_or_repo_id)
477:             .replaceAll('{revision}', encodeURIComponent(revision)),
478:         filename
479:     );
480: 
481:     /** @type {string} */
482:     let cacheKey;
483:     const proposedCacheKey = cache instanceof FileCache
484:         // Choose cache key for filesystem cache
485:         // When using the main revision (default), we use the request URL as the cache key.
486:         // If a specific revision is requested, we account for this in the cache key.
487:         ? revision === 'main' ? requestURL : pathJoin(path_or_repo_id, revision, filename)
488:         : remoteURL;
489: 
490:     // Whether to cache the final response in the end.
491:     let toCacheResponse = false;
492: 
493:     /** @type {Response|FileResponse|undefined} */
494:     let response;
495: 
496:     if (cache) {
497:         // A caching system is available, so we try to get the file from it.
498:         //  1. We first try to get from cache using the local path. In some environments (like deno),
499:         //     non-URL cache keys are not allowed. In these cases, `response` will be undefined.
500:         //  2. If no response is found, we try to get from cache using the remote URL or file system cache.
501:         response = await tryCache(cache, localPath, proposedCacheKey);
502:     }
503: 
504:     const cacheHit = response !== undefined;
505:     if (response === undefined) {
506:         // Caching not available, or file is not cached, so we perform the request
507: 
508:         if (env.allowLocalModels) {
509:             // Accessing local models is enabled, so we try to get the file locally.
510:             // If request is a valid HTTP URL, we skip the local file check. Otherwise, we try to get the file locally.
511:             const isURL = isValidUrl(requestURL, ['http:', 'https:']);
512:             if (!isURL) {
513:                 try {
514:                     response = await getFile(localPath);
515:                     cacheKey = localPath; // Update the cache key to be the local path
516:                 } catch (e) {
517:                     // Something went wrong while trying to get the file locally.
518:                     // NOTE: error handling is done in the next step (since `response` will be undefined)
519:                     console.warn(`Unable to load from local path "${localPath}": "${e}"`);
520:                 }
521:             } else if (options.local_files_only) {
522:                 throw new Error(`\`local_files_only=true\`, but attempted to load a remote file from: ${requestURL}.`);
523:             } else if (!env.allowRemoteModels) {
524:                 throw new Error(`\`env.allowRemoteModels=false\`, but attempted to load a remote file from: ${requestURL}.`);
525:             }
526:         }
527: 
528:         if (response === undefined || response.status === 404) {
529:             // File not found locally. This means either:
530:             // - The user has disabled local file access (`env.allowLocalModels=false`)
531:             // - the path is a valid HTTP url (`response === undefined`)
532:             // - the path is not a valid HTTP url and the file is not present on the file system or local server (`response.status === 404`)
533: 
534:             if (options.local_files_only || !env.allowRemoteModels) {
535:                 // User requested local files only, but the file is not found locally.
536:                 if (fatal) {
537:                     throw Error(`\`local_files_only=true\` or \`env.allowRemoteModels=false\` and file was not found locally at "${localPath}".`);
538:                 } else {
539:                     // File not found, but this file is optional.
540:                     // TODO in future, cache the response?
541:                     return null;
542:                 }
543:             }
544:             if (!validModelId) {
545:                 // Before making any requests to the remote server, we check if the model ID is valid.
546:                 // This prevents unnecessary network requests for invalid model IDs.
547:                 throw Error(`Local file missing at "${localPath}" and download aborted due to invalid model ID "${path_or_repo_id}".`);
548:             }
549: 
550:             // File not found locally, so we try to download it from the remote server
551:             response = await getFile(remoteURL);
552: 
553:             if (response.status !== 200) {
554:                 return handleError(response.status, remoteURL, fatal);
555:             }
556: 
557:             // Success! We use the proposed cache key from earlier
558:             cacheKey = proposedCacheKey;
559:         }
560: 
561:         // Only cache the response if:
562:         toCacheResponse =
563:             cache                              // 1. A caching system is available
564:             && typeof Response !== 'undefined' // 2. `Response` is defined (i.e., we are in a browser-like environment)
565:             && response instanceof Response    // 3. result is a `Response` object (i.e., not a `FileResponse`)
566:             && response.status === 200         // 4. request was successful (status code 200)
567:     }
568: 
569:     // Start downloading
570:     dispatchCallback(options.progress_callback, {
571:         status: 'download',
572:         name: path_or_repo_id,
573:         file: filename
574:     })
575: 
576:     let result;
577:     if (!(apis.IS_NODE_ENV && return_path)) {
578:         /** @type {Uint8Array} */
579:         let buffer;
580: 
581:         if (!options.progress_callback) {
582:             // If no progress callback is specified, we can use the `.arrayBuffer()`
583:             // method to read the response.
584:             buffer = new Uint8Array(await response.arrayBuffer());
585: 
586:         } else if (
587:             cacheHit // The item is being read from the cache
588:             &&
589:             typeof navigator !== 'undefined' && /firefox/i.test(navigator.userAgent) // We are in Firefox
590:         ) {
591:             // Due to bug in Firefox, we cannot display progress when loading from cache.
592:             // Fortunately, since this should be instantaneous, this should not impact users too much.
593:             buffer = new Uint8Array(await response.arrayBuffer());
594: 
595:             // For completeness, we still fire the final progress callback
596:             dispatchCallback(options.progress_callback, {
597:                 status: 'progress',
598:                 name: path_or_repo_id,
599:                 file: filename,
600:                 progress: 100,
601:                 loaded: buffer.length,
602:                 total: buffer.length,
603:             })
604:         } else {
605:             buffer = await readResponse(response, data => {
606:                 dispatchCallback(options.progress_callback, {
607:                     status: 'progress',
608:                     name: path_or_repo_id,
609:                     file: filename,
610:                     ...data,
611:                 })
612:             })
613:         }
614:         result = buffer;
615:     }
616: 
617:     if (
618:         // Only cache web responses
619:         // i.e., do not cache FileResponses (prevents duplication)
620:         toCacheResponse && cacheKey
621:         &&
622:         // Check again whether request is in cache. If not, we add the response to the cache
623:         (await cache.match(cacheKey) === undefined)
624:     ) {
625:         if (!result) {
626:             // We haven't yet read the response body, so we need to do so now.
627:             await cache.put(cacheKey, /** @type {Response} */(response), options.progress_callback);
628:         } else {
629:             // NOTE: We use `new Response(buffer, ...)` instead of `response.clone()` to handle LFS files
630:             await cache.put(cacheKey, new Response(result, {
631:                 headers: response.headers
632:             }))
633:                 .catch(err => {
634:                     // Do not crash if unable to add to cache (e.g., QuotaExceededError).
635:                     // Rather, log a warning and proceed with execution.
636:                     console.warn(`Unable to add response to browser cache: ${err}.`);
637:                 });
638:         }
639:     }
640:     dispatchCallback(options.progress_callback, {
641:         status: 'done',
642:         name: path_or_repo_id,
643:         file: filename
644:     });
645: 
646:     if (result) {
647:         if (!apis.IS_NODE_ENV && return_path) {
648:             throw new Error("Cannot return path in a browser environment.")
649:         }
650:         return result;
651:     }
652:     if (response instanceof FileResponse) {
653:         return response.filePath;
654:     }
655: 
656:     // Otherwise, return the cached response (most likely a `FileResponse`).
657:     // NOTE: A custom cache may return a Response, or a string (file path)
658:     const cachedResponse = await cache?.match(cacheKey);
659:     if (cachedResponse instanceof FileResponse) {
660:         return cachedResponse.filePath;
661:     } else if (cachedResponse instanceof Response) {
662:         return new Uint8Array(await cachedResponse.arrayBuffer());
663:     } else if (typeof cachedResponse === 'string') {
664:         return cachedResponse;
665:     }
666: 
667:     throw new Error("Unable to get model file path or buffer.");
668: }
669: 
670: /**
671:  * Fetches a text file from a given path and file name.
672:  *
673:  * @param {string} modelPath The path to the directory containing the file.
674:  * @param {string} fileName The name of the file to fetch.
675:  * @param {boolean} [fatal=true] Whether to throw an error if the file is not found.
676:  * @param {PretrainedOptions} [options] An object containing optional parameters.
677:  * @returns {Promise<string|null>} The text content of the file.
678:  * @throws Will throw an error if the file is not found and `fatal` is true.
679:  */
680: export async function getModelText(modelPath, fileName, fatal = true, options = {}) {
681:     const buffer = await getModelFile(modelPath, fileName, fatal, options, false);
682:     if (buffer === null) {
683:         return null;
684:     }
685: 
686:     const decoder = new TextDecoder('utf-8');
687:     return decoder.decode(/** @type {Uint8Array} */(buffer));
688: }
689: 
690: /**
691:  * Fetches a JSON file from a given path and file name.
692:  *
693:  * @param {string} modelPath The path to the directory containing the file.
694:  * @param {string} fileName The name of the file to fetch.
695:  * @param {boolean} [fatal=true] Whether to throw an error if the file is not found.
696:  * @param {PretrainedOptions} [options] An object containing optional parameters.
697:  * @returns {Promise<Object>} The JSON data parsed into a JavaScript object.
698:  * @throws Will throw an error if the file is not found and `fatal` is true.
699:  */
700: export async function getModelJSON(modelPath, fileName, fatal = true, options = {}) {
701:     const text = await getModelText(modelPath, fileName, fatal, options);
702:     if (text === null) {
703:         // Return empty object
704:         return {};
705:     }
706: 
707:     return JSON.parse(text);
708: }
709: /**
710:  * Read and track progress when reading a Response object
711:  *
712:  * @param {Response|FileResponse} response The Response object to read
713:  * @param {(data: {progress: number, loaded: number, total: number}) => void} progress_callback The function to call with progress updates
714:  * @returns {Promise<Uint8Array>} A Promise that resolves with the Uint8Array buffer
715:  */
716: async function readResponse(response, progress_callback) {
717: 
718:     const contentLength = response.headers.get('Content-Length');
719:     if (contentLength === null) {
720:         console.warn('Unable to determine content-length from response headers. Will expand buffer when needed.')
721:     }
722:     let total = parseInt(contentLength ?? '0');
723:     let buffer = new Uint8Array(total);
724:     let loaded = 0;
725: 
726:     const reader = response.body.getReader();
727:     async function read() {
728:         const { done, value } = await reader.read();
729:         if (done) return;
730: 
731:         const newLoaded = loaded + value.length;
732:         if (newLoaded > total) {
733:             total = newLoaded;
734: 
735:             // Adding the new data will overflow buffer.
736:             // In this case, we extend the buffer
737:             const newBuffer = new Uint8Array(total);
738: 
739:             // copy contents
740:             newBuffer.set(buffer);
741: 
742:             buffer = newBuffer;
743:         }
744:         buffer.set(value, loaded);
745:         loaded = newLoaded;
746: 
747:         const progress = (loaded / total) * 100;
748: 
749:         // Call your function here
750:         progress_callback({ progress, loaded, total });
751: 
752:         return read();
753:     }
754: 
755:     // Actually read
756:     await read();
757: 
758:     return buffer;
759: }
760: 
761: /**
762:  * Joins multiple parts of a path into a single path, while handling leading and trailing slashes.
763:  *
764:  * @param {...string} parts Multiple parts of a path.
765:  * @returns {string} A string representing the joined path.
766:  */
767: function pathJoin(...parts) {
768:     // https://stackoverflow.com/a/55142565
769:     parts = parts.map((part, index) => {
770:         if (index) {
771:             part = part.replace(new RegExp('^/'), '');
772:         }
773:         if (index !== parts.length - 1) {
774:             part = part.replace(new RegExp('/$'), '');
775:         }
776:         return part;
777:     })
778:     return parts.join('/');
779: }
</file>

<file path="src/tjs/utils/image.js">
  1: /**
  2:  * @file Helper module for image processing.
  3:  *
  4:  * These functions and classes are only used internally,
  5:  * meaning an end-user shouldn't need to access anything here.
  6:  *
  7:  * @module utils/image
  8:  */
  9: 
 10: import { isNullishDimension, saveBlob } from './core.js';
 11: import { getFile } from './hub.js';
 12: import { apis } from '../env.js';
 13: import { Tensor } from './tensor.js';
 14: 
 15: // Will be empty (or not used) if running in browser or web-worker
 16: import sharp from 'sharp';
 17: 
 18: let createCanvasFunction;
 19: let ImageDataClass;
 20: let loadImageFunction;
 21: const IS_BROWSER_OR_WEBWORKER = apis.IS_BROWSER_ENV || apis.IS_WEBWORKER_ENV;
 22: if (IS_BROWSER_OR_WEBWORKER) {
 23:     // Running in browser or web-worker
 24:     createCanvasFunction = (/** @type {number} */ width, /** @type {number} */ height) => {
 25:         if (!self.OffscreenCanvas) {
 26:             throw new Error('OffscreenCanvas not supported by this browser.');
 27:         }
 28:         return new self.OffscreenCanvas(width, height)
 29:     };
 30:     loadImageFunction = self.createImageBitmap;
 31:     ImageDataClass = self.ImageData;
 32: 
 33: } else if (sharp) {
 34:     // Running in Node.js, electron, or other non-browser environment
 35: 
 36:     loadImageFunction = async (/**@type {sharp.Sharp}*/img) => {
 37:         const metadata = await img.metadata();
 38:         const rawChannels = metadata.channels;
 39: 
 40:         const { data, info } = await img.rotate().raw().toBuffer({ resolveWithObject: true });
 41: 
 42:         const newImage = new RawImage(new Uint8ClampedArray(data), info.width, info.height, info.channels);
 43:         if (rawChannels !== undefined && rawChannels !== info.channels) {
 44:             // Make sure the new image has the same number of channels as the input image.
 45:             // This is necessary for grayscale images.
 46:             newImage.convert(rawChannels);
 47:         }
 48:         return newImage;
 49:     }
 50: 
 51: } else {
 52:     throw new Error('Unable to load image processing library.');
 53: }
 54: 
 55: 
 56: // Defined here: https://github.com/python-pillow/Pillow/blob/a405e8406b83f8bfb8916e93971edc7407b8b1ff/src/libImaging/Imaging.h#L262-L268
 57: const RESAMPLING_MAPPING = {
 58:     0: 'nearest',
 59:     1: 'lanczos',
 60:     2: 'bilinear',
 61:     3: 'bicubic',
 62:     4: 'box',
 63:     5: 'hamming',
 64: }
 65: 
 66: /**
 67:  * Mapping from file extensions to MIME types.
 68:  */
 69: const CONTENT_TYPE_MAP = new Map([
 70:     ['png', 'image/png'],
 71:     ['jpg', 'image/jpeg'],
 72:     ['jpeg', 'image/jpeg'],
 73:     ['gif', 'image/gif'],
 74: ]);
 75: 
 76: export class RawImage {
 77: 
 78:     /**
 79:      * Create a new `RawImage` object.
 80:      * @param {Uint8ClampedArray|Uint8Array} data The pixel data.
 81:      * @param {number} width The width of the image.
 82:      * @param {number} height The height of the image.
 83:      * @param {1|2|3|4} channels The number of channels.
 84:      */
 85:     constructor(data, width, height, channels) {
 86:         this.data = data;
 87:         this.width = width;
 88:         this.height = height;
 89:         this.channels = channels;
 90:     }
 91: 
 92:     /**
 93:      * Returns the size of the image (width, height).
 94:      * @returns {[number, number]} The size of the image (width, height).
 95:      */
 96:     get size() {
 97:         return [this.width, this.height];
 98:     }
 99: 
100:     /**
101:      * Helper method for reading an image from a variety of input types.
102:      * @param {RawImage|string|URL|Blob|HTMLCanvasElement|OffscreenCanvas} input
103:      * @returns The image object.
104:      *
105:      * **Example:** Read image from a URL.
106:      * ```javascript
107:      * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');
108:      * // RawImage {
109:      * //   "data": Uint8ClampedArray [ 25, 25, 25, 19, 19, 19, ... ],
110:      * //   "width": 800,
111:      * //   "height": 533,
112:      * //   "channels": 3
113:      * // }
114:      * ```
115:      */
116:     static async read(input) {
117:         if (input instanceof RawImage) {
118:             return input;
119:         } else if (typeof input === 'string' || input instanceof URL) {
120:             return await this.fromURL(input);
121:         } else if (input instanceof Blob) {
122:             return await this.fromBlob(input);
123:         } else if (
124:             (typeof HTMLCanvasElement !== "undefined" && input instanceof HTMLCanvasElement)
125:             ||
126:             (typeof OffscreenCanvas !== "undefined" && input instanceof OffscreenCanvas)
127:         ) {
128:             return this.fromCanvas(input);
129:         } else {
130:             throw new Error(`Unsupported input type: ${typeof input}`);
131:         }
132:     }
133: 
134:     /**
135:      * Read an image from a canvas.
136:      * @param {HTMLCanvasElement|OffscreenCanvas} canvas The canvas to read the image from.
137:      * @returns {RawImage} The image object.
138:      */
139:     static fromCanvas(canvas) {
140:         if (!IS_BROWSER_OR_WEBWORKER) {
141:             throw new Error('fromCanvas() is only supported in browser environments.')
142:         }
143: 
144:         const ctx = /** @type {CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D} */ (canvas.getContext('2d'));
145:         const data = ctx.getImageData(0, 0, canvas.width, canvas.height).data;
146:         return new RawImage(data, canvas.width, canvas.height, 4);
147:     }
148: 
149:     /**
150:      * Read an image from a URL or file path.
151:      * @param {string|URL} url The URL or file path to read the image from.
152:      * @returns {Promise<RawImage>} The image object.
153:      */
154:     static async fromURL(url) {
155:         const response = await getFile(url);
156:         if (response.status !== 200) {
157:             throw new Error(`Unable to read image from "${url}" (${response.status} ${response.statusText})`);
158:         }
159:         const blob = await response.blob();
160:         return this.fromBlob(blob);
161:     }
162: 
163:     /**
164:      * Helper method to create a new Image from a blob.
165:      * @param {Blob} blob The blob to read the image from.
166:      * @returns {Promise<RawImage>} The image object.
167:      */
168:     static async fromBlob(blob) {
169:         if (IS_BROWSER_OR_WEBWORKER) {
170:             // Running in environment with canvas
171:             const img = await loadImageFunction(blob);
172: 
173:             const ctx = createCanvasFunction(img.width, img.height).getContext('2d');
174: 
175:             // Draw image to context
176:             ctx.drawImage(img, 0, 0);
177: 
178:             return new this(ctx.getImageData(0, 0, img.width, img.height).data, img.width, img.height, 4);
179: 
180:         } else {
181:             // Use sharp.js to read (and possible resize) the image.
182:             const img = sharp(await blob.arrayBuffer());
183: 
184:             return await loadImageFunction(img);
185:         }
186:     }
187: 
188:     /**
189:      * Helper method to create a new Image from a tensor
190:      * @param {Tensor} tensor
191:      */
192:     static fromTensor(tensor, channel_format = 'CHW') {
193:         if (tensor.dims.length !== 3) {
194:             throw new Error(`Tensor should have 3 dimensions, but has ${tensor.dims.length} dimensions.`);
195:         }
196: 
197:         if (channel_format === 'CHW') {
198:             tensor = tensor.transpose(1, 2, 0);
199:         } else if (channel_format === 'HWC') {
200:             // Do nothing
201:         } else {
202:             throw new Error(`Unsupported channel format: ${channel_format}`);
203:         }
204:         if (!(tensor.data instanceof Uint8ClampedArray || tensor.data instanceof Uint8Array)) {
205:             throw new Error(`Unsupported tensor type: ${tensor.type}`);
206:         }
207:         switch (tensor.dims[2]) {
208:             case 1:
209:             case 2:
210:             case 3:
211:             case 4:
212:                 return new RawImage(tensor.data, tensor.dims[1], tensor.dims[0], tensor.dims[2]);
213:             default:
214:                 throw new Error(`Unsupported number of channels: ${tensor.dims[2]}`);
215:         }
216:     }
217: 
218:     /**
219:      * Convert the image to grayscale format.
220:      * @returns {RawImage} `this` to support chaining.
221:      */
222:     grayscale() {
223:         if (this.channels === 1) {
224:             return this;
225:         }
226: 
227:         const newData = new Uint8ClampedArray(this.width * this.height * 1);
228:         switch (this.channels) {
229:             case 3: // rgb to grayscale
230:             case 4: // rgba to grayscale
231:                 for (let i = 0, offset = 0; i < this.data.length; i += this.channels) {
232:                     const red = this.data[i];
233:                     const green = this.data[i + 1];
234:                     const blue = this.data[i + 2];
235: 
236:                     newData[offset++] = Math.round(0.2989 * red + 0.5870 * green + 0.1140 * blue);
237:                 }
238:                 break;
239:             default:
240:                 throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
241:         }
242:         return this._update(newData, this.width, this.height, 1);
243:     }
244: 
245:     /**
246:      * Convert the image to RGB format.
247:      * @returns {RawImage} `this` to support chaining.
248:      */
249:     rgb() {
250:         if (this.channels === 3) {
251:             return this;
252:         }
253: 
254:         const newData = new Uint8ClampedArray(this.width * this.height * 3);
255: 
256:         switch (this.channels) {
257:             case 1: // grayscale to rgb
258:                 for (let i = 0, offset = 0; i < this.data.length; ++i) {
259:                     newData[offset++] = this.data[i];
260:                     newData[offset++] = this.data[i];
261:                     newData[offset++] = this.data[i];
262:                 }
263:                 break;
264:             case 4: // rgba to rgb
265:                 for (let i = 0, offset = 0; i < this.data.length; i += 4) {
266:                     newData[offset++] = this.data[i];
267:                     newData[offset++] = this.data[i + 1];
268:                     newData[offset++] = this.data[i + 2];
269:                 }
270:                 break;
271:             default:
272:                 throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
273:         }
274:         return this._update(newData, this.width, this.height, 3);
275: 
276:     }
277: 
278:     /**
279:      * Convert the image to RGBA format.
280:      * @returns {RawImage} `this` to support chaining.
281:      */
282:     rgba() {
283:         if (this.channels === 4) {
284:             return this;
285:         }
286: 
287:         const newData = new Uint8ClampedArray(this.width * this.height * 4);
288: 
289:         switch (this.channels) {
290:             case 1: // grayscale to rgba
291:                 for (let i = 0, offset = 0; i < this.data.length; ++i) {
292:                     newData[offset++] = this.data[i];
293:                     newData[offset++] = this.data[i];
294:                     newData[offset++] = this.data[i];
295:                     newData[offset++] = 255;
296:                 }
297:                 break;
298:             case 3: // rgb to rgba
299:                 for (let i = 0, offset = 0; i < this.data.length; i += 3) {
300:                     newData[offset++] = this.data[i];
301:                     newData[offset++] = this.data[i + 1];
302:                     newData[offset++] = this.data[i + 2];
303:                     newData[offset++] = 255;
304:                 }
305:                 break;
306:             default:
307:                 throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
308:         }
309: 
310:         return this._update(newData, this.width, this.height, 4);
311:     }
312: 
313:     /**
314:      * Apply an alpha mask to the image. Operates in place.
315:      * @param {RawImage} mask The mask to apply. It should have a single channel.
316:      * @returns {RawImage} The masked image.
317:      * @throws {Error} If the mask is not the same size as the image.
318:      * @throws {Error} If the image does not have 4 channels.
319:      * @throws {Error} If the mask is not a single channel.
320:      */
321:     putAlpha(mask) {
322:         if (mask.width !== this.width || mask.height !== this.height) {
323:             throw new Error(`Expected mask size to be ${this.width}x${this.height}, but got ${mask.width}x${mask.height}`);
324:         }
325:         if (mask.channels !== 1) {
326:             throw new Error(`Expected mask to have 1 channel, but got ${mask.channels}`);
327:         }
328: 
329:         const this_data = this.data;
330:         const mask_data = mask.data;
331:         const num_pixels = this.width * this.height;
332:         if (this.channels === 3) {
333:             // Convert to RGBA and simultaneously apply mask to alpha channel
334:             const newData = new Uint8ClampedArray(num_pixels * 4);
335:             for (let i = 0, in_offset = 0, out_offset = 0; i < num_pixels; ++i) {
336:                 newData[out_offset++] = this_data[in_offset++];
337:                 newData[out_offset++] = this_data[in_offset++];
338:                 newData[out_offset++] = this_data[in_offset++];
339:                 newData[out_offset++] = mask_data[i];
340:             }
341:             return this._update(newData, this.width, this.height, 4);
342: 
343:         } else if (this.channels === 4) {
344:             // Apply mask to alpha channel in place
345:             for (let i = 0; i < num_pixels; ++i) {
346:                 this_data[4 * i + 3] = mask_data[i];
347:             }
348:             return this;
349:         }
350:         throw new Error(`Expected image to have 3 or 4 channels, but got ${this.channels}`);
351:     }
352: 
353:     /**
354:      * Resize the image to the given dimensions. This method uses the canvas API to perform the resizing.
355:      * @param {number} width The width of the new image. `null` or `-1` will preserve the aspect ratio.
356:      * @param {number} height The height of the new image. `null` or `-1` will preserve the aspect ratio.
357:      * @param {Object} options Additional options for resizing.
358:      * @param {0|1|2|3|4|5|string} [options.resample] The resampling method to use.
359:      * @returns {Promise<RawImage>} `this` to support chaining.
360:      */
361:     async resize(width, height, {
362:         resample = 2,
363:     } = {}) {
364: 
365:         // Do nothing if the image already has the desired size
366:         if (this.width === width && this.height === height) {
367:             return this;
368:         }
369: 
370:         // Ensure resample method is a string
371:         let resampleMethod = RESAMPLING_MAPPING[resample] ?? resample;
372: 
373:         // Calculate width / height to maintain aspect ratio, in the event that
374:         // the user passed a null value in.
375:         // This allows users to pass in something like `resize(320, null)` to
376:         // resize to 320 width, but maintain aspect ratio.
377:         const nullish_width = isNullishDimension(width);
378:         const nullish_height = isNullishDimension(height);
379:         if (nullish_width && nullish_height) {
380:             return this;
381:         } else if (nullish_width) {
382:             width = (height / this.height) * this.width;
383:         } else if (nullish_height) {
384:             height = (width / this.width) * this.height;
385:         }
386: 
387:         if (IS_BROWSER_OR_WEBWORKER) {
388:             // TODO use `resample` in browser environment
389: 
390:             // Store number of channels before resizing
391:             const numChannels = this.channels;
392: 
393:             // Create canvas object for this image
394:             const canvas = this.toCanvas();
395: 
396:             // Actually perform resizing using the canvas API
397:             const ctx = createCanvasFunction(width, height).getContext('2d');
398: 
399:             // Draw image to context, resizing in the process
400:             ctx.drawImage(canvas, 0, 0, width, height);
401: 
402:             // Create image from the resized data
403:             const resizedImage = new RawImage(ctx.getImageData(0, 0, width, height).data, width, height, 4);
404: 
405:             // Convert back so that image has the same number of channels as before
406:             return resizedImage.convert(numChannels);
407: 
408:         } else {
409:             // Create sharp image from raw data, and resize
410:             let img = this.toSharp();
411: 
412:             switch (resampleMethod) {
413:                 case 'box':
414:                 case 'hamming':
415:                     if (resampleMethod === 'box' || resampleMethod === 'hamming') {
416:                         console.warn(`Resampling method ${resampleMethod} is not yet supported. Using bilinear instead.`);
417:                         resampleMethod = 'bilinear';
418:                     }
419: 
420:                 case 'nearest':
421:                 case 'bilinear':
422:                 case 'bicubic':
423:                     // Perform resizing using affine transform.
424:                     // This matches how the python Pillow library does it.
425:                     img = img.affine([width / this.width, 0, 0, height / this.height], {
426:                         interpolator: resampleMethod
427:                     });
428:                     break;
429: 
430:                 case 'lanczos':
431:                     // https://github.com/python-pillow/Pillow/discussions/5519
432:                     // https://github.com/lovell/sharp/blob/main/docs/api-resize.md
433:                     img = img.resize({
434:                         width, height,
435:                         fit: 'fill',
436:                         kernel: 'lanczos3', // PIL Lanczos uses a kernel size of 3
437:                     });
438:                     break;
439: 
440:                 default:
441:                     throw new Error(`Resampling method ${resampleMethod} is not supported.`);
442:             }
443: 
444:             return await loadImageFunction(img);
445:         }
446: 
447:     }
448: 
449:     async pad([left, right, top, bottom]) {
450:         left = Math.max(left, 0);
451:         right = Math.max(right, 0);
452:         top = Math.max(top, 0);
453:         bottom = Math.max(bottom, 0);
454: 
455:         if (left === 0 && right === 0 && top === 0 && bottom === 0) {
456:             // No padding needed
457:             return this;
458:         }
459: 
460:         if (IS_BROWSER_OR_WEBWORKER) {
461:             // Store number of channels before padding
462:             const numChannels = this.channels;
463: 
464:             // Create canvas object for this image
465:             const canvas = this.toCanvas();
466: 
467:             const newWidth = this.width + left + right;
468:             const newHeight = this.height + top + bottom;
469: 
470:             // Create a new canvas of the desired size.
471:             const ctx = createCanvasFunction(newWidth, newHeight).getContext('2d');
472: 
473:             // Draw image to context, padding in the process
474:             ctx.drawImage(canvas,
475:                 0, 0, this.width, this.height,
476:                 left, top, this.width, this.height
477:             );
478: 
479:             // Create image from the padded data
480:             const paddedImage = new RawImage(
481:                 ctx.getImageData(0, 0, newWidth, newHeight).data,
482:                 newWidth, newHeight, 4
483:             );
484: 
485:             // Convert back so that image has the same number of channels as before
486:             return paddedImage.convert(numChannels);
487: 
488:         } else {
489:             const img = this.toSharp().extend({ left, right, top, bottom });
490:             return await loadImageFunction(img);
491:         }
492:     }
493: 
494:     async crop([x_min, y_min, x_max, y_max]) {
495:         // Ensure crop bounds are within the image
496:         x_min = Math.max(x_min, 0);
497:         y_min = Math.max(y_min, 0);
498:         x_max = Math.min(x_max, this.width - 1);
499:         y_max = Math.min(y_max, this.height - 1);
500: 
501:         // Do nothing if the crop is the entire image
502:         if (x_min === 0 && y_min === 0 && x_max === this.width - 1 && y_max === this.height - 1) {
503:             return this;
504:         }
505: 
506:         const crop_width = x_max - x_min + 1;
507:         const crop_height = y_max - y_min + 1;
508: 
509:         if (IS_BROWSER_OR_WEBWORKER) {
510:             // Store number of channels before resizing
511:             const numChannels = this.channels;
512: 
513:             // Create canvas object for this image
514:             const canvas = this.toCanvas();
515: 
516:             // Create a new canvas of the desired size. This is needed since if the
517:             // image is too small, we need to pad it with black pixels.
518:             const ctx = createCanvasFunction(crop_width, crop_height).getContext('2d');
519: 
520:             // Draw image to context, cropping in the process
521:             ctx.drawImage(canvas,
522:                 x_min, y_min, crop_width, crop_height,
523:                 0, 0, crop_width, crop_height
524:             );
525: 
526:             // Create image from the resized data
527:             const resizedImage = new RawImage(ctx.getImageData(0, 0, crop_width, crop_height).data, crop_width, crop_height, 4);
528: 
529:             // Convert back so that image has the same number of channels as before
530:             return resizedImage.convert(numChannels);
531: 
532:         } else {
533:             // Create sharp image from raw data
534:             const img = this.toSharp().extract({
535:                 left: x_min,
536:                 top: y_min,
537:                 width: crop_width,
538:                 height: crop_height,
539:             });
540: 
541:             return await loadImageFunction(img);
542:         }
543: 
544:     }
545: 
546:     async center_crop(crop_width, crop_height) {
547:         // If the image is already the desired size, return it
548:         if (this.width === crop_width && this.height === crop_height) {
549:             return this;
550:         }
551: 
552:         // Determine bounds of the image in the new canvas
553:         const width_offset = (this.width - crop_width) / 2;
554:         const height_offset = (this.height - crop_height) / 2;
555: 
556: 
557:         if (IS_BROWSER_OR_WEBWORKER) {
558:             // Store number of channels before resizing
559:             const numChannels = this.channels;
560: 
561:             // Create canvas object for this image
562:             const canvas = this.toCanvas();
563: 
564:             // Create a new canvas of the desired size. This is needed since if the
565:             // image is too small, we need to pad it with black pixels.
566:             const ctx = createCanvasFunction(crop_width, crop_height).getContext('2d');
567: 
568:             let sourceX = 0;
569:             let sourceY = 0;
570:             let destX = 0;
571:             let destY = 0;
572: 
573:             if (width_offset >= 0) {
574:                 sourceX = width_offset;
575:             } else {
576:                 destX = -width_offset;
577:             }
578: 
579:             if (height_offset >= 0) {
580:                 sourceY = height_offset;
581:             } else {
582:                 destY = -height_offset;
583:             }
584: 
585:             // Draw image to context, cropping in the process
586:             ctx.drawImage(canvas,
587:                 sourceX, sourceY, crop_width, crop_height,
588:                 destX, destY, crop_width, crop_height
589:             );
590: 
591:             // Create image from the resized data
592:             const resizedImage = new RawImage(ctx.getImageData(0, 0, crop_width, crop_height).data, crop_width, crop_height, 4);
593: 
594:             // Convert back so that image has the same number of channels as before
595:             return resizedImage.convert(numChannels);
596: 
597:         } else {
598:             // Create sharp image from raw data
599:             let img = this.toSharp();
600: 
601:             if (width_offset >= 0 && height_offset >= 0) {
602:                 // Cropped image lies entirely within the original image
603:                 img = img.extract({
604:                     left: Math.floor(width_offset),
605:                     top: Math.floor(height_offset),
606:                     width: crop_width,
607:                     height: crop_height,
608:                 })
609:             } else if (width_offset <= 0 && height_offset <= 0) {
610:                 // Cropped image lies entirely outside the original image,
611:                 // so we add padding
612:                 const top = Math.floor(-height_offset);
613:                 const left = Math.floor(-width_offset);
614:                 img = img.extend({
615:                     top: top,
616:                     left: left,
617: 
618:                     // Ensures the resulting image has the desired dimensions
619:                     right: crop_width - this.width - left,
620:                     bottom: crop_height - this.height - top,
621:                 });
622:             } else {
623:                 // Cropped image lies partially outside the original image.
624:                 // We first pad, then crop.
625: 
626:                 let y_padding = [0, 0];
627:                 let y_extract = 0;
628:                 if (height_offset < 0) {
629:                     y_padding[0] = Math.floor(-height_offset);
630:                     y_padding[1] = crop_height - this.height - y_padding[0];
631:                 } else {
632:                     y_extract = Math.floor(height_offset);
633:                 }
634: 
635:                 let x_padding = [0, 0];
636:                 let x_extract = 0;
637:                 if (width_offset < 0) {
638:                     x_padding[0] = Math.floor(-width_offset);
639:                     x_padding[1] = crop_width - this.width - x_padding[0];
640:                 } else {
641:                     x_extract = Math.floor(width_offset);
642:                 }
643: 
644:                 img = img.extend({
645:                     top: y_padding[0],
646:                     bottom: y_padding[1],
647:                     left: x_padding[0],
648:                     right: x_padding[1],
649:                 }).extract({
650:                     left: x_extract,
651:                     top: y_extract,
652:                     width: crop_width,
653:                     height: crop_height,
654:                 })
655:             }
656: 
657:             return await loadImageFunction(img);
658:         }
659:     }
660: 
661:     async toBlob(type = 'image/png', quality = 1) {
662:         if (!IS_BROWSER_OR_WEBWORKER) {
663:             throw new Error('toBlob() is only supported in browser environments.')
664:         }
665: 
666:         const canvas = this.toCanvas();
667:         return await canvas.convertToBlob({ type, quality });
668:     }
669: 
670:     toTensor(channel_format = 'CHW') {
671:         let tensor = new Tensor(
672:             'uint8',
673:             new Uint8Array(this.data),
674:             [this.height, this.width, this.channels]
675:         );
676: 
677:         if (channel_format === 'HWC') {
678:             // Do nothing
679:         } else if (channel_format === 'CHW') { // hwc -> chw
680:             tensor = tensor.permute(2, 0, 1);
681:         } else {
682:             throw new Error(`Unsupported channel format: ${channel_format}`);
683:         }
684:         return tensor;
685:     }
686: 
687:     toCanvas() {
688:         if (!IS_BROWSER_OR_WEBWORKER) {
689:             throw new Error('toCanvas() is only supported in browser environments.')
690:         }
691: 
692:         // Clone, and convert data to RGBA before drawing to canvas.
693:         // This is because the canvas API only supports RGBA
694:         const cloned = this.clone().rgba();
695: 
696:         // Create canvas object for the cloned image
697:         const clonedCanvas = createCanvasFunction(cloned.width, cloned.height);
698: 
699:         // Draw image to context
700:         const data = new ImageDataClass(cloned.data, cloned.width, cloned.height);
701:         clonedCanvas.getContext('2d').putImageData(data, 0, 0);
702: 
703:         return clonedCanvas;
704:     }
705: 
706:     /**
707:      * Split this image into individual bands. This method returns an array of individual image bands from an image.
708:      * For example, splitting an "RGB" image creates three new images each containing a copy of one of the original bands (red, green, blue).
709:      * 
710:      * Inspired by PIL's `Image.split()` [function](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.split).
711:      * @returns {RawImage[]} An array containing bands.
712:      */
713:     split() {
714:         const { data, width, height, channels } = this;
715: 
716:         /** @type {typeof Uint8Array | typeof Uint8ClampedArray} */
717:         const data_type = /** @type {any} */(data.constructor);
718:         const per_channel_length = data.length / channels;
719: 
720:         // Pre-allocate buffers for each channel
721:         const split_data = Array.from(
722:             { length: channels },
723:             () => new data_type(per_channel_length),
724:         );
725: 
726:         // Write pixel data
727:         for (let i = 0; i < per_channel_length; ++i) {
728:             const data_offset = channels * i;
729:             for (let j = 0; j < channels; ++j) {
730:                 split_data[j][i] = data[data_offset + j];
731:             }
732:         }
733:         return split_data.map((data) => new RawImage(data, width, height, 1));
734:     }
735: 
736:     /**
737:      * Helper method to update the image data.
738:      * @param {Uint8ClampedArray} data The new image data.
739:      * @param {number} width The new width of the image.
740:      * @param {number} height The new height of the image.
741:      * @param {1|2|3|4|null} [channels] The new number of channels of the image.
742:      * @private
743:      */
744:     _update(data, width, height, channels = null) {
745:         this.data = data;
746:         this.width = width;
747:         this.height = height;
748:         if (channels !== null) {
749:             this.channels = channels;
750:         }
751:         return this;
752:     }
753: 
754:     /**
755:      * Clone the image
756:      * @returns {RawImage} The cloned image
757:      */
758:     clone() {
759:         return new RawImage(this.data.slice(), this.width, this.height, this.channels);
760:     }
761: 
762:     /**
763:      * Helper method for converting image to have a certain number of channels
764:      * @param {number} numChannels The number of channels. Must be 1, 3, or 4.
765:      * @returns {RawImage} `this` to support chaining.
766:      */
767:     convert(numChannels) {
768:         if (this.channels === numChannels) return this; // Already correct number of channels
769: 
770:         switch (numChannels) {
771:             case 1:
772:                 this.grayscale();
773:                 break;
774:             case 3:
775:                 this.rgb();
776:                 break;
777:             case 4:
778:                 this.rgba();
779:                 break;
780:             default:
781:                 throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
782:         }
783:         return this;
784:     }
785: 
786:     /**
787:      * Save the image to the given path.
788:      * @param {string} path The path to save the image to.
789:      */
790:     async save(path) {
791: 
792:         if (IS_BROWSER_OR_WEBWORKER) {
793:             if (apis.IS_WEBWORKER_ENV) {
794:                 throw new Error('Unable to save an image from a Web Worker.')
795:             }
796: 
797:             const extension = path.split('.').pop().toLowerCase();
798:             const mime = CONTENT_TYPE_MAP.get(extension) ?? 'image/png';
799: 
800:             // Convert image to Blob
801:             const blob = await this.toBlob(mime);
802: 
803:             saveBlob(path, blob)
804: 
805:         } else if (!apis.IS_FS_AVAILABLE) {
806:             throw new Error('Unable to save the image because filesystem is disabled in this environment.')
807: 
808:         } else {
809:             const img = this.toSharp();
810:             return await img.toFile(path);
811:         }
812:     }
813: 
814:     toSharp() {
815:         if (IS_BROWSER_OR_WEBWORKER) {
816:             throw new Error('toSharp() is only supported in server-side environments.')
817:         }
818: 
819:         return sharp(this.data, {
820:             raw: {
821:                 width: this.width,
822:                 height: this.height,
823:                 channels: this.channels
824:             }
825:         });
826:     }
827: }
828: 
829: /**
830:  * Helper function to load an image from a URL, path, etc.
831:  */
832: export const load_image = RawImage.read.bind(RawImage);
</file>

<file path="src/tjs/utils/maths.js">
   1: /**
   2:  * @file Helper module for mathematical processing. 
   3:  * 
   4:  * These functions and classes are only used internally, 
   5:  * meaning an end-user shouldn't need to access anything here.
   6:  * 
   7:  * @module utils/maths
   8:  */
   9: 
  10: /**
  11:  * @typedef {Int8Array | Uint8Array | Uint8ClampedArray | Int16Array | Uint16Array | Int32Array | Uint32Array | Float16Array | Float32Array | Float64Array} TypedArray
  12:  * @typedef {BigInt64Array | BigUint64Array} BigTypedArray
  13:  * @typedef {TypedArray | BigTypedArray} AnyTypedArray
  14:  */
  15: 
  16: /**
  17:  * @param {TypedArray} input
  18:  */
  19: export function interpolate_data(input, [in_channels, in_height, in_width], [out_height, out_width], mode = 'bilinear', align_corners = false) {
  20:     // TODO use mode and align_corners
  21: 
  22:     // Output image dimensions
  23:     const x_scale = out_width / in_width;
  24:     const y_scale = out_height / in_height;
  25: 
  26:     // Output image
  27:     // @ts-ignore
  28:     const out_img = new input.constructor(out_height * out_width * in_channels);
  29: 
  30:     // Pre-calculate strides
  31:     const inStride = in_height * in_width;
  32:     const outStride = out_height * out_width;
  33: 
  34:     for (let i = 0; i < out_height; ++i) {
  35:         for (let j = 0; j < out_width; ++j) {
  36:             // Calculate output offset
  37:             const outOffset = i * out_width + j;
  38: 
  39:             // Calculate input pixel coordinates
  40:             const x = (j + 0.5) / x_scale - 0.5;
  41:             const y = (i + 0.5) / y_scale - 0.5;
  42: 
  43:             // Calculate the four nearest input pixels
  44:             // We also check if the input pixel coordinates are within the image bounds
  45:             let x1 = Math.floor(x);
  46:             let y1 = Math.floor(y);
  47:             const x2 = Math.min(x1 + 1, in_width - 1);
  48:             const y2 = Math.min(y1 + 1, in_height - 1);
  49: 
  50:             x1 = Math.max(x1, 0);
  51:             y1 = Math.max(y1, 0);
  52: 
  53: 
  54:             // Calculate the fractional distances between the input pixel and the four nearest pixels
  55:             const s = x - x1;
  56:             const t = y - y1;
  57: 
  58:             // Perform bilinear interpolation
  59:             const w1 = (1 - s) * (1 - t);
  60:             const w2 = s * (1 - t);
  61:             const w3 = (1 - s) * t;
  62:             const w4 = s * t;
  63: 
  64:             // Calculate the four nearest input pixel indices
  65:             const yStride = y1 * in_width;
  66:             const xStride = y2 * in_width;
  67:             const idx1 = yStride + x1;
  68:             const idx2 = yStride + x2;
  69:             const idx3 = xStride + x1;
  70:             const idx4 = xStride + x2;
  71: 
  72:             for (let k = 0; k < in_channels; ++k) {
  73:                 // Calculate channel offset
  74:                 const cOffset = k * inStride;
  75: 
  76:                 out_img[k * outStride + outOffset] =
  77:                     w1 * input[cOffset + idx1] +
  78:                     w2 * input[cOffset + idx2] +
  79:                     w3 * input[cOffset + idx3] +
  80:                     w4 * input[cOffset + idx4];
  81:             }
  82:         }
  83:     }
  84: 
  85:     return out_img;
  86: }
  87: 
  88: 
  89: /**
  90:  * Helper method to permute a `AnyTypedArray` directly
  91:  * @template {AnyTypedArray} T 
  92:  * @param {T} array 
  93:  * @param {number[]} dims 
  94:  * @param {number[]} axes 
  95:  * @returns {[T, number[]]} The permuted array and the new shape.
  96:  */
  97: export function permute_data(array, dims, axes) {
  98:     // Calculate the new shape of the permuted array
  99:     // and the stride of the original array
 100:     const shape = new Array(axes.length);
 101:     const stride = new Array(axes.length);
 102: 
 103:     for (let i = axes.length - 1, s = 1; i >= 0; --i) {
 104:         stride[i] = s;
 105:         shape[i] = dims[axes[i]];
 106:         s *= shape[i];
 107:     }
 108: 
 109:     // Precompute inverse mapping of stride
 110:     const invStride = axes.map((_, i) => stride[axes.indexOf(i)]);
 111: 
 112:     // Create the permuted array with the new shape
 113:     // @ts-ignore
 114:     const permutedData = new array.constructor(array.length);
 115: 
 116:     // Permute the original array to the new array
 117:     for (let i = 0; i < array.length; ++i) {
 118:         let newIndex = 0;
 119:         for (let j = dims.length - 1, k = i; j >= 0; --j) {
 120:             newIndex += (k % dims[j]) * invStride[j];
 121:             k = Math.floor(k / dims[j]);
 122:         }
 123:         permutedData[newIndex] = array[i];
 124:     }
 125: 
 126:     return [permutedData, shape];
 127: }
 128: 
 129: 
 130: /**
 131:  * Compute the softmax of an array of numbers.
 132:  * @template {TypedArray|number[]} T
 133:  * @param {T} arr The array of numbers to compute the softmax of.
 134:  * @returns {T} The softmax array.
 135:  */
 136: export function softmax(arr) {
 137:     // Compute the maximum value in the array
 138:     const maxVal = max(arr)[0];
 139: 
 140:     // Compute the exponentials of the array values
 141:     const exps = arr.map(x => Math.exp(x - maxVal));
 142: 
 143:     // Compute the sum of the exponentials
 144:     // @ts-ignore
 145:     const sumExps = exps.reduce((acc, val) => acc + val, 0);
 146: 
 147:     // Compute the softmax values
 148:     const softmaxArr = exps.map(x => x / sumExps);
 149: 
 150:     return /** @type {T} */(softmaxArr);
 151: }
 152: 
 153: /**
 154:  * Calculates the logarithm of the softmax function for the input array.
 155:  * @template {TypedArray|number[]} T
 156:  * @param {T} arr The input array to calculate the log_softmax function for.
 157:  * @returns {T} The resulting log_softmax array.
 158:  */
 159: export function log_softmax(arr) {
 160:     // Compute the maximum value in the array
 161:     const maxVal = max(arr)[0];
 162: 
 163:     // Compute the sum of the exponentials
 164:     let sumExps = 0;
 165:     for(let i = 0; i < arr.length; ++i) {
 166:         sumExps += Math.exp(arr[i] - maxVal);
 167:     }
 168: 
 169:     // Compute the log of the sum
 170:     const logSum = Math.log(sumExps);
 171: 
 172:     // Compute the softmax values
 173:     const logSoftmaxArr = arr.map(x => x - maxVal - logSum);
 174: 
 175:     return /** @type {T} */(logSoftmaxArr);
 176: }
 177: 
 178: /**
 179:  * Calculates the dot product of two arrays.
 180:  * @param {number[]} arr1 The first array.
 181:  * @param {number[]} arr2 The second array.
 182:  * @returns {number} The dot product of arr1 and arr2.
 183:  */
 184: export function dot(arr1, arr2) {
 185:     let result = 0;
 186:     for (let i = 0; i < arr1.length; ++i) {
 187:         result += arr1[i] * arr2[i];
 188:     }
 189:     return result;
 190: }
 191: 
 192: /**
 193:  * Computes the cosine similarity between two arrays.
 194:  *
 195:  * @param {number[]} arr1 The first array.
 196:  * @param {number[]} arr2 The second array.
 197:  * @returns {number} The cosine similarity between the two arrays.
 198:  */
 199: export function cos_sim(arr1, arr2) {
 200:     // Calculate dot product of the two arrays
 201:     const dotProduct = dot(arr1, arr2);
 202: 
 203:     // Calculate the magnitude of the first array
 204:     const magnitudeA = magnitude(arr1);
 205: 
 206:     // Calculate the magnitude of the second array
 207:     const magnitudeB = magnitude(arr2);
 208: 
 209:     // Calculate the cosine similarity
 210:     const cosineSimilarity = dotProduct / (magnitudeA * magnitudeB);
 211: 
 212:     return cosineSimilarity;
 213: }
 214: 
 215: /**
 216:  * Calculates the magnitude of a given array.
 217:  * @param {number[]} arr The array to calculate the magnitude of.
 218:  * @returns {number} The magnitude of the array.
 219:  */
 220: export function magnitude(arr) {
 221:     return Math.sqrt(arr.reduce((acc, val) => acc + val * val, 0));
 222: }
 223: 
 224: 
 225: /**
 226:  * Returns the value and index of the minimum element in an array.
 227:  * @template {number[]|bigint[]|AnyTypedArray} T
 228:  * @param {T} arr array of numbers.
 229:  * @returns {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} the value and index of the minimum element, of the form: [valueOfMin, indexOfMin]
 230:  * @throws {Error} If array is empty.
 231:  */
 232: export function min(arr) {
 233:     if (arr.length === 0) throw Error('Array must not be empty');
 234:     let min = arr[0];
 235:     let indexOfMin = 0;
 236:     for (let i = 1; i < arr.length; ++i) {
 237:         if (arr[i] < min) {
 238:             min = arr[i];
 239:             indexOfMin = i;
 240:         }
 241:     }
 242:     return /** @type {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} */([min, indexOfMin]);
 243: }
 244: 
 245: 
 246: /**
 247:  * Returns the value and index of the maximum element in an array.
 248:  * @template {number[]|bigint[]|AnyTypedArray} T
 249:  * @param {T} arr array of numbers.
 250:  * @returns {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} the value and index of the maximum element, of the form: [valueOfMax, indexOfMax]
 251:  * @throws {Error} If array is empty.
 252:  */
 253: export function max(arr) {
 254:     if (arr.length === 0) throw Error('Array must not be empty');
 255:     let max = arr[0];
 256:     let indexOfMax = 0;
 257:     for (let i = 1; i < arr.length; ++i) {
 258:         if (arr[i] > max) {
 259:             max = arr[i];
 260:             indexOfMax = i;
 261:         }
 262:     }
 263:     return /** @type {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} */([max, indexOfMax]);
 264: }
 265: 
 266: function isPowerOfTwo(number) {
 267:     // Check if the number is greater than 0 and has only one bit set to 1
 268:     return (number > 0) && ((number & (number - 1)) === 0);
 269: }
 270: 
 271: /**
 272:  * Implementation of Radix-4 FFT.
 273:  * 
 274:  * P2FFT class provides functionality for performing Fast Fourier Transform on arrays
 275:  * which are a power of two in length.
 276:  * Code adapted from https://www.npmjs.com/package/fft.js
 277:  */
 278: class P2FFT {
 279:     /**
 280:      * @param {number} size The size of the input array. Must be a power of two larger than 1.
 281:      * @throws {Error} FFT size must be a power of two larger than 1.
 282:      */
 283:     constructor(size) {
 284:         this.size = size | 0; // convert to a 32-bit signed integer
 285:         if (this.size <= 1 || !isPowerOfTwo(this.size))
 286:             throw new Error('FFT size must be a power of two larger than 1');
 287: 
 288:         this._csize = size << 1;
 289: 
 290:         this.table = new Float64Array(this.size * 2);
 291:         for (let i = 0; i < this.table.length; i += 2) {
 292:             const angle = Math.PI * i / this.size;
 293:             this.table[i] = Math.cos(angle);
 294:             this.table[i + 1] = -Math.sin(angle);
 295:         }
 296: 
 297:         // Find size's power of two
 298:         let power = 0;
 299:         for (let t = 1; this.size > t; t <<= 1)
 300:             ++power;
 301: 
 302:         // Calculate initial step's width:
 303:         //   * If we are full radix-4, it is 2x smaller to give inital len=8
 304:         //   * Otherwise it is the same as `power` to give len=4
 305:         this._width = power % 2 === 0 ? power - 1 : power;
 306: 
 307:         // Pre-compute bit-reversal patterns
 308:         this._bitrev = new Int32Array(1 << this._width);
 309:         for (let j = 0; j < this._bitrev.length; ++j) {
 310:             this._bitrev[j] = 0;
 311:             for (let shift = 0; shift < this._width; shift += 2) {
 312:                 const revShift = this._width - shift - 2;
 313:                 this._bitrev[j] |= ((j >>> shift) & 3) << revShift;
 314:             }
 315:         }
 316:     }
 317: 
 318:     /**
 319:      * Create a complex number array with size `2 * size`
 320:      *
 321:      * @returns {Float64Array} A complex number array with size `2 * size`
 322:      */
 323:     createComplexArray() {
 324:         return new Float64Array(this._csize);
 325:     }
 326: 
 327:     /**
 328:      * Converts a complex number representation stored in a Float64Array to an array of real numbers.
 329:      * 
 330:      * @param {Float64Array} complex The complex number representation to be converted.
 331:      * @param {number[]} [storage] An optional array to store the result in.
 332:      * @returns {number[]} An array of real numbers representing the input complex number representation.
 333:      */
 334:     fromComplexArray(complex, storage) {
 335:         const res = storage || new Array(complex.length >>> 1);
 336:         for (let i = 0; i < complex.length; i += 2)
 337:             res[i >>> 1] = complex[i];
 338:         return res;
 339:     }
 340: 
 341:     /**
 342:      * Convert a real-valued input array to a complex-valued output array.
 343:      * @param {Float64Array} input The real-valued input array.
 344:      * @param {Float64Array} [storage] Optional buffer to store the output array.
 345:      * @returns {Float64Array} The complex-valued output array.
 346:      */
 347:     toComplexArray(input, storage) {
 348:         const res = storage || this.createComplexArray();
 349:         for (let i = 0; i < res.length; i += 2) {
 350:             res[i] = input[i >>> 1];
 351:             res[i + 1] = 0;
 352:         }
 353:         return res;
 354:     }
 355: 
 356:     /**
 357:      * Performs a Fast Fourier Transform (FFT) on the given input data and stores the result in the output buffer.
 358:      * 
 359:      * @param {Float64Array} out The output buffer to store the result.
 360:      * @param {Float64Array} data The input data to transform.
 361:      * 
 362:      * @throws {Error} Input and output buffers must be different.
 363:      * 
 364:      * @returns {void}
 365:      */
 366:     transform(out, data) {
 367:         if (out === data)
 368:             throw new Error('Input and output buffers must be different');
 369: 
 370:         this._transform4(out, data, 1 /* DONE */);
 371:     }
 372: 
 373:     /**
 374:      * Performs a real-valued forward FFT on the given input buffer and stores the result in the given output buffer.
 375:      * The input buffer must contain real values only, while the output buffer will contain complex values. The input and
 376:      * output buffers must be different.
 377:      *
 378:      * @param {Float64Array} out The output buffer.
 379:      * @param {Float64Array} data The input buffer containing real values.
 380:      *
 381:      * @throws {Error} If the input and output buffers are the same.
 382:      */
 383:     realTransform(out, data) {
 384:         if (out === data)
 385:             throw new Error('Input and output buffers must be different');
 386: 
 387:         this._realTransform4(out, data, 1 /* DONE */);
 388:     }
 389: 
 390:     /**
 391:      * Performs an inverse FFT transformation on the given `data` array, and stores the result in `out`.
 392:      * The `out` array must be a different buffer than the `data` array. The `out` array will contain the
 393:      * result of the transformation. The `data` array will not be modified.
 394:      * 
 395:      * @param {Float64Array} out The output buffer for the transformed data.
 396:      * @param {Float64Array} data The input data to transform.
 397:      * @throws {Error} If `out` and `data` refer to the same buffer.
 398:      * @returns {void}
 399:      */
 400:     inverseTransform(out, data) {
 401:         if (out === data)
 402:             throw new Error('Input and output buffers must be different');
 403: 
 404:         this._transform4(out, data, -1 /* DONE */);
 405:         for (let i = 0; i < out.length; ++i)
 406:             out[i] /= this.size;
 407:     }
 408: 
 409:     /**
 410:      * Performs a radix-4 implementation of a discrete Fourier transform on a given set of data.
 411:      *
 412:      * @param {Float64Array} out The output buffer for the transformed data.
 413:      * @param {Float64Array} data The input buffer of data to be transformed.
 414:      * @param {number} inv A scaling factor to apply to the transform.
 415:      * @returns {void}
 416:      */
 417:     _transform4(out, data, inv) {
 418:         // radix-4 implementation
 419: 
 420:         const size = this._csize;
 421: 
 422:         // Initial step (permute and transform)
 423:         const width = this._width;
 424:         let step = 1 << width;
 425:         let len = (size / step) << 1;
 426: 
 427:         let outOff;
 428:         let t;
 429:         const bitrev = this._bitrev;
 430:         if (len === 4) {
 431:             for (outOff = 0, t = 0; outOff < size; outOff += len, ++t) {
 432:                 const off = bitrev[t];
 433:                 this._singleTransform2(data, out, outOff, off, step);
 434:             }
 435:         } else {
 436:             // len === 8
 437:             for (outOff = 0, t = 0; outOff < size; outOff += len, ++t) {
 438:                 const off = bitrev[t];
 439:                 this._singleTransform4(data, out, outOff, off, step, inv);
 440:             }
 441:         }
 442: 
 443:         // Loop through steps in decreasing order
 444:         const table = this.table;
 445:         for (step >>= 2; step >= 2; step >>= 2) {
 446:             len = (size / step) << 1;
 447:             const quarterLen = len >>> 2;
 448: 
 449:             // Loop through offsets in the data
 450:             for (outOff = 0; outOff < size; outOff += len) {
 451:                 // Full case
 452:                 const limit = outOff + quarterLen - 1;
 453:                 for (let i = outOff, k = 0; i < limit; i += 2, k += step) {
 454:                     const A = i;
 455:                     const B = A + quarterLen;
 456:                     const C = B + quarterLen;
 457:                     const D = C + quarterLen;
 458: 
 459:                     // Original values
 460:                     const Ar = out[A];
 461:                     const Ai = out[A + 1];
 462:                     const Br = out[B];
 463:                     const Bi = out[B + 1];
 464:                     const Cr = out[C];
 465:                     const Ci = out[C + 1];
 466:                     const Dr = out[D];
 467:                     const Di = out[D + 1];
 468: 
 469:                     const tableBr = table[k];
 470:                     const tableBi = inv * table[k + 1];
 471:                     const MBr = Br * tableBr - Bi * tableBi;
 472:                     const MBi = Br * tableBi + Bi * tableBr;
 473: 
 474:                     const tableCr = table[2 * k];
 475:                     const tableCi = inv * table[2 * k + 1];
 476:                     const MCr = Cr * tableCr - Ci * tableCi;
 477:                     const MCi = Cr * tableCi + Ci * tableCr;
 478: 
 479:                     const tableDr = table[3 * k];
 480:                     const tableDi = inv * table[3 * k + 1];
 481:                     const MDr = Dr * tableDr - Di * tableDi;
 482:                     const MDi = Dr * tableDi + Di * tableDr;
 483: 
 484:                     // Pre-Final values
 485:                     const T0r = Ar + MCr;
 486:                     const T0i = Ai + MCi;
 487:                     const T1r = Ar - MCr;
 488:                     const T1i = Ai - MCi;
 489:                     const T2r = MBr + MDr;
 490:                     const T2i = MBi + MDi;
 491:                     const T3r = inv * (MBr - MDr);
 492:                     const T3i = inv * (MBi - MDi);
 493: 
 494:                     // Final values
 495:                     out[A] = T0r + T2r;
 496:                     out[A + 1] = T0i + T2i;
 497:                     out[B] = T1r + T3i;
 498:                     out[B + 1] = T1i - T3r;
 499:                     out[C] = T0r - T2r;
 500:                     out[C + 1] = T0i - T2i;
 501:                     out[D] = T1r - T3i;
 502:                     out[D + 1] = T1i + T3r;
 503:                 }
 504:             }
 505:         }
 506:     }
 507: 
 508:     /**
 509:      * Performs a radix-2 implementation of a discrete Fourier transform on a given set of data.
 510:      *
 511:      * @param {Float64Array} data The input buffer of data to be transformed.
 512:      * @param {Float64Array} out The output buffer for the transformed data.
 513:      * @param {number} outOff The offset at which to write the output data.
 514:      * @param {number} off The offset at which to begin reading the input data.
 515:      * @param {number} step The step size for indexing the input data.
 516:      * @returns {void}
 517:      */
 518:     _singleTransform2(data, out, outOff, off, step) {
 519:         // radix-2 implementation
 520:         // NOTE: Only called for len=4
 521: 
 522:         const evenR = data[off];
 523:         const evenI = data[off + 1];
 524:         const oddR = data[off + step];
 525:         const oddI = data[off + step + 1];
 526: 
 527:         out[outOff] = evenR + oddR;
 528:         out[outOff + 1] = evenI + oddI;
 529:         out[outOff + 2] = evenR - oddR;
 530:         out[outOff + 3] = evenI - oddI;
 531:     }
 532: 
 533:     /**
 534:      * Performs radix-4 transformation on input data of length 8
 535:      *
 536:      * @param {Float64Array} data Input data array of length 8
 537:      * @param {Float64Array} out Output data array of length 8
 538:      * @param {number} outOff Index of output array to start writing from
 539:      * @param {number} off Index of input array to start reading from
 540:      * @param {number} step Step size between elements in input array
 541:      * @param {number} inv Scaling factor for inverse transform
 542:      * 
 543:      * @returns {void}
 544:      */
 545:     _singleTransform4(data, out, outOff, off, step, inv) {
 546:         // radix-4
 547:         // NOTE: Only called for len=8
 548:         const step2 = step * 2;
 549:         const step3 = step * 3;
 550: 
 551:         // Original values
 552:         const Ar = data[off];
 553:         const Ai = data[off + 1];
 554:         const Br = data[off + step];
 555:         const Bi = data[off + step + 1];
 556:         const Cr = data[off + step2];
 557:         const Ci = data[off + step2 + 1];
 558:         const Dr = data[off + step3];
 559:         const Di = data[off + step3 + 1];
 560: 
 561:         // Pre-Final values
 562:         const T0r = Ar + Cr;
 563:         const T0i = Ai + Ci;
 564:         const T1r = Ar - Cr;
 565:         const T1i = Ai - Ci;
 566:         const T2r = Br + Dr;
 567:         const T2i = Bi + Di;
 568:         const T3r = inv * (Br - Dr);
 569:         const T3i = inv * (Bi - Di);
 570: 
 571:         // Final values
 572:         out[outOff] = T0r + T2r;
 573:         out[outOff + 1] = T0i + T2i;
 574:         out[outOff + 2] = T1r + T3i;
 575:         out[outOff + 3] = T1i - T3r;
 576:         out[outOff + 4] = T0r - T2r;
 577:         out[outOff + 5] = T0i - T2i;
 578:         out[outOff + 6] = T1r - T3i;
 579:         out[outOff + 7] = T1i + T3r;
 580:     }
 581: 
 582:     /**
 583:      * Real input radix-4 implementation
 584:      * @param {Float64Array} out Output array for the transformed data
 585:      * @param {Float64Array} data Input array of real data to be transformed
 586:      * @param {number} inv The scale factor used to normalize the inverse transform
 587:      */
 588:     _realTransform4(out, data, inv) {
 589:         // Real input radix-4 implementation
 590:         const size = this._csize;
 591: 
 592:         // Initial step (permute and transform)
 593:         const width = this._width;
 594:         let step = 1 << width;
 595:         let len = (size / step) << 1;
 596: 
 597:         let outOff;
 598:         let t;
 599:         const bitrev = this._bitrev;
 600:         if (len === 4) {
 601:             for (outOff = 0, t = 0; outOff < size; outOff += len, ++t) {
 602:                 const off = bitrev[t];
 603:                 this._singleRealTransform2(data, out, outOff, off >>> 1, step >>> 1);
 604:             }
 605:         } else {
 606:             // len === 8
 607:             for (outOff = 0, t = 0; outOff < size; outOff += len, ++t) {
 608:                 const off = bitrev[t];
 609:                 this._singleRealTransform4(data, out, outOff, off >>> 1, step >>> 1, inv);
 610:             }
 611:         }
 612: 
 613:         // Loop through steps in decreasing order
 614:         const table = this.table;
 615:         for (step >>= 2; step >= 2; step >>= 2) {
 616:             len = (size / step) << 1;
 617:             const halfLen = len >>> 1;
 618:             const quarterLen = halfLen >>> 1;
 619:             const hquarterLen = quarterLen >>> 1;
 620: 
 621:             // Loop through offsets in the data
 622:             for (outOff = 0; outOff < size; outOff += len) {
 623:                 for (let i = 0, k = 0; i <= hquarterLen; i += 2, k += step) {
 624:                     const A = outOff + i;
 625:                     const B = A + quarterLen;
 626:                     const C = B + quarterLen;
 627:                     const D = C + quarterLen;
 628: 
 629:                     // Original values
 630:                     const Ar = out[A];
 631:                     const Ai = out[A + 1];
 632:                     const Br = out[B];
 633:                     const Bi = out[B + 1];
 634:                     const Cr = out[C];
 635:                     const Ci = out[C + 1];
 636:                     const Dr = out[D];
 637:                     const Di = out[D + 1];
 638: 
 639:                     // Middle values
 640:                     const MAr = Ar;
 641:                     const MAi = Ai;
 642: 
 643:                     const tableBr = table[k];
 644:                     const tableBi = inv * table[k + 1];
 645:                     const MBr = Br * tableBr - Bi * tableBi;
 646:                     const MBi = Br * tableBi + Bi * tableBr;
 647: 
 648:                     const tableCr = table[2 * k];
 649:                     const tableCi = inv * table[2 * k + 1];
 650:                     const MCr = Cr * tableCr - Ci * tableCi;
 651:                     const MCi = Cr * tableCi + Ci * tableCr;
 652: 
 653:                     const tableDr = table[3 * k];
 654:                     const tableDi = inv * table[3 * k + 1];
 655:                     const MDr = Dr * tableDr - Di * tableDi;
 656:                     const MDi = Dr * tableDi + Di * tableDr;
 657: 
 658:                     // Pre-Final values
 659:                     const T0r = MAr + MCr;
 660:                     const T0i = MAi + MCi;
 661:                     const T1r = MAr - MCr;
 662:                     const T1i = MAi - MCi;
 663:                     const T2r = MBr + MDr;
 664:                     const T2i = MBi + MDi;
 665:                     const T3r = inv * (MBr - MDr);
 666:                     const T3i = inv * (MBi - MDi);
 667: 
 668:                     // Final values
 669:                     out[A] = T0r + T2r;
 670:                     out[A + 1] = T0i + T2i;
 671:                     out[B] = T1r + T3i;
 672:                     out[B + 1] = T1i - T3r;
 673: 
 674:                     // Output final middle point
 675:                     if (i === 0) {
 676:                         out[C] = T0r - T2r;
 677:                         out[C + 1] = T0i - T2i;
 678:                         continue;
 679:                     }
 680: 
 681:                     // Do not overwrite ourselves
 682:                     if (i === hquarterLen)
 683:                         continue;
 684: 
 685:                     const SA = outOff + quarterLen - i;
 686:                     const SB = outOff + halfLen - i;
 687: 
 688:                     out[SA] = T1r - inv * T3i;
 689:                     out[SA + 1] = -T1i - inv * T3r;
 690:                     out[SB] = T0r - inv * T2r;
 691:                     out[SB + 1] = -T0i + inv * T2i;
 692:                 }
 693:             }
 694:         }
 695: 
 696:         // Complete the spectrum by adding its mirrored negative frequency components.
 697:         const half = size >>> 1;
 698:         for (let i = 2; i < half; i += 2) {
 699:             out[size - i] = out[i];
 700:             out[size - i + 1] = -out[i + 1];
 701:         }
 702:     }
 703: 
 704:     /**
 705:      * Performs a single real input radix-2 transformation on the provided data
 706:      * 
 707:      * @param {Float64Array} data The input data array
 708:      * @param {Float64Array} out The output data array
 709:      * @param {number} outOff The output offset
 710:      * @param {number} off The input offset
 711:      * @param {number} step The step
 712:      * 
 713:      * @returns {void}
 714:      */
 715:     _singleRealTransform2(data, out, outOff, off, step) {
 716:         // radix-2 implementation
 717:         // NOTE: Only called for len=4
 718: 
 719:         const evenR = data[off];
 720:         const oddR = data[off + step];
 721: 
 722:         out[outOff] = evenR + oddR;
 723:         out[outOff + 1] = 0;
 724:         out[outOff + 2] = evenR - oddR;
 725:         out[outOff + 3] = 0;
 726:     }
 727: 
 728:     /**
 729:      * Computes a single real-valued transform using radix-4 algorithm.
 730:      * This method is only called for len=8.
 731:      *
 732:      * @param {Float64Array} data The input data array.
 733:      * @param {Float64Array} out The output data array.
 734:      * @param {number} outOff The offset into the output array.
 735:      * @param {number} off The offset into the input array.
 736:      * @param {number} step The step size for the input array.
 737:      * @param {number} inv The value of inverse.
 738:      */
 739:     _singleRealTransform4(data, out, outOff, off, step, inv) {
 740:         // radix-4
 741:         // NOTE: Only called for len=8
 742:         const step2 = step * 2;
 743:         const step3 = step * 3;
 744: 
 745:         // Original values
 746:         const Ar = data[off];
 747:         const Br = data[off + step];
 748:         const Cr = data[off + step2];
 749:         const Dr = data[off + step3];
 750: 
 751:         // Pre-Final values
 752:         const T0r = Ar + Cr;
 753:         const T1r = Ar - Cr;
 754:         const T2r = Br + Dr;
 755:         const T3r = inv * (Br - Dr);
 756: 
 757:         // Final values
 758:         out[outOff] = T0r + T2r;
 759:         out[outOff + 1] = 0;
 760:         out[outOff + 2] = T1r;
 761:         out[outOff + 3] = -T3r;
 762:         out[outOff + 4] = T0r - T2r;
 763:         out[outOff + 5] = 0;
 764:         out[outOff + 6] = T1r;
 765:         out[outOff + 7] = T3r;
 766:     }
 767: }
 768: 
 769: /**
 770:  * NP2FFT class provides functionality for performing Fast Fourier Transform on arrays
 771:  * which are not a power of two in length. In such cases, the chirp-z transform is used.
 772:  * 
 773:  * For more information, see: https://math.stackexchange.com/questions/77118/non-power-of-2-ffts/77156#77156
 774:  */
 775: class NP2FFT {
 776: 
 777:     /**
 778:      * Constructs a new NP2FFT object.
 779:      * @param {number} fft_length The length of the FFT
 780:      */
 781:     constructor(fft_length) {
 782:         // Helper variables
 783:         const a = 2 * (fft_length - 1);
 784:         const b = 2 * (2 * fft_length - 1);
 785:         const nextP2 = 2 ** (Math.ceil(Math.log2(b)))
 786:         this.bufferSize = nextP2;
 787:         this._a = a;
 788: 
 789:         // Define buffers
 790:         // Compute chirp for transform
 791:         const chirp = new Float64Array(b);
 792:         const ichirp = new Float64Array(nextP2);
 793:         this._chirpBuffer = new Float64Array(nextP2);
 794:         this._buffer1 = new Float64Array(nextP2);
 795:         this._buffer2 = new Float64Array(nextP2);
 796:         this._outBuffer1 = new Float64Array(nextP2);
 797:         this._outBuffer2 = new Float64Array(nextP2);
 798: 
 799:         // Compute complex exponentiation
 800:         const theta = -2 * Math.PI / fft_length;
 801:         const baseR = Math.cos(theta);
 802:         const baseI = Math.sin(theta);
 803: 
 804:         // Precompute helper for chirp-z transform
 805:         for (let i = 0; i < b >> 1; ++i) {
 806:             // Compute complex power:
 807:             const e = (i + 1 - fft_length) ** 2 / 2.0;
 808: 
 809:             // Compute the modulus and argument of the result
 810:             const result_mod = Math.sqrt(baseR ** 2 + baseI ** 2) ** e;
 811:             const result_arg = e * Math.atan2(baseI, baseR);
 812: 
 813:             // Convert the result back to rectangular form
 814:             // and assign to chirp and ichirp
 815:             const i2 = 2 * i;
 816:             chirp[i2] = result_mod * Math.cos(result_arg);
 817:             chirp[i2 + 1] = result_mod * Math.sin(result_arg);
 818: 
 819:             // conjugate
 820:             ichirp[i2] = chirp[i2];
 821:             ichirp[i2 + 1] = - chirp[i2 + 1];
 822:         }
 823:         this._slicedChirpBuffer = chirp.subarray(a, b);
 824: 
 825:         // create object to perform Fast Fourier Transforms
 826:         // with `nextP2` complex numbers
 827:         this._f = new P2FFT(nextP2 >> 1);
 828:         this._f.transform(this._chirpBuffer, ichirp);
 829:     }
 830: 
 831:     _transform(output, input, real) {
 832:         const ib1 = this._buffer1;
 833:         const ib2 = this._buffer2;
 834:         const ob2 = this._outBuffer1;
 835:         const ob3 = this._outBuffer2;
 836:         const cb = this._chirpBuffer;
 837:         const sb = this._slicedChirpBuffer;
 838:         const a = this._a;
 839: 
 840:         if (real) {
 841:             // Real multiplication
 842:             for (let j = 0; j < sb.length; j += 2) {
 843:                 const j2 = j + 1
 844:                 const j3 = j >> 1;
 845: 
 846:                 const a_real = input[j3];
 847:                 ib1[j] = a_real * sb[j];
 848:                 ib1[j2] = a_real * sb[j2];
 849:             }
 850:         } else {
 851:             // Complex multiplication
 852:             for (let j = 0; j < sb.length; j += 2) {
 853:                 const j2 = j + 1
 854:                 ib1[j] = input[j] * sb[j] - input[j2] * sb[j2];
 855:                 ib1[j2] = input[j] * sb[j2] + input[j2] * sb[j];
 856:             }
 857:         }
 858:         this._f.transform(ob2, ib1);
 859: 
 860:         for (let j = 0; j < cb.length; j += 2) {
 861:             const j2 = j + 1;
 862: 
 863:             ib2[j] = ob2[j] * cb[j] - ob2[j2] * cb[j2];
 864:             ib2[j2] = ob2[j] * cb[j2] + ob2[j2] * cb[j];
 865:         }
 866:         this._f.inverseTransform(ob3, ib2);
 867: 
 868:         for (let j = 0; j < ob3.length; j += 2) {
 869:             const a_real = ob3[j + a];
 870:             const a_imag = ob3[j + a + 1];
 871:             const b_real = sb[j];
 872:             const b_imag = sb[j + 1];
 873: 
 874:             output[j] = a_real * b_real - a_imag * b_imag;
 875:             output[j + 1] = a_real * b_imag + a_imag * b_real;
 876:         }
 877:     }
 878: 
 879:     transform(output, input) {
 880:         this._transform(output, input, false);
 881:     }
 882: 
 883:     realTransform(output, input) {
 884:         this._transform(output, input, true);
 885:     }
 886: }
 887: 
 888: export class FFT {
 889:     constructor(fft_length) {
 890:         this.fft_length = fft_length;
 891:         this.isPowerOfTwo = isPowerOfTwo(fft_length);
 892:         if (this.isPowerOfTwo) {
 893:             this.fft = new P2FFT(fft_length);
 894:             this.outputBufferSize = 2 * fft_length;
 895:         } else {
 896:             this.fft = new NP2FFT(fft_length);
 897:             this.outputBufferSize = this.fft.bufferSize;
 898:         }
 899:     }
 900: 
 901:     realTransform(out, input) {
 902:         this.fft.realTransform(out, input);
 903:     }
 904: 
 905:     transform(out, input) {
 906:         this.fft.transform(out, input);
 907:     }
 908: }
 909: 
 910: 
 911: /**
 912:  * Performs median filter on the provided data. Padding is done by mirroring the data.
 913:  * @param {AnyTypedArray} data The input array
 914:  * @param {number} windowSize The window size
 915:  */
 916: export function medianFilter(data, windowSize) {
 917: 
 918:     if (windowSize % 2 === 0 || windowSize <= 0) {
 919:         throw new Error('Window size must be a positive odd number');
 920:     }
 921: 
 922:     // @ts-ignore
 923:     const outputArray = new data.constructor(data.length);
 924: 
 925:     // @ts-ignore
 926:     const buffer = new data.constructor(windowSize); // Reusable array for storing values
 927: 
 928:     const halfWindowSize = Math.floor(windowSize / 2);
 929: 
 930:     for (let i = 0; i < data.length; ++i) {
 931:         let valuesIndex = 0;
 932: 
 933:         for (let j = -halfWindowSize; j <= halfWindowSize; ++j) {
 934:             let index = i + j;
 935:             if (index < 0) {
 936:                 index = Math.abs(index);
 937:             } else if (index >= data.length) {
 938:                 index = 2 * (data.length - 1) - index;
 939:             }
 940: 
 941:             buffer[valuesIndex++] = data[index];
 942:         }
 943: 
 944:         buffer.sort();
 945:         outputArray[i] = buffer[halfWindowSize];
 946:     }
 947: 
 948:     return outputArray;
 949: }
 950: 
 951: /**
 952:  * Helper function to round a number to a given number of decimals
 953:  * @param {number} num The number to round
 954:  * @param {number} decimals The number of decimals
 955:  * @returns {number} The rounded number
 956:  */
 957: export function round(num, decimals) {
 958:     const pow = Math.pow(10, decimals);
 959:     return Math.round(num * pow) / pow;
 960: }
 961: 
 962: /**
 963:  * Helper function to round a number to the nearest integer, with ties rounded to the nearest even number.
 964:  * Also known as "bankers' rounding". This is the default rounding mode in python. For example:
 965:  * 1.5 rounds to 2 and 2.5 rounds to 2.
 966:  * 
 967:  * @param {number} x The number to round
 968:  * @returns {number} The rounded number
 969:  */
 970: export function bankers_round(x) {
 971:     const r = Math.round(x);
 972:     const br = Math.abs(x) % 1 === 0.5 ? (r % 2 === 0 ? r : r - 1) : r;
 973:     return br;
 974: }
 975: 
 976: 
 977: /**
 978:  * Measures similarity between two temporal sequences (e.g., input audio and output tokens
 979:  * to generate token-level timestamps).
 980:  * @param {number[][]} matrix 
 981:  * @returns {number[][]}
 982:  */
 983: export function dynamic_time_warping(matrix) {
 984:     const output_length = matrix.length;
 985:     const input_length = matrix[0].length;
 986: 
 987:     const outputShape = [output_length + 1, input_length + 1];
 988: 
 989:     const cost = Array.from(
 990:         { length: outputShape[0] },
 991:         () => Array(outputShape[1]).fill(Infinity)
 992:     );
 993:     cost[0][0] = 0;
 994: 
 995:     const trace = Array.from(
 996:         { length: outputShape[0] },
 997:         () => Array(outputShape[1]).fill(-1)
 998:     );
 999: 
1000:     for (let j = 1; j < outputShape[1]; ++j) {
1001:         for (let i = 1; i < outputShape[0]; ++i) {
1002:             const c0 = cost[i - 1][j - 1];
1003:             const c1 = cost[i - 1][j];
1004:             const c2 = cost[i][j - 1];
1005: 
1006:             let c, t;
1007:             if (c0 < c1 && c0 < c2) {
1008:                 c = c0;
1009:                 t = 0;
1010:             } else if (c1 < c0 && c1 < c2) {
1011:                 c = c1;
1012:                 t = 1;
1013:             } else {
1014:                 c = c2;
1015:                 t = 2;
1016:             }
1017:             cost[i][j] = matrix[i - 1][j - 1] + c;
1018:             trace[i][j] = t;
1019:         }
1020:     }
1021: 
1022:     for (let i = 0; i < outputShape[1]; ++i) { // trace[0, :] = 2
1023:         trace[0][i] = 2;
1024:     }
1025:     for (let i = 0; i < outputShape[0]; ++i) { // trace[:, 0] = 1
1026:         trace[i][0] = 1;
1027:     }
1028: 
1029:     // backtrace
1030:     let i = output_length;
1031:     let j = input_length;
1032:     let text_indices = [];
1033:     let time_indices = [];
1034:     while (i > 0 || j > 0) {
1035:         text_indices.push(i - 1);
1036:         time_indices.push(j - 1);
1037: 
1038:         switch (trace[i][j]) {
1039:             case 0:
1040:                 --i; --j;
1041:                 break;
1042:             case 1:
1043:                 --i;
1044:                 break;
1045:             case 2:
1046:                 --j;
1047:                 break;
1048:             default:
1049:                 throw new Error(
1050:                     `Internal error in dynamic time warping. Unexpected trace[${i}, ${j}]. Please file a bug report.`
1051:                 )
1052:         }
1053:     }
1054: 
1055:     text_indices.reverse();
1056:     time_indices.reverse();
1057: 
1058:     return [text_indices, time_indices];
1059: 
1060: }
</file>

<file path="src/tjs/utils/tensor.js">
   1: /**
   2:  * @file Helper module for `Tensor` processing.
   3:  *
   4:  * These functions and classes are only used internally,
   5:  * meaning an end-user shouldn't need to access anything here.
   6:  *
   7:  * @module utils/tensor
   8:  */
   9: 
  10: import {
  11:     interpolate_data,
  12:     max,
  13:     min,
  14:     permute_data
  15: } from './maths.js';
  16: 
  17: import {
  18:     Tensor as ONNXTensor, isONNXTensor,
  19: } from '../backends/onnx.js';
  20: 
  21: import { TensorOpRegistry } from '../ops/registry.js';
  22: 
  23: export const DataTypeMap = Object.freeze({
  24:     float32: Float32Array,
  25:     // @ts-ignore ts(2552) Limited availability of Float16Array across browsers:
  26:     // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float16Array
  27:     float16: typeof Float16Array !== "undefined" ? Float16Array: Uint16Array,
  28:     float64: Float64Array,
  29:     string: Array, // string[]
  30:     int8: Int8Array,
  31:     uint8: Uint8Array,
  32:     int16: Int16Array,
  33:     uint16: Uint16Array,
  34:     int32: Int32Array,
  35:     uint32: Uint32Array,
  36:     int64: BigInt64Array,
  37:     uint64: BigUint64Array,
  38:     bool: Uint8Array,
  39:     uint4: Uint8Array,
  40:     int4: Int8Array,
  41: });
  42: 
  43: /**
  44:  * @typedef {keyof typeof DataTypeMap} DataType
  45:  * @typedef {import('./maths.js').AnyTypedArray | any[]} DataArray
  46:  */
  47: 
  48: 
  49: export class Tensor {
  50:     /** @type {number[]} Dimensions of the tensor. */
  51:     get dims() {
  52:         // @ts-ignore
  53:         return this.ort_tensor.dims;
  54:     }
  55:     set dims(value) {
  56:         // FIXME: ONNXTensor declares dims as readonly so one needs to use the constructor() if dims change.
  57:         // @ts-ignore
  58:         this.ort_tensor.dims = value;
  59:     }
  60: 
  61:     /** @type {DataType} Type of the tensor. */
  62:     get type() {
  63:         return this.ort_tensor.type;
  64:     };
  65: 
  66:     /** @type {DataArray} The data stored in the tensor. */
  67:     get data() {
  68:         return this.ort_tensor.data;
  69:     }
  70: 
  71:     /** @type {number} The number of elements in the tensor. */
  72:     get size() {
  73:         return this.ort_tensor.size;
  74:     };
  75: 
  76:     /** @type {string} The location of the tensor data. */
  77:     get location() {
  78:         return this.ort_tensor.location;
  79:     };
  80: 
  81:     ort_tensor;
  82: 
  83:     /**
  84:      * Create a new Tensor or copy an existing Tensor.
  85:      * @param {[DataType, DataArray, number[]]|[ONNXTensor]} args
  86:      */
  87:     constructor(...args) {
  88:         if (isONNXTensor(args[0])) {
  89:             this.ort_tensor = /** @type {ONNXTensor} */ (args[0]);
  90:         } else {
  91:             // Create new tensor
  92:             this.ort_tensor = new ONNXTensor(
  93:                 /** @type {DataType} */(args[0]),
  94:                 // @ts-expect-error ts(2769) Type 'number' is not assignable to type 'bigint'.
  95:                 /** @type {Exclude<import('./maths.js').AnyTypedArray, Uint8ClampedArray>} */(args[1]),
  96:                 args[2],
  97:             );
  98:         }
  99: 
 100:         return new Proxy(this, {
 101:             get: (obj, key) => {
 102:                 if (typeof key === 'string') {
 103:                     let index = Number(key);
 104:                     if (Number.isInteger(index)) {
 105:                         // key is an integer (i.e., index)
 106:                         return obj._getitem(index);
 107:                     }
 108:                 }
 109:                 // @ts-ignore
 110:                 return obj[key];
 111:             },
 112:             set: (obj, key, value) => {
 113:                 // TODO allow setting of data
 114: 
 115:                 // @ts-ignore
 116:                 return obj[key] = value;
 117:             }
 118:         });
 119:     }
 120: 
 121:     dispose() {
 122:         this.ort_tensor.dispose();
 123:         // this.ort_tensor = undefined;
 124:     }
 125: 
 126:     /**
 127:      * Returns an iterator object for iterating over the tensor data in row-major order.
 128:      * If the tensor has more than one dimension, the iterator will yield subarrays.
 129:      * @returns {Iterator} An iterator object for iterating over the tensor data in row-major order.
 130:      */
 131:     *[Symbol.iterator]() {
 132:         const [iterLength, ...iterDims] = this.dims;
 133: 
 134:         if (iterDims.length > 0) {
 135:             const iterSize = iterDims.reduce((a, b) => a * b);
 136:             for (let i = 0; i < iterLength; ++i) {
 137:                 yield this._subarray(i, iterSize, iterDims);
 138:             }
 139:         } else {
 140:             yield* this.data
 141:         }
 142: 
 143:     }
 144: 
 145:     /**
 146:      * Index into a Tensor object.
 147:      * @param {number} index The index to access.
 148:      * @returns {Tensor} The data at the specified index.
 149:      */
 150:     _getitem(index) {
 151:         const [iterLength, ...iterDims] = this.dims;
 152: 
 153:         index = safeIndex(index, iterLength);
 154: 
 155:         if (iterDims.length > 0) {
 156:             const iterSize = iterDims.reduce((a, b) => a * b);
 157:             return this._subarray(index, iterSize, iterDims);
 158:         } else {
 159:             return new Tensor(this.type, [this.data[index]], iterDims);
 160:         }
 161:     }
 162: 
 163:     /**
 164:      * @param {number|bigint} item The item to search for in the tensor
 165:      * @returns {number} The index of the first occurrence of item in the tensor data.
 166:      */
 167:     indexOf(item) {
 168:         const this_data = this.data;
 169:         for (let index = 0; index < this_data.length; ++index) {
 170:             // Note: == instead of === so we can match Ints with BigInts
 171:             if (this_data[index] == item) {
 172:                 return index;
 173:             }
 174:         }
 175:         return -1;
 176:     }
 177: 
 178:     /**
 179:      * @param {number} index
 180:      * @param {number} iterSize
 181:      * @param {any} iterDims
 182:      * @returns {Tensor}
 183:      */
 184:     _subarray(index, iterSize, iterDims) {
 185:         const o1 = index * iterSize;
 186:         const o2 = (index + 1) * iterSize;
 187: 
 188:         // We use subarray if available (typed array), otherwise we use slice (normal array)
 189:         const data =
 190:             ('subarray' in this.data)
 191:                 ? this.data.subarray(o1, o2)
 192:                 : this.data.slice(o1, o2);
 193:         return new Tensor(this.type, data, iterDims);
 194:     }
 195: 
 196:     /**
 197:      * Returns the value of this tensor as a standard JavaScript Number. This only works
 198:      * for tensors with one element. For other cases, see `Tensor.tolist()`.
 199:      * @returns {number|bigint} The value of this tensor as a standard JavaScript Number.
 200:      * @throws {Error} If the tensor has more than one element.
 201:      */
 202:     item() {
 203:         const this_data = this.data;
 204:         if (this_data.length !== 1) {
 205:             throw new Error(`a Tensor with ${this_data.length} elements cannot be converted to Scalar`);
 206:         }
 207:         return this_data[0];
 208:     }
 209: 
 210:     /**
 211:      * Convert tensor data to a n-dimensional JS list
 212:      * @returns {Array}
 213:      */
 214:     tolist() {
 215:         return reshape(this.data, this.dims)
 216:     }
 217: 
 218:     /**
 219:      * Return a new Tensor with the sigmoid function applied to each element.
 220:      * @returns {Tensor} The tensor with the sigmoid function applied.
 221:      */
 222:     sigmoid() {
 223:         return this.clone().sigmoid_();
 224:     }
 225: 
 226:     /**
 227:      * Applies the sigmoid function to the tensor in place.
 228:      * @returns {Tensor} Returns `this`.
 229:      */
 230:     sigmoid_() {
 231:         const this_data = this.data;
 232:         for (let i = 0; i < this_data.length; ++i) {
 233:             this_data[i] = 1 / (1 + Math.exp(-this_data[i]));
 234:         }
 235:         return this;
 236:     }
 237: 
 238:     /**
 239:      * Return a new Tensor with a callback function applied to each element.
 240:      * @param {Function} callback - The function to apply to each element. It should take three arguments:
 241:      *                              the current element, its index, and the tensor's data array.
 242:      * @returns {Tensor} A new Tensor with the callback function applied to each element.
 243:      */
 244:     map(callback) {
 245:         return this.clone().map_(callback);
 246:     }
 247: 
 248:     /**
 249:      * Apply a callback function to each element of the tensor in place.
 250:      * @param {Function} callback - The function to apply to each element. It should take three arguments:
 251:      *                              the current element, its index, and the tensor's data array.
 252:      * @returns {Tensor} Returns `this`.
 253:      */
 254:     map_(callback) {
 255:         const this_data = this.data;
 256:         for (let i = 0; i < this_data.length; ++i) {
 257:             this_data[i] = callback(this_data[i], i, this_data);
 258:         }
 259:         return this;
 260:     }
 261: 
 262:     /**
 263:      * Return a new Tensor with every element multiplied by a constant.
 264:      * @param {number} val The value to multiply by.
 265:      * @returns {Tensor} The new tensor.
 266:      */
 267:     mul(val) {
 268:         return this.clone().mul_(val);
 269:     }
 270: 
 271:     /**
 272:      * Multiply the tensor by a constant in place.
 273:      * @param {number} val The value to multiply by.
 274:      * @returns {Tensor} Returns `this`.
 275:      */
 276:     mul_(val) {
 277:         const this_data = this.data;
 278:         for (let i = 0; i < this_data.length; ++i) {
 279:             this_data[i] *= val;
 280:         }
 281:         return this;
 282:     }
 283: 
 284:     /**
 285:      * Return a new Tensor with every element divided by a constant.
 286:      * @param {number} val The value to divide by.
 287:      * @returns {Tensor} The new tensor.
 288:      */
 289:     div(val) {
 290:         return this.clone().div_(val);
 291:     }
 292: 
 293:     /**
 294:      * Divide the tensor by a constant in place.
 295:      * @param {number} val The value to divide by.
 296:      * @returns {Tensor} Returns `this`.
 297:      */
 298:     div_(val) {
 299:         const this_data = this.data;
 300:         for (let i = 0; i < this_data.length; ++i) {
 301:             this_data[i] /= val;
 302:         }
 303:         return this;
 304:     }
 305: 
 306:     /**
 307:      * Return a new Tensor with every element added by a constant.
 308:      * @param {number} val The value to add by.
 309:      * @returns {Tensor} The new tensor.
 310:      */
 311:     add(val) {
 312:         return this.clone().add_(val);
 313:     }
 314: 
 315:     /**
 316:      * Add the tensor by a constant in place.
 317:      * @param {number} val The value to add by.
 318:      * @returns {Tensor} Returns `this`.
 319:      */
 320:     add_(val) {
 321:         const this_data = this.data;
 322:         for (let i = 0; i < this_data.length; ++i) {
 323:             this_data[i] += val;
 324:         }
 325:         return this;
 326:     }
 327: 
 328:     /**
 329:      * Return a new Tensor with every element subtracted by a constant.
 330:      * @param {number} val The value to subtract by.
 331:      * @returns {Tensor} The new tensor.
 332:      */
 333:     sub(val) {
 334:         return this.clone().sub_(val);
 335:     }
 336: 
 337:     /**
 338:      * Subtract the tensor by a constant in place.
 339:      * @param {number} val The value to subtract by.
 340:      * @returns {Tensor} Returns `this`.
 341:      */
 342:     sub_(val) {
 343:         const this_data = this.data;
 344:         for (let i = 0; i < this_data.length; ++i) {
 345:             this_data[i] -= val;
 346:         }
 347:         return this;
 348:     }
 349: 
 350:     /**
 351:      * Creates a deep copy of the current Tensor.
 352:      * @returns {Tensor} A new Tensor with the same type, data, and dimensions as the original.
 353:      */
 354:     clone() {
 355:         return new Tensor(this.type, this.data.slice(), this.dims.slice());
 356:     }
 357: 
 358:     /**
 359:      * Performs a slice operation on the Tensor along specified dimensions.
 360:      *
 361:      * Consider a Tensor that has a dimension of [4, 7]:
 362:      * ```
 363:      * [ 1,  2,  3,  4,  5,  6,  7]
 364:      * [ 8,  9, 10, 11, 12, 13, 14]
 365:      * [15, 16, 17, 18, 19, 20, 21]
 366:      * [22, 23, 24, 25, 26, 27, 28]
 367:      * ```
 368:      * We can slice against the two dims of row and column, for instance in this
 369:      * case we can start at the second element, and return to the second last,
 370:      * like this:
 371:      * ```
 372:      * tensor.slice([1, -1], [1, -1]);
 373:      * ```
 374:      * which would return:
 375:      * ```
 376:      * [  9, 10, 11, 12, 13 ]
 377:      * [ 16, 17, 18, 19, 20 ]
 378:      * ```
 379:      *
 380:      * @param {...(number|number[]|null)} slices The slice specifications for each dimension.
 381:      * - If a number is given, then a single element is selected.
 382:      * - If an array of two numbers is given, then a range of elements [start, end (exclusive)] is selected.
 383:      * - If null is given, then the entire dimension is selected.
 384:      * @returns {Tensor} A new Tensor containing the selected elements.
 385:      * @throws {Error} If the slice input is invalid.
 386:      */
 387:     slice(...slices) {
 388:         // This allows for slicing with ranges and numbers
 389:         const newTensorDims = [];
 390:         const newOffsets = [];
 391: 
 392:         // slices is an array of numbers or arrays of numbers
 393:         // e.g., slices = [0, [1, 3], null, [0, 3]]
 394:         for (let sliceIndex = 0; sliceIndex < this.dims.length; ++sliceIndex) {
 395:             let slice = slices[sliceIndex];
 396: 
 397:             if (slice === null || slice === undefined) {
 398:                 // null or undefined means take the whole dimension
 399:                 newOffsets.push([0, this.dims[sliceIndex]]);
 400:                 newTensorDims.push(this.dims[sliceIndex]);
 401: 
 402:             } else if (typeof slice === 'number') {
 403:                 slice = safeIndex(slice, this.dims[sliceIndex], sliceIndex);
 404: 
 405:                 // A number means take a single element
 406:                 newOffsets.push([slice, slice + 1]);
 407: 
 408:             } else if (Array.isArray(slice) && slice.length === 2) {
 409:                 // An array of length 2 means take a range of elements
 410:                 let [start, end] = slice;
 411:                 start = start === null
 412:                     ? 0
 413:                     : safeIndex(start, this.dims[sliceIndex], sliceIndex, false);
 414:                 end = end === null
 415:                     ? this.dims[sliceIndex]
 416:                     : safeIndex(end, this.dims[sliceIndex], sliceIndex, false);
 417: 
 418:                 if (start > end) {
 419:                     throw new Error(`Invalid slice: ${slice}`);
 420:                 }
 421: 
 422:                 const offsets = [
 423:                     Math.max(start, 0),
 424:                     Math.min(end, this.dims[sliceIndex])
 425:                 ];
 426: 
 427:                 newOffsets.push(offsets);
 428:                 newTensorDims.push(offsets[1] - offsets[0]);
 429: 
 430:             } else {
 431:                 throw new Error(`Invalid slice: ${slice}`);
 432:             }
 433:         }
 434: 
 435:         const newDims = newOffsets.map(([start, end]) => end - start);
 436:         const newBufferSize = newDims.reduce((a, b) => a * b);
 437: 
 438:         const this_data = this.data;
 439:         // Allocate memory
 440:         // @ts-ignore
 441:         const data = new this_data.constructor(newBufferSize);
 442: 
 443:         // Precompute strides
 444:         const stride = this.stride();
 445: 
 446:         // Detect if the slice is contiguous
 447:         let isContiguous = true;
 448:         for (let i = 1; i < newDims.length; ++i) {
 449:             if (newOffsets[i][0] !== 0 || newOffsets[i][1] !== this.dims[i]) {
 450:                 isContiguous = false;
 451:                 break;
 452:             }
 453:         }
 454: 
 455:         if (isContiguous) {
 456:             // Perform bulk copy for contiguous slices to improve performance
 457:             const start = newOffsets[0][0] * stride[0];
 458:             const end = newOffsets[0][1] * stride[0];
 459: 
 460:             if (ArrayBuffer.isView(this_data)) {
 461:                 // If this.data is a TypedArray, use subarray
 462:                 // @ts-ignore
 463:                 data.set(this_data.subarray(start, end));
 464:             } else if (Array.isArray(this_data)) {
 465:                 // If this.data is a plain array, use slice
 466:                 const slicedData = this_data.slice(start, end);
 467:                 for (let i = 0; i < slicedData.length; ++i) {
 468:                     data[i] = slicedData[i];
 469:                 }
 470:             } else {
 471:                 throw new Error("Unsupported data type for slicing");
 472:             }
 473:         } else {
 474:             // Fallback to manual copying for non-contiguous slices
 475:             for (let i = 0; i < newBufferSize; ++i) {
 476:                 let originalIndex = 0;
 477:                 for (let j = newDims.length - 1, num = i; j >= 0; --j) {
 478:                     const size = newDims[j];
 479:                     originalIndex += ((num % size) + newOffsets[j][0]) * stride[j];
 480:                     num = Math.floor(num / size);
 481:                 }
 482:                 data[i] = this_data[originalIndex];
 483:             }
 484:         }
 485: 
 486:         return new Tensor(this.type, data, newTensorDims);
 487:     }
 488: 
 489:     /**
 490:      * Return a permuted version of this Tensor, according to the provided dimensions.
 491:      * @param  {...number} dims Dimensions to permute.
 492:      * @returns {Tensor} The permuted tensor.
 493:      */
 494:     permute(...dims) {
 495:         return permute(this, dims);
 496:     }
 497: 
 498:     // TODO: implement transpose. For now (backwards compatibility), it's just an alias for permute()
 499:     transpose(...dims) {
 500:         return this.permute(...dims);
 501:     }
 502: 
 503:     /**
 504:      * Returns the sum of each row of the input tensor in the given dimension dim.
 505:      *
 506:      * @param {number} [dim=null] The dimension or dimensions to reduce. If `null`, all dimensions are reduced.
 507:      * @param {boolean} keepdim Whether the output tensor has `dim` retained or not.
 508:      * @returns The summed tensor
 509:      */
 510:     sum(dim = null, keepdim = false) {
 511:         return this.norm(1, dim, keepdim);
 512:     }
 513: 
 514:     /**
 515:      * Returns the matrix norm or vector norm of a given tensor.
 516:      * @param {number|string} [p='fro'] The order of norm
 517:      * @param {number} [dim=null] Specifies which dimension of the tensor to calculate the norm across.
 518:      * If dim is None, the norm will be calculated across all dimensions of input.
 519:      * @param {boolean} [keepdim=false] Whether the output tensors have dim retained or not.
 520:      * @returns {Tensor} The norm of the tensor.
 521:      */
 522:     norm(p = 'fro', dim = null, keepdim = false) {
 523:         if (p === 'fro') {
 524:             // NOTE: Since we only support integer dims, Frobenius norm produces the same result as p=2.
 525:             p = 2;
 526:         } else if (typeof p === 'string') {
 527:             throw Error(`Unsupported norm: ${p}`);
 528:         }
 529: 
 530:         const this_data = this.data;
 531:         const fn = (a, b) => a + (b ** p);
 532: 
 533:         if (dim === null) {
 534:             // @ts-ignore
 535:             const val = this_data.reduce(fn, 0) ** (1 / p);
 536:             return new Tensor(this.type, [val], []);
 537:         }
 538: 
 539:         const [type, result, resultDims] = reduce_helper(fn, this, dim, keepdim);
 540: 
 541:         if (p !== 1) {
 542:             for (let i = 0; i < result.length; ++i) {
 543:                 result[i] = result[i] ** (1 / p);
 544:             }
 545:         }
 546:         return new Tensor(type, result, resultDims);
 547:     }
 548: 
 549:     /**
 550:      * Performs `L_p` normalization of inputs over specified dimension. Operates in place.
 551:      * @param {number} [p=2] The exponent value in the norm formulation
 552:      * @param {number} [dim=1] The dimension to reduce
 553:      * @returns {Tensor} `this` for operation chaining.
 554:      */
 555:     normalize_(p = 2.0, dim = 1) {
 556:         dim = safeIndex(dim, this.dims.length);
 557: 
 558:         const norm = this.norm(p, dim, true);
 559: 
 560:         const this_data = this.data;
 561:         const norm_data = norm.data;
 562:         for (let i = 0; i < this_data.length; ++i) {
 563: 
 564:             // Calculate the index in the resulting array
 565:             let resultIndex = 0;
 566: 
 567:             for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {
 568:                 const size = this.dims[j];
 569:                 if (j !== dim) {
 570:                     const index = num % size;
 571:                     resultIndex += index * resultMultiplier;
 572:                     resultMultiplier *= this.dims[j];
 573:                 }
 574:                 num = Math.floor(num / size);
 575:             }
 576: 
 577:             // Divide by normalized value
 578:             this_data[i] /= norm_data[resultIndex];
 579:         }
 580: 
 581:         return this;
 582:     }
 583: 
 584:     /**
 585:      * Performs `L_p` normalization of inputs over specified dimension.
 586:      * @param {number} [p=2] The exponent value in the norm formulation
 587:      * @param {number} [dim=1] The dimension to reduce
 588:      * @returns {Tensor} The normalized tensor.
 589:      */
 590:     normalize(p = 2.0, dim = 1) {
 591:         return this.clone().normalize_(p, dim);
 592:     }
 593: 
 594:     /**
 595:      * Compute and return the stride of this tensor.
 596:      * Stride is the jump necessary to go from one element to the next one in the specified dimension dim.
 597:      * @returns {number[]} The stride of this tensor.
 598:      */
 599:     stride() {
 600:         return dimsToStride(this.dims);
 601:     }
 602: 
 603:     /**
 604:      * Returns a tensor with all specified dimensions of input of size 1 removed.
 605:      *
 606:      * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.
 607:      * If you would like a copy, use `tensor.clone()` before squeezing.
 608:      *
 609:      * @param {number|number[]} [dim=null] If given, the input will be squeezed only in the specified dimensions.
 610:      * @returns {Tensor} The squeezed tensor
 611:      */
 612:     squeeze(dim = null) {
 613:         return new Tensor(
 614:             this.type,
 615:             this.data,
 616:             calc_squeeze_dims(this.dims, dim)
 617:         )
 618:     }
 619: 
 620:     /**
 621:      * In-place version of @see {@link Tensor.squeeze}
 622:      */
 623:     squeeze_(dim = null) {
 624:         this.dims = calc_squeeze_dims(this.dims, dim);
 625:         return this;
 626:     }
 627: 
 628:     /**
 629:      * Returns a new tensor with a dimension of size one inserted at the specified position.
 630:      *
 631:      * NOTE: The returned tensor shares the same underlying data with this tensor.
 632:      *
 633:      * @param {number} dim The index at which to insert the singleton dimension
 634:      * @returns {Tensor} The unsqueezed tensor
 635:      */
 636:     unsqueeze(dim = null) {
 637:         return new Tensor(
 638:             this.type,
 639:             this.data,
 640:             calc_unsqueeze_dims(this.dims, dim)
 641:         );
 642:     }
 643: 
 644:     /**
 645:      * In-place version of @see {@link Tensor.unsqueeze}
 646:      */
 647:     unsqueeze_(dim = null) {
 648:         this.dims = calc_unsqueeze_dims(this.dims, dim);
 649:         return this;
 650:     }
 651: 
 652:     /**
 653:      * In-place version of @see {@link Tensor.flatten}
 654:      */
 655:     flatten_(start_dim = 0, end_dim = -1) {
 656:         // TODO validate inputs
 657:         end_dim = (end_dim + this.dims.length) % this.dims.length;
 658: 
 659:         let dimsToKeepBefore = this.dims.slice(0, start_dim);
 660:         let dimsToFlatten = this.dims.slice(start_dim, end_dim + 1);
 661:         let dimsToKeepAfter = this.dims.slice(end_dim + 1);
 662: 
 663:         this.dims = [...dimsToKeepBefore, dimsToFlatten.reduce((a, b) => a * b, 1), ...dimsToKeepAfter]
 664:         return this;
 665:     }
 666: 
 667:     /**
 668:      * Flattens input by reshaping it into a one-dimensional tensor.
 669:      * If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim`
 670:      * and ending with `end_dim` are flattened. The order of elements in input is unchanged.
 671:      * @param {number} start_dim the first dim to flatten
 672:      * @param {number} end_dim the last dim to flatten
 673:      * @returns {Tensor} The flattened tensor.
 674:      */
 675:     flatten(start_dim = 0, end_dim = -1) {
 676:         return this.clone().flatten_(start_dim, end_dim);
 677:     }
 678: 
 679:     /**
 680:      * Returns a new tensor with the same data as the `self` tensor but of a different `shape`.
 681:      * @param  {...number} dims the desired size
 682:      * @returns {Tensor} The tensor with the same data but different shape
 683:      */
 684:     view(...dims) {
 685:         // TODO: validate dims
 686:         let inferredIndex = -1;
 687:         for (let i = 0; i < dims.length; ++i) {
 688:             if (dims[i] === -1) {
 689:                 if (inferredIndex !== -1) {
 690:                     throw new Error("Only one dimension can be inferred");
 691:                 }
 692:                 inferredIndex = i;
 693:             }
 694:         }
 695: 
 696:         const this_data = this.data;
 697:         if (inferredIndex !== -1) {
 698:             // Some dimension must be inferred
 699:             const productOther = dims.reduce((product, curr, index) => {
 700:                 return index !== inferredIndex ? product * curr : product
 701:             }, 1);
 702: 
 703:             dims[inferredIndex] = this_data.length / productOther;
 704:         }
 705:         return new Tensor(this.type, this_data, dims); // NOTE: uses same underlying storage
 706:     }
 707: 
 708:     neg_() {
 709:         const this_data = this.data;
 710:         for (let i = 0; i < this_data.length; ++i) {
 711:             this_data[i] = -this_data[i];
 712:         }
 713:         return this;
 714:     }
 715:     neg() {
 716:         return this.clone().neg_();
 717:     }
 718: 
 719:     /**
 720:      * Computes input > val element-wise.
 721:      * @param {number} val The value to compare with.
 722:      * @returns {Tensor} A boolean tensor that is `true` where input is greater than other and `false` elsewhere.
 723:      */
 724:     gt(val) {
 725:         const mask = new Uint8Array(this.data.length);
 726:         const this_data = this.data;
 727:         for (let i = 0; i < this_data.length; ++i) {
 728:             mask[i] = this_data[i] > val ? 1 : 0;
 729:         }
 730:         return new Tensor('bool', mask, this.dims);
 731:     }
 732: 
 733:     /**
 734:      * Computes input < val element-wise.
 735:      * @param {number} val The value to compare with.
 736:      * @returns {Tensor} A boolean tensor that is `true` where input is less than other and `false` elsewhere.
 737:      */
 738:     lt(val) {
 739:         const mask = new Uint8Array(this.data.length);
 740:         const this_data = this.data;
 741:         for (let i = 0; i < this_data.length; ++i) {
 742:             mask[i] = this_data[i] < val ? 1 : 0;
 743:         }
 744:         return new Tensor('bool', mask, this.dims);
 745:     }
 746: 
 747:     /**
 748:      * In-place version of @see {@link Tensor.clamp}
 749:      */
 750:     clamp_(min, max) {
 751:         const this_data = this.data;
 752:         for (let i = 0; i < this_data.length; ++i) {
 753:             this_data[i] = Math.min(Math.max(this_data[i], min), max);
 754:         }
 755:         return this;
 756:     }
 757: 
 758:     /**
 759:      * Clamps all elements in input into the range [ min, max ]
 760:      * @param {number} min lower-bound of the range to be clamped to
 761:      * @param {number} max upper-bound of the range to be clamped to
 762:      * @returns {Tensor} the output tensor.
 763:      */
 764:     clamp(min, max) {
 765:         return this.clone().clamp_(min, max);
 766:     }
 767: 
 768:     /**
 769:      * In-place version of @see {@link Tensor.round}
 770:      */
 771:     round_() {
 772:         const this_data = this.data;
 773:         for (let i = 0; i < this_data.length; ++i) {
 774:             this_data[i] = Math.round(this_data[i]);
 775:         }
 776:         return this;
 777:     }
 778: 
 779:     /**
 780:      * Rounds elements of input to the nearest integer.
 781:      * @returns {Tensor} the output tensor.
 782:      */
 783:     round() {
 784:         return this.clone().round_();
 785:     }
 786: 
 787:     mean(dim = null, keepdim = false) {
 788:         return mean(this, dim, keepdim);
 789:     }
 790: 
 791:     min(dim = null, keepdim = false) {
 792:         if (dim === null) {
 793:             // None to reduce over all dimensions.
 794:             const val = min(this.data)[0];
 795:             return new Tensor(this.type, [val], [/* scalar */]);
 796:         }
 797:         const [type, result, resultDims] = reduce_helper((a, b) => Math.min(a, b), this, dim, keepdim, Infinity);
 798:         return new Tensor(type, result, resultDims);
 799:     }
 800: 
 801:     max(dim = null, keepdim = false) {
 802:         if (dim === null) {
 803:             // None to reduce over all dimensions.
 804:             const val = max(this.data)[0];
 805:             return new Tensor(this.type, [val], [/* scalar */]);
 806:         }
 807:         const [type, result, resultDims] = reduce_helper((a, b) => Math.max(a, b), this, dim, keepdim, -Infinity);
 808:         return new Tensor(type, result, resultDims);
 809:     }
 810: 
 811:     argmin(dim = null, keepdim = false) {
 812:         if (dim !== null) {
 813:             throw new Error("`dim !== null` not yet implemented.");
 814:         }
 815:         const index = min(this.data)[1];
 816:         return new Tensor('int64', [BigInt(index)], []);
 817:     }
 818:     argmax(dim = null, keepdim = false) {
 819:         if (dim !== null) {
 820:             throw new Error("`dim !== null` not yet implemented.");
 821:         }
 822:         const index = max(this.data)[1];
 823:         return new Tensor('int64', [BigInt(index)], []);
 824:     }
 825: 
 826:     /**
 827:      * Performs Tensor dtype conversion.
 828:      * @param {DataType} type The desired data type.
 829:      * @returns {Tensor} The converted tensor.
 830:      */
 831:     to(type) {
 832:         // If the self Tensor already has the correct dtype, then self is returned.
 833:         if (this.type === type) return this;
 834: 
 835:         // Otherwise, the returned tensor is a copy of self with the desired dtype.
 836:         if (!DataTypeMap.hasOwnProperty(type)) {
 837:             throw new Error(`Unsupported type: ${type}`);
 838:         }
 839: 
 840:         // Handle special cases where a mapping function is needed (e.g., where one type is a bigint and the other is a number)
 841:         let map_fn;
 842:         const is_source_bigint = ['int64', 'uint64'].includes(this.type);
 843:         const is_dest_bigint = ['int64', 'uint64'].includes(type);
 844:         if (is_source_bigint && !is_dest_bigint) {
 845:             // TypeError: Cannot convert a BigInt value to a number
 846:             map_fn = Number;
 847:         } else if (!is_source_bigint && is_dest_bigint) {
 848:             // TypeError: Cannot convert [x] to a BigInt
 849:             map_fn = BigInt;
 850:         }
 851: 
 852:         // @ts-ignore
 853:         return new Tensor(type, DataTypeMap[type].from(this.data, map_fn), this.dims);
 854:     }
 855: }
 856: 
 857: /**
 858:  * This creates a nested array of a given type and depth (see examples).
 859:  *
 860:  * @example
 861:  *   NestArray<string, 1>; // string[]
 862:  * @example
 863:  *   NestArray<number, 2>; // number[][]
 864:  * @example
 865:  *   NestArray<string, 3>; // string[][][] etc.
 866:  * @template T
 867:  * @template {number} Depth
 868:  * @template {never[]} [Acc=[]]
 869:  * @typedef {Acc['length'] extends Depth ? T : NestArray<T[], Depth, [...Acc, never]>} NestArray
 870:  */
 871: 
 872: /**
 873:  * Reshapes a 1-dimensional array into an n-dimensional array, according to the provided dimensions.
 874:  *
 875:  * @example
 876:  *   reshape([10                    ], [1      ]); // Type: number[]      Value: [10]
 877:  *   reshape([1, 2, 3, 4            ], [2, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4]]
 878:  *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [2, 2, 2]); // Type: number[][][]  Value: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]
 879:  *   reshape([1, 2, 3, 4, 5, 6, 7, 8], [4, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4], [5, 6], [7, 8]]
 880:  * @param {T[]|DataArray} data The input array to reshape.
 881:  * @param {DIM} dimensions The target shape/dimensions.
 882:  * @template T
 883:  * @template {[number]|number[]} DIM
 884:  * @returns {NestArray<T, DIM["length"]>} The reshaped array.
 885:  */
 886: function reshape(data, dimensions) {
 887: 
 888:     const totalElements = data.length;
 889:     const dimensionSize = dimensions.reduce((a, b) => a * b);
 890: 
 891:     if (totalElements !== dimensionSize) {
 892:         throw Error(`cannot reshape array of size ${totalElements} into shape (${dimensions})`);
 893:     }
 894: 
 895:     /** @type {any} */
 896:     let reshapedArray = data;
 897: 
 898:     for (let i = dimensions.length - 1; i >= 0; i--) {
 899:         reshapedArray = reshapedArray.reduce((acc, val) => {
 900:             let lastArray = acc[acc.length - 1];
 901: 
 902:             if (lastArray.length < dimensions[i]) {
 903:                 lastArray.push(val);
 904:             } else {
 905:                 acc.push([val]);
 906:             }
 907: 
 908:             return acc;
 909:         }, [[]]);
 910:     }
 911: 
 912:     return reshapedArray[0];
 913: }
 914: 
 915: /**
 916:  * Permutes a tensor according to the provided axes.
 917:  * @param {any} tensor The input tensor to permute.
 918:  * @param {Array} axes The axes to permute the tensor along.
 919:  * @returns {Tensor} The permuted tensor.
 920:  */
 921: export function permute(tensor, axes) {
 922:     const [permutedData, shape] = permute_data(tensor.data, tensor.dims, axes);
 923:     return new Tensor(tensor.type, permutedData, shape);
 924: }
 925: 
 926: 
 927: /**
 928:  * Interpolates an Tensor to the given size.
 929:  * @param {Tensor} input The input tensor to interpolate. Data must be channel-first (i.e., [c, h, w])
 930:  * @param {number[]} size The output size of the image
 931:  * @param {string} mode The interpolation mode
 932:  * @param {boolean} align_corners Whether to align corners.
 933:  * @returns {Tensor} The interpolated tensor.
 934:  */
 935: export function interpolate(input, [out_height, out_width], mode = 'bilinear', align_corners = false) {
 936: 
 937:     // Input image dimensions
 938:     const in_channels = input.dims.at(-3) ?? 1;
 939:     const in_height = input.dims.at(-2);
 940:     const in_width = input.dims.at(-1);
 941: 
 942:     let output = interpolate_data(
 943:         /** @type {import('./maths.js').TypedArray}*/(input.data),
 944:         [in_channels, in_height, in_width],
 945:         [out_height, out_width],
 946:         mode,
 947:         align_corners
 948:     );
 949:     return new Tensor(input.type, output, [in_channels, out_height, out_width]);
 950: }
 951: 
 952: 
 953: /**
 954:  * Down/up samples the input.
 955:  * Inspired by https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html.
 956:  * @param {Tensor} input the input tensor
 957:  * @param {Object} options the options for the interpolation
 958:  * @param {[number, number]|[number, number, number]|[number, number, number, number]} [options.size=null] output spatial size.
 959:  * @param {"nearest"|"bilinear"|"bicubic"} [options.mode='bilinear'] algorithm used for upsampling
 960:  * @returns {Promise<Tensor>} The interpolated tensor.
 961:  */
 962: export async function interpolate_4d(input, {
 963:     size = null,
 964:     mode = 'bilinear',
 965: } = {}) {
 966: 
 967:     // Error checking
 968:     if (input.dims.length !== 4) {
 969:         throw new Error('`interpolate_4d` currently only supports 4D input.');
 970:     }
 971:     if (!size) {
 972:         // TODO: support scale_factor
 973:         throw new Error('`interpolate_4d` requires a `size` argument.');
 974:     }
 975: 
 976:     // Fill in missing dimensions
 977:     let targetDims;
 978:     if (size.length === 2) {
 979:         targetDims = [...input.dims.slice(0, 2), ...size];
 980:     } else if (size.length === 3) {
 981:         targetDims = [input.dims[0], ...size];
 982:     } else if (size.length === 4) {
 983:         targetDims = size;
 984:     } else {
 985:         throw new Error('`size` must be of length 2, 3, or 4.');
 986:     }
 987: 
 988:     let op;
 989:     if (mode === 'nearest') {
 990:         op = await TensorOpRegistry.nearest_interpolate_4d;
 991:     } else if (mode === 'bilinear') {
 992:         op = await TensorOpRegistry.bilinear_interpolate_4d;
 993:     } else if (mode === 'bicubic') {
 994:         op = await TensorOpRegistry.bicubic_interpolate_4d;
 995:     } else {
 996:         throw new Error(`Unsupported mode: ${mode}`);
 997:     }
 998: 
 999:     const sizeTensor = new Tensor('int64', new BigInt64Array(targetDims.map(BigInt)), [targetDims.length]);
1000:     return await op({ x: input, s: sizeTensor });
1001: }
1002: 
1003: /**
1004:  * Matrix product of two tensors.
1005:  * Inspired by https://pytorch.org/docs/stable/generated/torch.matmul.html
1006:  * @param {Tensor} a the first tensor to be multiplied
1007:  * @param {Tensor} b the second tensor to be multiplied
1008:  * @returns {Promise<Tensor>} The matrix product of the two tensors.
1009:  */
1010: export async function matmul(a, b) {
1011:     const op = await TensorOpRegistry.matmul;
1012:     return await op({ a, b });
1013: }
1014: 
1015: /**
1016:  * Computes the one dimensional Fourier transform of real-valued input.
1017:  * Inspired by https://pytorch.org/docs/stable/generated/torch.fft.rfft.html
1018:  * @param {Tensor} x the real input tensor
1019:  * @param {Tensor} a The dimension along which to take the one dimensional real FFT.
1020:  * @returns {Promise<Tensor>} the output tensor.
1021:  */
1022: export async function rfft(x, a) {
1023:     const op = await TensorOpRegistry.rfft;
1024:     return await op({ x, a });
1025: }
1026: 
1027: 
1028: /**
1029:  * Returns the k largest elements of the given input tensor.
1030:  * Inspired by https://pytorch.org/docs/stable/generated/torch.topk.html
1031:  * @param {Tensor} x the input tensor
1032:  * @param {number} [k] the k in "top-k"
1033:  * @returns {Promise<[Tensor, Tensor]>} the output tuple of (Tensor, LongTensor) of top-k elements and their indices.
1034:  */
1035: export async function topk(x, k) {
1036:     const op = await TensorOpRegistry.top_k;
1037: 
1038:     if (k == null) {
1039:         k = x.dims.at(-1);
1040:     } else {
1041:         k = Math.min(k, x.dims.at(-1));
1042:     }
1043:     return await op({
1044:         x,
1045:         k: new Tensor(
1046:             'int64',
1047:             [BigInt(k)],
1048:             [1]
1049:         )
1050:     });
1051: }
1052: 
1053: 
1054: const arrayToIndexTensor = (array) => new Tensor('int64', array, [array.length]);
1055: /**
1056:  * Slice a multidimensional float32 tensor.
1057:  * @param {Tensor} data: Tensor of data to extract slices from
1058:  * @param {number[]} starts: 1-D array of starting indices of corresponding axis in axes
1059:  * @param {number[]} ends: 1-D array of ending indices (exclusive) of corresponding axis in axes
1060:  * @param {number[]} axes: 1-D array of axes that starts and ends apply to
1061:  * @param {number[]} [steps]: 1-D array of slice step of corresponding axis in axes.
1062:  * @returns {Promise<Tensor>} Sliced data tensor.
1063:  */
1064: export async function slice(data, starts, ends, axes, steps) {
1065:     const op = await TensorOpRegistry.slice;
1066:     return await op({
1067:         x: data,
1068:         s: arrayToIndexTensor(starts),
1069:         e: arrayToIndexTensor(ends),
1070:         a: arrayToIndexTensor(axes),
1071:         t: arrayToIndexTensor(steps ?? new Array(axes.length).fill(1)),
1072:     });
1073: }
1074: 
1075: 
1076: /**
1077:  * Perform mean pooling of the last hidden state followed by a normalization step.
1078:  * @param {Tensor} last_hidden_state Tensor of shape [batchSize, seqLength, embedDim]
1079:  * @param {Tensor} attention_mask Tensor of shape [batchSize, seqLength]
1080:  * @returns {Tensor} Returns a new Tensor of shape [batchSize, embedDim].
1081:  */
1082: export function mean_pooling(last_hidden_state, attention_mask) {
1083:     // last_hidden_state: [batchSize, seqLength, embedDim]
1084:     // attention_mask:    [batchSize, seqLength]
1085:     const lastHiddenStateData = last_hidden_state.data;
1086:     const attentionMaskData = attention_mask.data;
1087: 
1088:     const shape = [last_hidden_state.dims[0], last_hidden_state.dims[2]];
1089: 
1090:     // @ts-ignore
1091:     const returnedData = new lastHiddenStateData.constructor(shape[0] * shape[1]);
1092:     const [batchSize, seqLength, embedDim] = last_hidden_state.dims;
1093: 
1094:     let outIndex = 0;
1095:     for (let i = 0; i < batchSize; ++i) {
1096:         const offset = i * embedDim * seqLength;
1097: 
1098:         for (let k = 0; k < embedDim; ++k) {
1099:             let sum = 0;
1100:             let count = 0;
1101: 
1102:             const attnMaskOffset = i * seqLength;
1103:             const offset2 = offset + k;
1104:             // Pool over all words in sequence
1105:             for (let j = 0; j < seqLength; ++j) {
1106:                 // index into attention mask
1107:                 const attn = Number(attentionMaskData[attnMaskOffset + j]);
1108: 
1109:                 count += attn;
1110:                 sum += lastHiddenStateData[offset2 + j * embedDim] * attn;
1111:             }
1112: 
1113:             const avg = sum / count;
1114:             returnedData[outIndex++] = avg;
1115:         }
1116:     }
1117: 
1118:     return new Tensor(
1119:         last_hidden_state.type,
1120:         returnedData,
1121:         shape
1122:     )
1123: }
1124: 
1125: /**
1126:  * Apply Layer Normalization for last certain number of dimensions.
1127:  * @param {Tensor} input The input tensor
1128:  * @param {number[]} normalized_shape input shape from an expected input of size
1129:  * @param {Object} options The options for the layer normalization
1130:  * @param {number} [options.eps=1e-5] A value added to the denominator for numerical stability.
1131:  * @returns {Tensor} The normalized tensor.
1132:  */
1133: export function layer_norm(input, normalized_shape, {
1134:     eps = 1e-5,
1135: } = {}) {
1136:     if (input.dims.length !== 2) {
1137:         throw new Error('`layer_norm` currently only supports 2D input.');
1138:     }
1139: 
1140:     const [batchSize, featureDim] = input.dims;
1141: 
1142:     if (normalized_shape.length !== 1 && normalized_shape[0] !== featureDim) {
1143:         throw new Error('`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.');
1144:     }
1145: 
1146:     const [std, mean] = std_mean(input, 1, 0, true);
1147:     const stdData = /** @type {Float32Array} */(std.data);
1148:     const meanData = /** @type {Float32Array} */(mean.data);
1149: 
1150:     const inputData = /** @type {Float32Array} */(input.data);
1151: 
1152:     // @ts-ignore
1153:     const returnedData = new inputData.constructor(inputData.length);
1154: 
1155:     for (let i = 0; i < batchSize; ++i) {
1156:         const offset = i * featureDim;
1157:         for (let j = 0; j < featureDim; ++j) {
1158:             const offset2 = offset + j;
1159:             returnedData[offset2] = (inputData[offset2] - meanData[i]) / (stdData[i] + eps);
1160:         }
1161:     }
1162:     return new Tensor(input.type, returnedData, input.dims);
1163: }
1164: 
1165: /**
1166:  * Helper function to calculate new dimensions when performing a squeeze operation.
1167:  * @param {number[]} dims The dimensions of the tensor.
1168:  * @param {number|number[]|null} dim The dimension(s) to squeeze.
1169:  * @returns {number[]} The new dimensions.
1170:  * @private
1171:  */
1172: function calc_squeeze_dims(dims, dim) {
1173:     dims = dims.slice();
1174:     if (dim === null) {
1175:         dims = dims.filter((d) => d !== 1);
1176:     } else if (typeof dim === 'number') {
1177:         if (dims[dim] === 1) {
1178:             dims.splice(dim, 1);
1179:         }
1180:     } else if (Array.isArray(dim)) {
1181:         dims = dims.filter((x, i) => {
1182:             return x !== 1 || !dim.includes(i);
1183:         });
1184:     }
1185:     return dims;
1186: }
1187: 
1188: /**
1189:  * Helper function to calculate new dimensions when performing an unsqueeze operation.
1190:  * @param {number[]} dims The dimensions of the tensor.
1191:  * @param {number} dim The dimension to unsqueeze.
1192:  * @returns {number[]} The new dimensions.
1193:  * @private
1194:  */
1195: function calc_unsqueeze_dims(dims, dim) {
1196:     // Dimension out of range (e.g., "expected to be in range of [-4, 3], but got 4")
1197:     // + 1 since we allow inserting at the end (i.e. dim = -1)
1198:     dim = safeIndex(dim, dims.length + 1);
1199:     dims = dims.slice();
1200:     // Insert 1 into specified dimension
1201:     dims.splice(dim, 0, 1);
1202:     return dims;
1203: }
1204: 
1205: /**
1206:  * Safely calculate the index for an array of a given size, allowing negative indexing.
1207:  * @param {number} index The index that will be used.
1208:  * @param {number} size The size of the array.
1209:  * @param {number} [dimension=null] The dimension that the index is for (optional).
1210:  * @returns {number} The index, guaranteed to be non-negative and less than `arrayLength`.
1211:  *
1212:  * @throws {Error} If the index is out of range.
1213:  * @private
1214:  */
1215: function safeIndex(index, size, dimension = null, boundsCheck = true) {
1216:     if (index < -size || index >= size) {
1217:         if (boundsCheck) {
1218:             throw new Error(`IndexError: index ${index} is out of bounds for dimension${dimension === null ? '' : ' ' + dimension} with size ${size}`);
1219:         } else {
1220:             return index < -size ? 0 : size;
1221:         }
1222:     }
1223: 
1224:     if (index < 0) {
1225:         // Negative indexing, ensuring positive index
1226:         index = ((index % size) + size) % size;
1227:     }
1228:     return index;
1229: }
1230: 
1231: /**
1232:  * Concatenates an array of tensors along a specified dimension.
1233:  * @param {Tensor[]} tensors The array of tensors to concatenate.
1234:  * @param {number} dim The dimension to concatenate along.
1235:  * @returns {Tensor} The concatenated tensor.
1236:  */
1237: export function cat(tensors, dim = 0) {
1238:     dim = safeIndex(dim, tensors[0].dims.length);
1239: 
1240:     // TODO do validation of shapes
1241: 
1242:     const resultDims = tensors[0].dims.slice();
1243:     resultDims[dim] = tensors.reduce((a, b) => a + b.dims[dim], 0);
1244: 
1245:     // Create a new array to store the accumulated values
1246:     const resultSize = resultDims.reduce((a, b) => a * b, 1);
1247:     // @ts-ignore
1248:     const result = new tensors[0].data.constructor(resultSize);
1249: 
1250:     // Create output tensor of same type as first
1251:     const resultType = tensors[0].type;
1252: 
1253:     if (dim === 0) {
1254:         // Handle special case for performance reasons
1255: 
1256:         let offset = 0;
1257:         for (const tensor of tensors) {
1258:             const tensorData = tensor.data;
1259:             result.set(tensorData, offset);
1260:             offset += tensorData.length;
1261:         }
1262: 
1263:     } else {
1264: 
1265:         let currentDim = 0;
1266: 
1267:         for (let t = 0; t < tensors.length; ++t) {
1268:             const { data, dims } = tensors[t];
1269: 
1270:             // Iterate over the data array
1271:             for (let i = 0; i < data.length; ++i) {
1272:                 // Calculate the index in the resulting array
1273:                 let resultIndex = 0;
1274: 
1275:                 for (let j = dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {
1276:                     const size = dims[j];
1277:                     let index = num % size;
1278:                     if (j === dim) {
1279:                         index += currentDim;
1280:                     }
1281:                     resultIndex += index * resultMultiplier;
1282:                     resultMultiplier *= resultDims[j];
1283:                     num = Math.floor(num / size);
1284:                 }
1285:                 // Accumulate the value at the current index
1286:                 result[resultIndex] = data[i];
1287:             }
1288: 
1289:             currentDim += dims[dim];
1290:         }
1291:     }
1292:     return new Tensor(resultType, result, resultDims);
1293: }
1294: 
1295: /**
1296:  * Stack an array of tensors along a specified dimension.
1297:  * @param {Tensor[]} tensors The array of tensors to stack.
1298:  * @param {number} dim The dimension to stack along.
1299:  * @returns {Tensor} The stacked tensor.
1300:  */
1301: export function stack(tensors, dim = 0) {
1302:     // TODO do validation of shapes
1303:     // NOTE: stack expects each tensor to be equal size
1304:     return cat(tensors.map(t => t.unsqueeze(dim)), dim);
1305: }
1306: 
1307: 
1308: /**
1309:  * @param {(previousValue: any, currentValue: any, currentIndex?: number, resultIndex?: number) => any} callbackfn
1310:  * @param {Tensor} input the input tensor.
1311:  * @param {number|null} dim the dimension to reduce.
1312:  * @param {boolean} keepdim whether the output tensor has dim retained or not.
1313:  * @returns {[DataType, any, number[]]} The reduced tensor data.
1314:  */
1315: function reduce_helper(callbackfn, input, dim = null, keepdim = false, initialValue = null) {
1316:     const inputData = input.data;
1317:     const inputDims = input.dims;
1318: 
1319:     // Negative indexing
1320:     dim = safeIndex(dim, inputDims.length);
1321: 
1322:     // Calculate the shape of the resulting array after summation
1323:     const resultDims = inputDims.slice(); // Copy the original dimensions
1324:     resultDims[dim] = 1; // Remove the specified axis
1325: 
1326:     // Create a new array to store the accumulated values
1327:     // @ts-ignore
1328:     const result = new inputData.constructor(inputData.length / inputDims[dim]);
1329:     if (initialValue !== null) {
1330:         result.fill(initialValue);
1331:     }
1332: 
1333:     // Iterate over the data array
1334:     for (let i = 0; i < inputData.length; ++i) {
1335: 
1336:         // Calculate the index in the resulting array
1337:         let resultIndex = 0;
1338: 
1339:         for (let j = inputDims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {
1340:             const size = inputDims[j];
1341:             if (j !== dim) {
1342:                 const index = num % size;
1343:                 resultIndex += index * resultMultiplier;
1344:                 resultMultiplier *= resultDims[j];
1345:             }
1346:             num = Math.floor(num / size);
1347:         }
1348: 
1349:         // Accumulate the value at the current index
1350:         result[resultIndex] = callbackfn(result[resultIndex], inputData[i], i, resultIndex);
1351:     }
1352: 
1353:     if (!keepdim) resultDims.splice(dim, 1);
1354: 
1355:     return [input.type, result, resultDims];
1356: }
1357: 
1358: 
1359: /**
1360:  * Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension or `null` to reduce over all dimensions.
1361:  * @param {Tensor} input the input tenso
1362:  * @param {number|null} dim the dimension to reduce. If None, all dimensions are reduced.
1363:  * @param {number} correction difference between the sample size and sample degrees of freedom. Defaults to Bessel's correction, correction=1.
1364:  * @param {boolean} keepdim whether the output tensor has dim retained or not.
1365:  * @returns {Tensor[]} A tuple of (std, mean) tensors.
1366:  */
1367: export function std_mean(input, dim = null, correction = 1, keepdim = false) {
1368:     const inputData = /** @type {Float32Array} */(input.data);
1369:     const inputDims = input.dims;
1370: 
1371:     if (dim === null) {
1372:         // None to reduce over all dimensions.
1373:         const sum = inputData.reduce((a, b) => a + b, 0);
1374:         const mean = sum / inputData.length;
1375:         const std = Math.sqrt(inputData.reduce((a, b) => a + (b - mean) ** 2, 0) / (inputData.length - correction));
1376: 
1377:         const meanTensor = new Tensor(input.type, [mean], [/* scalar */]);
1378:         const stdTensor = new Tensor(input.type, [std], [/* scalar */]);
1379: 
1380:         return [stdTensor, meanTensor];
1381:     }
1382:     dim = safeIndex(dim, inputDims.length);
1383:     const meanTensor = mean(input, dim, keepdim);
1384:     const meanTensorData = meanTensor.data;
1385: 
1386:     // Compute squared sum
1387:     const [type, result, resultDims] = reduce_helper((a, b, i, j) => a + (b - meanTensorData[j]) ** 2, input, dim, keepdim);
1388: 
1389:     // Square root of the squared sum
1390:     for (let i = 0; i < result.length; ++i) {
1391:         result[i] = Math.sqrt(result[i] / (inputDims[dim] - correction));
1392:     }
1393: 
1394:     const stdTensor = new Tensor(type, result, resultDims);
1395: 
1396:     return [stdTensor, meanTensor];
1397: }
1398: 
1399: /**
1400:  * Returns the mean value of each row of the input tensor in the given dimension dim.
1401:  * @param {Tensor} input the input tensor.
1402:  * @param {number|null} dim the dimension to reduce.
1403:  * @param {boolean} keepdim whether the output tensor has dim retained or not.
1404:  * @returns {Tensor} A new tensor with means taken along the specified dimension.
1405:  */
1406: export function mean(input, dim = null, keepdim = false) {
1407:     const inputDims = input.dims;
1408:     const inputData = /** @type {Float32Array} */(input.data);
1409: 
1410:     if (dim === null) {
1411:         // None to reduce over all dimensions.
1412:         const val = inputData.reduce((a, b) => a + b, 0);
1413:         return new Tensor(input.type, [val / inputData.length], [/* scalar */]);
1414:     }
1415:     dim = safeIndex(dim, inputDims.length);
1416: 
1417:     // Compute sum
1418:     const [type, result, resultDims] = reduce_helper((a, b) => a + b, input, dim, keepdim);
1419: 
1420:     // Divide by number of elements in the dimension
1421:     if (inputDims[dim] !== 1) {
1422:         for (let i = 0; i < result.length; ++i) {
1423:             result[i] /= inputDims[dim];
1424:         }
1425:     }
1426: 
1427:     return new Tensor(type, result, resultDims);
1428: }
1429: 
1430: 
1431: function dimsToStride(dims) {
1432:     const stride = new Array(dims.length);
1433:     for (let i = dims.length - 1, s2 = 1; i >= 0; --i) {
1434:         stride[i] = s2;
1435:         s2 *= dims[i];
1436:     }
1437:     return stride;
1438: }
1439: 
1440: function fullHelper(size, fill_value, dtype, cls) {
1441:     const numElements = size.reduce((a, b) => a * b, 1);
1442:     return new Tensor(
1443:         dtype,
1444:         new cls(numElements).fill(fill_value),
1445:         size
1446:     )
1447: }
1448: 
1449: /**
1450:  * Creates a tensor of size size filled with fill_value. The tensor's dtype is inferred from fill_value.
1451:  * @param {number[]} size A sequence of integers defining the shape of the output tensor.
1452:  * @param {number|bigint|boolean} fill_value The value to fill the output tensor with.
1453:  * @returns {Tensor} The filled tensor.
1454:  */
1455: export function full(size, fill_value) {
1456:     let dtype;
1457:     let typedArrayCls;
1458:     if (typeof fill_value === 'number') {
1459:         dtype = 'float32';
1460:         typedArrayCls = Float32Array;
1461:     } else if (typeof fill_value === 'bigint') {
1462:         dtype = 'int64';
1463:         typedArrayCls = BigInt64Array;
1464:     } else if (typeof fill_value === 'boolean') {
1465:         dtype = 'bool';
1466:         typedArrayCls = Uint8Array;
1467:     } else {
1468:         // TODO: support other dtypes
1469:         throw new Error(`Unsupported data type: ${typeof fill_value}`);
1470:     }
1471:     return fullHelper(size, fill_value, dtype, typedArrayCls);
1472: }
1473: 
1474: export function full_like(tensor, fill_value) {
1475:     return full(tensor.dims, fill_value);
1476: }
1477: 
1478: /**
1479:  * Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.
1480:  * @param {number[]} size A sequence of integers defining the shape of the output tensor.
1481:  * @returns {Tensor} The ones tensor.
1482:  */
1483: export function ones(size) {
1484:     return fullHelper(size, 1n, 'int64', BigInt64Array);
1485: }
1486: 
1487: /**
1488:  * Returns a tensor filled with the scalar value 1, with the same size as input.
1489:  * @param {Tensor} tensor The size of input will determine size of the output tensor.
1490:  * @returns {Tensor} The ones tensor.
1491:  */
1492: export function ones_like(tensor) {
1493:     return ones(tensor.dims);
1494: }
1495: 
1496: /**
1497:  * Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.
1498:  * @param {number[]} size A sequence of integers defining the shape of the output tensor.
1499:  * @returns {Tensor} The zeros tensor.
1500:  */
1501: export function zeros(size) {
1502:     return fullHelper(size, 0n, 'int64', BigInt64Array);
1503: }
1504: 
1505: /**
1506:  * Returns a tensor filled with the scalar value 0, with the same size as input.
1507:  * @param {Tensor} tensor The size of input will determine size of the output tensor.
1508:  * @returns {Tensor} The zeros tensor.
1509:  */
1510: export function zeros_like(tensor) {
1511:     return zeros(tensor.dims);
1512: }
1513: 
1514: /**
1515:  * Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)
1516:  * @param {number[]} size A sequence of integers defining the shape of the output tensor.
1517:  * @returns {Tensor} The random tensor.
1518:  */
1519: export function rand(size) {
1520:     const length = size.reduce((a, b) => a * b, 1);
1521:     return new Tensor(
1522:         "float32",
1523:         Float32Array.from({ length }, () => Math.random()),
1524:         size,
1525:     )
1526: }
1527: 
1528: /**
1529:  * Quantizes the embeddings tensor to binary or unsigned binary precision.
1530:  * @param {Tensor} tensor The tensor to quantize.
1531:  * @param {'binary'|'ubinary'} precision The precision to use for quantization.
1532:  * @returns {Tensor} The quantized tensor.
1533:  */
1534: export function quantize_embeddings(tensor, precision) {
1535:     if (tensor.dims.length !== 2) {
1536:         throw new Error("The tensor must have 2 dimensions");
1537:     }
1538:     if (tensor.dims.at(-1) % 8 !== 0) {
1539:         throw new Error("The last dimension of the tensor must be a multiple of 8");
1540:     }
1541:     if (!['binary', 'ubinary'].includes(precision)) {
1542:         throw new Error("The precision must be either 'binary' or 'ubinary'");
1543:     }
1544: 
1545:     const signed = precision === 'binary';
1546:     const dtype = signed ? 'int8' : 'uint8';
1547: 
1548:     // Create a typed array to store the packed bits
1549:     const cls = signed ? Int8Array : Uint8Array;
1550:     const inputData = tensor.data;
1551:     const outputData = new cls(inputData.length / 8);
1552: 
1553:     // Iterate over each number in the array
1554:     for (let i = 0; i < inputData.length; ++i) {
1555:         // Determine if the number is greater than 0
1556:         const bit = inputData[i] > 0 ? 1 : 0;
1557: 
1558:         // Calculate the index in the typed array and the position within the byte
1559:         const arrayIndex = Math.floor(i / 8);
1560:         const bitPosition = i % 8;
1561: 
1562:         // Pack the bit into the typed array
1563:         outputData[arrayIndex] |= bit << (7 - bitPosition);
1564:         if (signed && bitPosition === 0) {
1565:             outputData[arrayIndex] -= 128;
1566:         }
1567:     };
1568: 
1569:     return new Tensor(dtype, outputData, [tensor.dims[0], tensor.dims[1] / 8]);
1570: }
</file>

<file path="src/tjs/utils/video.js">
  1: import { RawImage } from "./image.js";
  2: import { apis } from "../env.js";
  3: 
  4: export class RawVideoFrame {
  5: 
  6:     /**
  7:      * @param {RawImage} image
  8:      * @param {number} timestamp
  9:      */
 10:     constructor(image, timestamp) {
 11:         this.image = image;
 12:         this.timestamp = timestamp;
 13:     }
 14: }
 15: 
 16: export class RawVideo {
 17:     /**
 18:      * @param {RawVideoFrame[]|RawImage[]} frames
 19:      * @param {number} duration
 20:      */
 21:     constructor(frames, duration) {
 22:         if (frames.length > 0 && frames[0] instanceof RawImage) {
 23:             // Assume uniform timestamps
 24:             frames = frames.map((image, i) => new RawVideoFrame(image, (i + 1) / (frames.length + 1) * duration));
 25:         }
 26:         this.frames = /** @type {RawVideoFrame[]} */ (frames);
 27:         this.duration = duration;
 28:     }
 29: 
 30:     get width() {
 31:         return this.frames[0].image.width;
 32:     }
 33:     get height() {
 34:         return this.frames[0].image.height;
 35:     }
 36: 
 37:     get fps() {
 38:         return this.frames.length / this.duration;
 39:     }
 40: }
 41: 
 42: 
 43: /**
 44:  * Loads a video.
 45:  *
 46:  * @param {string|Blob|HTMLVideoElement} src The video to process.
 47:  * @param {Object} [options] Optional parameters.
 48:  * @param {number} [options.num_frames=null] The number of frames to sample uniformly.
 49:  * @param {number} [options.fps=null] The number of frames to sample per second.
 50:  *
 51:  * @returns {Promise<RawVideo>} The loaded video.
 52:  */
 53: export async function load_video(src, { num_frames = null, fps = null } = {}) {
 54:     if (!apis.IS_BROWSER_ENV) {
 55:         throw new Error("`load_video` is currently only supported in browser environments.");
 56:     }
 57: 
 58:     // TODO: Support efficiently loading all frames using the WebCodecs API.
 59:     // Specfically, https://developer.mozilla.org/en-US/docs/Web/API/VideoDecoder
 60:     if (num_frames == null && fps == null) {
 61:         throw new Error("Either num_frames or fps must be provided.");
 62:     }
 63: 
 64:     const frames = [];
 65: 
 66:     const video = document.createElement("video");
 67:     video.crossOrigin = "anonymous";
 68:     video.muted = true; // mute to allow autoplay and seeking
 69: 
 70:     if (typeof src === 'string') {
 71:         video.src = src;
 72:     } else if (src instanceof Blob) {
 73:         video.src = URL.createObjectURL(src);
 74:     } else if (src instanceof HTMLVideoElement) {
 75:         video.src = src.src;
 76:     } else {
 77:         throw new Error("Invalid URL or video element provided.");
 78:     }
 79:     // Wait for metadata to load to obtain duration
 80:     await new Promise((resolve) => video.onloadedmetadata = resolve);
 81: 
 82:     if (video.seekable.start(0) === video.seekable.end(0)) {
 83:         // Fallback: Download entire video if not seekable
 84:         const response = await fetch(video.src);
 85:         const blob = await response.blob();
 86:         video.src = URL.createObjectURL(blob);
 87:         await new Promise((resolve) => video.onloadedmetadata = resolve);
 88:     }
 89: 
 90:     const duration = video.duration;
 91: 
 92:     let count, step;
 93:     if (num_frames != null) {
 94:         count = num_frames;
 95:         step = num_frames === 1 ? 0 : duration / (num_frames - 1);
 96:     } else {
 97:         step = 1 / fps;
 98:         count = Math.floor(duration / step);
 99:     }
100: 
101:     // Build an array of sample times based on num_frames or fps
102:     let sampleTimes = [];
103:     for (let i = 0; i < count; ++i) {
104:         sampleTimes.push(num_frames === 1 ? duration / 2 : i * step);
105:     }
106: 
107:     const canvas = document.createElement("canvas");
108:     canvas.width = video.videoWidth;
109:     canvas.height = video.videoHeight;
110:     const ctx = canvas.getContext("2d", { willReadFrequently: true });
111:     for (const t of sampleTimes) {
112:         video.currentTime = t;
113:         await new Promise((resolve) => {
114:             video.onseeked = resolve;
115:         });
116:         ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
117:         const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
118:         const frameData = new RawImage(imageData.data, canvas.width, canvas.height, 4);
119: 
120:         const frame = new RawVideoFrame(frameData, t);
121:         frames.push(frame);
122:     }
123: 
124:     // Clean up video element.
125:     video.remove();
126: 
127:     return new RawVideo(frames, duration);
128: }
</file>

<file path="src/tjs/env.js">
  1: /**
  2:  * @file Module used to configure Transformers.js.
  3:  * 
  4:  * **Example:** Disable remote models.
  5:  * ```javascript
  6:  * import { env } from '@huggingface/transformers';
  7:  * env.allowRemoteModels = false;
  8:  * ```
  9:  * 
 10:  * **Example:** Set local model path.
 11:  * ```javascript
 12:  * import { env } from '@huggingface/transformers';
 13:  * env.localModelPath = '/path/to/local/models/';
 14:  * ```
 15:  * 
 16:  * **Example:** Set cache directory.
 17:  * ```javascript
 18:  * import { env } from '@huggingface/transformers';
 19:  * env.cacheDir = '/path/to/cache/directory/';
 20:  * ```
 21:  * 
 22:  * @module env
 23:  */
 24: 
 25: import fs from 'node:fs';
 26: import path from 'node:path';
 27: import url from 'node:url';
 28: 
 29: const VERSION = '3.7.2';
 30: 
 31: // Check if various APIs are available (depends on environment)
 32: const IS_BROWSER_ENV = typeof window !== "undefined" && typeof window.document !== "undefined";
 33: const IS_WEBWORKER_ENV = typeof self !== "undefined" && (['DedicatedWorkerGlobalScope', 'ServiceWorkerGlobalScope', 'SharedWorkerGlobalScope'].includes(self.constructor?.name));
 34: const IS_WEB_CACHE_AVAILABLE = typeof self !== "undefined" && 'caches' in self;
 35: const IS_WEBGPU_AVAILABLE = typeof navigator !== 'undefined' && 'gpu' in navigator;
 36: const IS_WEBNN_AVAILABLE = typeof navigator !== 'undefined' && 'ml' in navigator;
 37: 
 38: const IS_PROCESS_AVAILABLE = typeof process !== 'undefined';
 39: const IS_NODE_ENV = IS_PROCESS_AVAILABLE && process?.release?.name === 'node';
 40: const IS_FS_AVAILABLE = !isEmpty(fs);
 41: const IS_PATH_AVAILABLE = !isEmpty(path);
 42: 
 43: // Runtime detection
 44: const IS_DENO_RUNTIME = typeof globalThis.Deno !== 'undefined';
 45: const IS_BUN_RUNTIME = typeof globalThis.Bun !== 'undefined';
 46: 
 47: /**
 48:  * A read-only object containing information about the APIs available in the current environment.
 49:  */
 50: export const apis = Object.freeze({
 51:     /** Whether we are running in a browser environment (and not a web worker) */
 52:     IS_BROWSER_ENV,
 53: 
 54:     /** Whether we are running in a web worker environment */
 55:     IS_WEBWORKER_ENV,
 56: 
 57:     /** Whether the Cache API is available */
 58:     IS_WEB_CACHE_AVAILABLE,
 59: 
 60:     /** Whether the WebGPU API is available */
 61:     IS_WEBGPU_AVAILABLE,
 62: 
 63:     /** Whether the WebNN API is available */
 64:     IS_WEBNN_AVAILABLE,
 65: 
 66:     /** Whether the Node.js process API is available */
 67:     IS_PROCESS_AVAILABLE,
 68: 
 69:     /** Whether we are running in a Node.js-like environment (node, deno, bun) */
 70:     IS_NODE_ENV,
 71: 
 72:     /** Whether the filesystem API is available */
 73:     IS_FS_AVAILABLE,
 74: 
 75:     /** Whether the path API is available */
 76:     IS_PATH_AVAILABLE,
 77: });
 78: 
 79: const RUNNING_LOCALLY = IS_FS_AVAILABLE && IS_PATH_AVAILABLE;
 80: 
 81: let dirname__ = './';
 82: if (RUNNING_LOCALLY) {
 83:     // NOTE: We wrap `import.meta` in a call to `Object` to prevent Webpack from trying to bundle it in CommonJS.
 84:     // Although we get the warning: "Accessing import.meta directly is unsupported (only property access or destructuring is supported)",
 85:     // it is safe to ignore since the bundled value (`{}`) isn't used for CommonJS environments (we use __dirname instead).
 86:     const _import_meta_url = Object(import.meta).url;
 87: 
 88:     if (_import_meta_url) {
 89:         dirname__ = path.dirname(path.dirname(url.fileURLToPath(_import_meta_url))) // ESM
 90:     } else if (typeof __dirname !== 'undefined') {
 91:         dirname__ = path.dirname(__dirname) // CommonJS
 92:     }
 93: }
 94: 
 95: // Only used for environments with access to file system
 96: const DEFAULT_CACHE_DIR = RUNNING_LOCALLY
 97:     ? path.join(dirname__, '/.cache/')
 98:     : null;
 99: 
100: // Set local model path, based on available APIs
101: const DEFAULT_LOCAL_MODEL_PATH = '/models/';
102: const localModelPath = RUNNING_LOCALLY
103:     ? path.join(dirname__, DEFAULT_LOCAL_MODEL_PATH)
104:     : DEFAULT_LOCAL_MODEL_PATH;
105: 
106: /**
107:  * Global variable given visible to users to control execution. This provides users a simple way to configure Transformers.js.
108:  * @typedef {Object} TransformersEnvironment
109:  * @property {string} version This version of Transformers.js.
110:  * @property {{onnx: Partial<import('onnxruntime-common').Env>}} backends Expose environment variables of different backends,
111:  * allowing users to set these variables if they want to.
112:  * @property {boolean} allowRemoteModels Whether to allow loading of remote files, defaults to `true`.
113:  * If set to `false`, it will have the same effect as setting `local_files_only=true` when loading pipelines, models, tokenizers, processors, etc.
114:  * @property {string} remoteHost Host URL to load models from. Defaults to the Hugging Face Hub.
115:  * @property {string} remotePathTemplate Path template to fill in and append to `remoteHost` when loading models.
116:  * @property {boolean} allowLocalModels Whether to allow loading of local files, defaults to `false` if running in-browser, and `true` otherwise.
117:  * If set to `false`, it will skip the local file check and try to load the model from the remote host.
118:  * @property {string} localModelPath Path to load local models from. Defaults to `/models/`.
119:  * @property {boolean} useFS Whether to use the file system to load files. By default, it is `true` if available.
120:  * @property {boolean} useBrowserCache Whether to use Cache API to cache models. By default, it is `true` if available.
121:  * @property {boolean} useFSCache Whether to use the file system to cache files. By default, it is `true` if available.
122:  * @property {string} cacheDir The directory to use for caching files with the file system. By default, it is `./.cache`.
123:  * @property {boolean} useCustomCache Whether to use a custom cache system (defined by `customCache`), defaults to `false`.
124:  * @property {Object} customCache The custom cache to use. Defaults to `null`. Note: this must be an object which
125:  * implements the `match` and `put` functions of the Web Cache API. For more information, see https://developer.mozilla.org/en-US/docs/Web/API/Cache.
126:  * If you wish, you may also return a `Promise<string>` from the `match` function if you'd like to use a file path instead of `Promise<Response>`.
127:  */
128: 
129: /** @type {TransformersEnvironment} */
130: export const env = {
131:     version: VERSION,
132: 
133:     /////////////////// Backends settings ///////////////////
134:     // NOTE: These will be populated later by the backends themselves.
135:     backends: {
136:         // onnxruntime-web/onnxruntime-node
137:         onnx: {},
138:     },
139: 
140:     /////////////////// Model settings ///////////////////
141:     allowRemoteModels: true,
142:     remoteHost: 'https://huggingface.co/',
143:     remotePathTemplate: '{model}/resolve/{revision}/',
144: 
145:     allowLocalModels: !(IS_BROWSER_ENV || IS_WEBWORKER_ENV),
146:     localModelPath: localModelPath,
147:     useFS: IS_FS_AVAILABLE,
148: 
149:     /////////////////// Cache settings ///////////////////
150:     useBrowserCache: IS_WEB_CACHE_AVAILABLE && !IS_DENO_RUNTIME,
151: 
152:     useFSCache: IS_FS_AVAILABLE,
153:     cacheDir: DEFAULT_CACHE_DIR,
154: 
155:     useCustomCache: false,
156:     customCache: null,
157:     //////////////////////////////////////////////////////
158: }
159: 
160: 
161: /**
162:  * @param {Object} obj
163:  * @private
164:  */
165: function isEmpty(obj) {
166:     return Object.keys(obj).length === 0;
167: }
</file>

<file path="src/App.jsx">
  1: import React, { useState, useRef, useCallback } from 'react';
  2: import { F5TTS } from './f5-tts.js';
  3: import { processReferenceAudio } from './audio-utils.js';
  4: 
  5: const App = () => {
  6:   const [f5tts, setF5tts] = useState(null);
  7:   const [loading, setLoading] = useState(false);
  8:   const [progress, setProgress] = useState({ value: 0, message: '' });
  9:   const [refAudio, setRefAudio] = useState(null);
 10:   const [refText, setRefText] = useState('');
 11:   const [genText, setGenText] = useState('');
 12:   const [generatedAudio, setGeneratedAudio] = useState(null);
 13:   const [modelPaths, setModelPaths] = useState({
 14:     preprocess: 'models/F5_Preprocess.onnx',
 15:     transformer: 'models/F5_Transformer.onnx', 
 16:     decode: 'models/F5_Decode.onnx',
 17:     vocab: 'models/Emilia_ZH_EN_pinyin/vocab.txt'
 18:   });
 19: 
 20:   const audioRef = useRef();
 21:   const fileInputRef = useRef();
 22: 
 23:   const initializeModels = useCallback(async () => {
 24:     if (f5tts) return;
 25:     
 26:     setLoading(true);
 27:     setProgress({ value: 0, message: 'Loading models...' });
 28:     
 29:     try {
 30:       const instance = new F5TTS();
 31:       await instance.loadModels(modelPaths);
 32:       setF5tts(instance);
 33:       setProgress({ value: 100, message: 'Models loaded successfully' });
 34:     } catch (error) {
 35:       console.error('Failed to load models:', error);
 36:       setProgress({ value: 0, message: `Error: ${error.message}` });
 37:     } finally {
 38:       setLoading(false);
 39:     }
 40:   }, [f5tts, modelPaths]);
 41: 
 42:   const handleAudioUpload = useCallback(async (e) => {
 43:     const file = e.target.files[0];
 44:     if (!file || !f5tts) return;
 45: 
 46:     setLoading(true);
 47:     setProgress({ value: 0, message: 'Processing reference audio...' });
 48: 
 49:     try {
 50:       const processedAudio = await processReferenceAudio(file);
 51:       setRefAudio(processedAudio);
 52:       setProgress({ value: 100, message: 'Reference audio processed' });
 53:     } catch (error) {
 54:       console.error('Audio processing failed:', error);
 55:       setProgress({ value: 0, message: `Audio error: ${error.message}` });
 56:     } finally {
 57:       setLoading(false);
 58:     }
 59:   }, [f5tts]);
 60: 
 61:   const generateSpeech = useCallback(async () => {
 62:     if (!f5tts || !refAudio || !refText.trim() || !genText.trim()) {
 63:       alert('Please load models, upload reference audio, and provide both reference and generation text');
 64:       return;
 65:     }
 66: 
 67:     setLoading(true);
 68:     setGeneratedAudio(null);
 69: 
 70:     try {
 71:       const audioData = await f5tts.generateSpeech(
 72:         refAudio,
 73:         refText,
 74:         genText,
 75:         (progress, message) => {
 76:           setProgress({ 
 77:             value: Math.round(progress), 
 78:             message 
 79:           });
 80:         }
 81:       );
 82: 
 83:       // Create WAV blob
 84:       const wavBlob = audioBufferToWav(audioData, 24000);
 85:       setGeneratedAudio(URL.createObjectURL(wavBlob));
 86:       
 87:       setProgress({ value: 100, message: 'Generation complete' });
 88: 
 89:     } catch (error) {
 90:       console.error('Generation failed:', error);
 91:       setProgress({ value: 0, message: `Generation error: ${error.message}` });
 92:     } finally {
 93:       setLoading(false);
 94:     }
 95:   }, [f5tts, refAudio, refText, genText]);
 96: 
 97:   // WAV file creation
 98:   const audioBufferToWav = useCallback((audioData, sampleRate) => {
 99:     const length = audioData.length;
100:     const buffer = new ArrayBuffer(44 + length * 2);
101:     const view = new DataView(buffer);
102:     const samples = new Int16Array(buffer, 44);
103: 
104:     // WAV header
105:     const writeString = (offset, string) => {
106:       for (let i = 0; i < string.length; i++) {
107:         view.setUint8(offset + i, string.charCodeAt(i));
108:       }
109:     };
110: 
111:     writeString(0, 'RIFF');
112:     view.setUint32(4, 36 + length * 2, true);
113:     writeString(8, 'WAVE');
114:     writeString(12, 'fmt ');
115:     view.setUint32(16, 16, true);
116:     view.setUint16(20, 1, true);
117:     view.setUint16(22, 1, true);
118:     view.setUint32(24, sampleRate, true);
119:     view.setUint32(28, sampleRate * 2, true);
120:     view.setUint16(32, 2, true);
121:     view.setUint16(34, 16, true);
122:     writeString(36, 'data');
123:     view.setUint32(40, length * 2, true);
124: 
125:     // Convert to 16-bit PCM
126:     for (let i = 0; i < length; i++) {
127:       samples[i] = Math.max(-1, Math.min(1, audioData[i])) * 0x7FFF;
128:     }
129: 
130:     return new Blob([buffer], { type: 'audio/wav' });
131:   }, []);
132: 
133:   return (
134:     <div className="max-w-4xl mx-auto p-6 space-y-6">
135:       <div className="bg-white rounded-lg shadow-lg p-6">
136:         <h1 className="text-3xl font-bold text-gray-800 mb-6">F5-TTS Web</h1>
137:         
138:         {/* Model Configuration */}
139:         <div className="space-y-4 mb-6">
140:           <h2 className="text-xl font-semibold">Model Configuration</h2>
141:           <div className="grid grid-cols-2 gap-4">
142:             <input
143:               type="text"
144:               placeholder="Preprocess model path"
145:               value={modelPaths.preprocess}
146:               onChange={(e) => setModelPaths(prev => ({ ...prev, preprocess: e.target.value }))}
147:               className="px-3 py-2 border border-gray-300 rounded-md"
148:             />
149:             <input
150:               type="text"
151:               placeholder="Transformer model path"
152:               value={modelPaths.transformer}
153:               onChange={(e) => setModelPaths(prev => ({ ...prev, transformer: e.target.value }))}
154:               className="px-3 py-2 border border-gray-300 rounded-md"
155:             />
156:             <input
157:               type="text"
158:               placeholder="Decode model path"
159:               value={modelPaths.decode}
160:               onChange={(e) => setModelPaths(prev => ({ ...prev, decode: e.target.value }))}
161:               className="px-3 py-2 border border-gray-300 rounded-md"
162:             />
163:             <input
164:               type="text"
165:               placeholder="Vocab file path"
166:               value={modelPaths.vocab}
167:               onChange={(e) => setModelPaths(prev => ({ ...prev, vocab: e.target.value }))}
168:               className="px-3 py-2 border border-gray-300 rounded-md"
169:             />
170:           </div>
171:           <button
172:             onClick={initializeModels}
173:             disabled={loading || f5tts}
174:             className="bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400 text-white px-4 py-2 rounded-md"
175:           >
176:             {f5tts ? 'Models Loaded' : 'Load Models'}
177:           </button>
178:         </div>
179: 
180:         {/* Reference Audio */}
181:         <div className="space-y-4 mb-6">
182:           <h2 className="text-xl font-semibold">Reference Audio</h2>
183:           <input
184:             ref={fileInputRef}
185:             type="file"
186:             accept="audio/*"
187:             onChange={handleAudioUpload}
188:             disabled={!f5tts}
189:             className="block w-full text-sm text-gray-500 file:mr-4 file:py-2 file:px-4 file:rounded-md file:border-0 file:text-sm file:font-semibold file:bg-blue-50 file:text-blue-700 hover:file:bg-blue-100 disabled:opacity-50"
190:           />
191:           {refAudio && (
192:             <div className="text-sm text-gray-600">
193:               Duration: {(refAudio.length / refAudio.sampleRate).toFixed(2)}s | 
194:               Sample Rate: {refAudio.sampleRate}Hz
195:             </div>
196:           )}
197:         </div>
198: 
199:         {/* Text Inputs */}
200:         <div className="space-y-4 mb-6">
201:           <div>
202:             <label className="block text-sm font-medium text-gray-700 mb-2">
203:               Reference Text (what the reference audio says)
204:             </label>
205:             <textarea
206:               value={refText}
207:               onChange={(e) => setRefText(e.target.value)}
208:               placeholder="Enter the text that matches your reference audio..."
209:               className="w-full px-3 py-2 border border-gray-300 rounded-md h-24 resize-none"
210:             />
211:           </div>
212:           
213:           <div>
214:             <label className="block text-sm font-medium text-gray-700 mb-2">
215:               Text to Generate
216:             </label>
217:             <textarea
218:               value={genText}
219:               onChange={(e) => setGenText(e.target.value)}
220:               placeholder="Enter the text you want to generate speech for..."
221:               className="w-full px-3 py-2 border border-gray-300 rounded-md h-32 resize-none"
222:             />
223:           </div>
224:         </div>
225: 
226:         {/* Generate Button */}
227:         <button
228:           onClick={generateSpeech}
229:           disabled={loading || !f5tts || !refAudio || !refText.trim() || !genText.trim()}
230:           className="w-full bg-green-500 hover:bg-green-600 disabled:bg-gray-400 text-white py-3 px-4 rounded-md font-semibold"
231:         >
232:           {loading ? 'Generating...' : 'Generate Speech'}
233:         </button>
234: 
235:         {/* Progress */}
236:         {(loading || progress.value > 0) && (
237:           <div className="mt-4">
238:             <div className="flex justify-between text-sm text-gray-600 mb-1">
239:               <span>{progress.message}</span>
240:               <span>{progress.value}%</span>
241:             </div>
242:             <div className="w-full bg-gray-200 rounded-full h-2">
243:               <div 
244:                 className="bg-blue-500 h-2 rounded-full transition-all duration-300"
245:                 style={{ width: `${progress.value}%` }}
246:               />
247:             </div>
248:           </div>
249:         )}
250: 
251:         {/* Generated Audio */}
252:         {generatedAudio && (
253:           <div className="mt-6 p-4 bg-gray-50 rounded-md">
254:             <h3 className="text-lg font-semibold mb-3">Generated Audio</h3>
255:             <audio
256:               ref={audioRef}
257:               controls
258:               src={generatedAudio}
259:               className="w-full mb-3"
260:             />
261:             <a
262:               href={generatedAudio}
263:               download="generated_speech.wav"
264:               className="inline-block bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded-md text-sm"
265:             >
266:               Download WAV
267:             </a>
268:           </div>
269:         )}
270:       </div>
271:     </div>
272:   );
273: };
274: 
275: export default App;
</file>

<file path="src/audio-utils.js">
  1: /**
  2:  * Audio processing utilities for F5-TTS
  3:  */
  4: 
  5: export async function loadAudio(file) {
  6:   const buffer = await file.arrayBuffer();
  7:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
  8:   const audio = await ctx.decodeAudioData(buffer);
  9:   ctx.close();
 10:   return audio;
 11: }
 12: 
 13: export function toMono(audioBuffer) {
 14:   if (audioBuffer.numberOfChannels === 1) return audioBuffer;
 15:   
 16:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
 17:   const mono = ctx.createBuffer(1, audioBuffer.length, audioBuffer.sampleRate);
 18:   const output = mono.getChannelData(0);
 19:   
 20:   for (let i = 0; i < audioBuffer.length; i++) {
 21:     let sum = 0;
 22:     for (let ch = 0; ch < audioBuffer.numberOfChannels; ch++) {
 23:       sum += audioBuffer.getChannelData(ch)[i];
 24:     }
 25:     output[i] = sum / audioBuffer.numberOfChannels;
 26:   }
 27:   
 28:   ctx.close();
 29:   return mono;
 30: }
 31: 
 32: export function resample(audioBuffer, targetRate = 24000) {
 33:   if (audioBuffer.sampleRate === targetRate) return audioBuffer;
 34:   
 35:   const ratio = audioBuffer.sampleRate / targetRate;
 36:   const newLength = Math.round(audioBuffer.length / ratio);
 37:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
 38:   const resampled = ctx.createBuffer(1, newLength, targetRate);
 39:   
 40:   const input = audioBuffer.getChannelData(0);
 41:   const output = resampled.getChannelData(0);
 42:   
 43:   for (let i = 0; i < newLength; i++) {
 44:     const pos = i * ratio;
 45:     const idx = Math.floor(pos);
 46:     const frac = pos - idx;
 47:     
 48:     if (idx + 1 < input.length) {
 49:       output[i] = input[idx] * (1 - frac) + input[idx + 1] * frac;
 50:     } else {
 51:       output[i] = input[idx] || 0;
 52:     }
 53:   }
 54:   
 55:   ctx.close();
 56:   return resampled;
 57: }
 58: 
 59: export function calculateRMS(audioBuffer) {
 60:   const samples = audioBuffer.getChannelData(0);
 61:   let sum = 0;
 62:   for (let i = 0; i < samples.length; i++) {
 63:     sum += samples[i] * samples[i];
 64:   }
 65:   return Math.sqrt(sum / samples.length);
 66: }
 67: 
 68: export function normalizeRMS(audioBuffer, targetRMS = 0.1) {
 69:   const currentRMS = calculateRMS(audioBuffer);
 70:   if (currentRMS >= targetRMS) return audioBuffer;
 71:   
 72:   const scale = targetRMS / currentRMS;
 73:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
 74:   const normalized = ctx.createBuffer(1, audioBuffer.length, audioBuffer.sampleRate);
 75:   
 76:   const input = audioBuffer.getChannelData(0);
 77:   const output = normalized.getChannelData(0);
 78:   
 79:   for (let i = 0; i < input.length; i++) {
 80:     output[i] = input[i] * scale;
 81:   }
 82:   
 83:   ctx.close();
 84:   return normalized;
 85: }
 86: 
 87: export function removeSilence(audioBuffer, threshDb = -50, minSilenceMs = 1000, keepMs = 500) {
 88:   const samples = audioBuffer.getChannelData(0);
 89:   const sampleRate = audioBuffer.sampleRate;
 90:   const windowSize = Math.floor(sampleRate * 0.01);
 91:   const linearThresh = Math.pow(10, threshDb / 20);
 92:   const minSilenceSamples = (minSilenceMs / 1000) * sampleRate;
 93:   
 94:   const silenceSegments = [];
 95:   let silenceStart = null;
 96:   
 97:   for (let i = 0; i < samples.length; i += windowSize) {
 98:     let sum = 0;
 99:     const end = Math.min(i + windowSize, samples.length);
100:     for (let j = i; j < end; j++) {
101:       sum += samples[j] * samples[j];
102:     }
103:     const rms = Math.sqrt(sum / (end - i));
104:     
105:     if (rms < linearThresh) {
106:       if (silenceStart === null) silenceStart = i;
107:     } else {
108:       if (silenceStart !== null) {
109:         const duration = i - silenceStart;
110:         if (duration >= minSilenceSamples) {
111:           silenceSegments.push([silenceStart, i]);
112:         }
113:         silenceStart = null;
114:       }
115:     }
116:   }
117:   
118:   if (silenceSegments.length === 0) return audioBuffer;
119:   
120:   const keepSamples = Math.floor((keepMs / 1000) * sampleRate);
121:   let newLength = audioBuffer.length;
122:   for (const [start, end] of silenceSegments) {
123:     newLength -= Math.max(0, end - start - keepSamples);
124:   }
125:   
126:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
127:   const processed = ctx.createBuffer(1, newLength, sampleRate);
128:   const input = audioBuffer.getChannelData(0);
129:   const output = processed.getChannelData(0);
130:   
131:   let outputIdx = 0;
132:   let inputIdx = 0;
133:   
134:   for (const [silenceStart, silenceEnd] of silenceSegments) {
135:     while (inputIdx < silenceStart) {
136:       output[outputIdx++] = input[inputIdx++];
137:     }
138:     
139:     const silenceLength = silenceEnd - silenceStart;
140:     const keepLength = Math.min(keepSamples, silenceLength);
141:     for (let i = 0; i < keepLength; i++) {
142:       output[outputIdx++] = input[inputIdx + i];
143:     }
144:     
145:     inputIdx = silenceEnd;
146:   }
147:   
148:   while (inputIdx < input.length) {
149:     output[outputIdx++] = input[inputIdx++];
150:   }
151:   
152:   ctx.close();
153:   return processed;
154: }
155: 
156: export function limitDuration(audioBuffer, maxSeconds = 15) {
157:   const maxSamples = Math.floor(maxSeconds * audioBuffer.sampleRate);
158:   if (audioBuffer.length <= maxSamples) return audioBuffer;
159:   
160:   const ctx = new (window.AudioContext || window.webkitAudioContext)();
161:   const limited = ctx.createBuffer(1, maxSamples, audioBuffer.sampleRate);
162:   const input = audioBuffer.getChannelData(0);
163:   const output = limited.getChannelData(0);
164:   
165:   for (let i = 0; i < maxSamples; i++) {
166:     output[i] = input[i];
167:   }
168:   
169:   ctx.close();
170:   return limited;
171: }
172: 
173: export async function processReferenceAudio(file) {
174:   let audio = await loadAudio(file);
175:   audio = toMono(audio);
176:   audio = limitDuration(audio, 15);
177:   audio = removeSilence(audio);
178:   audio = normalizeRMS(audio);
179:   audio = resample(audio);
180:   return audio;
181: }
</file>

<file path="src/f5-tts.js">
  1: /**
  2:  * F5-TTS implementation matching Python ONNX version exactly
  3:  */
  4: 
  5: import * as ort from 'onnxruntime-web';
  6: 
  7: export class F5TTS {
  8:   constructor() {
  9:     this.sessionA = null;
 10:     this.sessionB = null;
 11:     this.sessionC = null;
 12:     this.vocabMap = null;
 13:     this.nfeStep = 32;
 14:     this.hopLength = 256;
 15:     this.targetSampleRate = 24000;
 16:     this.targetRMS = 0.1;
 17:   }
 18: 
 19:   saveDebugData(data, name, step = null) {
 20:     let dataDict;
 21: 
 22:     try {
 23:       if (data instanceof Float32Array || data instanceof Int32Array || data instanceof Int16Array) {
 24:         dataDict = {
 25:           shape: [data.length],
 26:           dtype: data.constructor.name,
 27:           data: Array.from(data.slice(0, 100))
 28:         };
 29:       } else if (data instanceof BigInt64Array) {
 30:         dataDict = {
 31:           shape: [data.length],
 32:           dtype: 'BigInt64Array',
 33:           data: Array.from(data.slice(0, 100)).map(x => Number(x))  // Convert BigInt to Number
 34:         };
 35:       } else if (data && data.data) {  // ONNX tensor
 36:         let tensorData;
 37:         if (data.data instanceof BigInt64Array) {
 38:           tensorData = Array.from(data.data.slice(0, 100)).map(x => Number(x));
 39:         } else {
 40:           tensorData = Array.from(data.data.slice(0, 100));
 41:         }
 42: 
 43:         dataDict = {
 44:           shape: data.dims || [],
 45:           dtype: data.type || 'unknown',
 46:           data: tensorData
 47:         };
 48:       } else if (typeof data === 'bigint') {
 49:         dataDict = { value: Number(data) };  // Convert single BigInt
 50:       } else {
 51:         dataDict = { value: data };
 52:       }
 53: 
 54:       const filename = `debug_${name}${step !== null ? '_' + step : ''}.json`;
 55:       console.log(filename, dataDict);
 56:       localStorage.setItem(filename, JSON.stringify(dataDict));
 57: 
 58:     } catch (error) {
 59:       console.warn(`Failed to save debug data for ${name}:`, error.message);
 60:       // Try minimal fallback
 61:       try {
 62:         const fallback = {
 63:           name: name,
 64:           type: typeof data,
 65:           constructor: data?.constructor?.name || 'unknown',
 66:           error: error.message
 67:         };
 68:         localStorage.setItem(`debug_${name}_error.json`, JSON.stringify(fallback));
 69:       } catch (e) {
 70:         console.error(`Complete failure saving debug data for ${name}`);
 71:       }
 72:     }
 73:   }
 74: 
 75:   async loadModels(modelPaths) {
 76:     // Configure ONNX Runtime with WebGPU support
 77:     const providers = ['webgpu', 'cpu'];
 78:     const sessionOptions = {
 79:       executionProviders: providers,
 80:       graphOptimizationLevel: 'all',
 81:       enableMemPattern: true,
 82:       enableCpuMemArena: true,
 83:       extra: {
 84:         session: {
 85:           intra_op_num_threads: 8,
 86:           inter_op_num_threads: 8,
 87:           allow_profiling: false
 88:         }
 89:       }
 90:     };
 91: 
 92:     // CPU-only options for preprocess and decode
 93:     const cpuOptions = {
 94:       executionProviders: ['cpu'],
 95:       graphOptimizationLevel: 'all',
 96:       enableMemPattern: true,
 97:       enableCpuMemArena: true
 98:     };
 99: 
100:     try {
101:       // Load models
102:       this.sessionA = await ort.InferenceSession.create(modelPaths.preprocess, cpuOptions);
103:       this.sessionB = await ort.InferenceSession.create(modelPaths.transformer, sessionOptions);
104:       this.sessionC = await ort.InferenceSession.create(modelPaths.decode, cpuOptions);
105: 
106:       // Load vocabulary
107:       const vocabResponse = await fetch(modelPaths.vocab);
108:       const vocabText = await vocabResponse.text();
109:       this.vocabMap = {};
110: 
111:       vocabText.split('\n').forEach((char, idx) => {
112:         if (char.trim()) {
113:           this.vocabMap[char.trim()] = idx;
114:         }
115:       });
116: 
117:       console.log('Models loaded successfully');
118:       console.log('Transformer providers:', this.sessionB.inputNames);
119:     } catch (error) {
120:       throw new Error(`Failed to load models: ${error.message}`);
121:     }
122:   }
123: 
124:   // Audio processing functions
125:   async loadAudio(file) {
126:     const buffer = await file.arrayBuffer();
127:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
128:     const audio = await ctx.decodeAudioData(buffer);
129:     await ctx.close();
130:     return audio;
131:   }
132: 
133:   toMono(audioBuffer) {
134:     if (audioBuffer.numberOfChannels === 1) return audioBuffer;
135: 
136:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
137:     const mono = ctx.createBuffer(1, audioBuffer.length, audioBuffer.sampleRate);
138:     const output = mono.getChannelData(0);
139: 
140:     for (let i = 0; i < audioBuffer.length; i++) {
141:       let sum = 0;
142:       for (let ch = 0; ch < audioBuffer.numberOfChannels; ch++) {
143:         sum += audioBuffer.getChannelData(ch)[i];
144:       }
145:       output[i] = sum / audioBuffer.numberOfChannels;
146:     }
147: 
148:     ctx.close();
149:     return mono;
150:   }
151: 
152:   resample(audioBuffer, targetRate = this.targetSampleRate) {
153:     if (audioBuffer.sampleRate === targetRate) return audioBuffer;
154: 
155:     const ratio = audioBuffer.sampleRate / targetRate;
156:     const newLength = Math.round(audioBuffer.length / ratio);
157:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
158:     const resampled = ctx.createBuffer(1, newLength, targetRate);
159: 
160:     const input = audioBuffer.getChannelData(0);
161:     const output = resampled.getChannelData(0);
162: 
163:     for (let i = 0; i < newLength; i++) {
164:       const pos = i * ratio;
165:       const idx = Math.floor(pos);
166:       const frac = pos - idx;
167: 
168:       if (idx + 1 < input.length) {
169:         output[i] = input[idx] * (1 - frac) + input[idx + 1] * frac;
170:       } else {
171:         output[i] = input[idx] || 0;
172:       }
173:     }
174: 
175:     ctx.close();
176:     return resampled;
177:   }
178: 
179:   calculateRMS(audioBuffer) {
180:     const samples = audioBuffer.getChannelData(0);
181:     let sum = 0;
182:     for (let i = 0; i < samples.length; i++) {
183:       sum += samples[i] * samples[i];
184:     }
185:     return Math.sqrt(sum / samples.length);
186:   }
187: 
188:   normalizeRMS(audioBuffer, targetRMS = this.targetRMS) {
189:     const currentRMS = this.calculateRMS(audioBuffer);
190:     if (currentRMS >= targetRMS) return audioBuffer;
191: 
192:     const scale = targetRMS / currentRMS;
193:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
194:     const normalized = ctx.createBuffer(1, audioBuffer.length, audioBuffer.sampleRate);
195: 
196:     const input = audioBuffer.getChannelData(0);
197:     const output = normalized.getChannelData(0);
198: 
199:     for (let i = 0; i < input.length; i++) {
200:       output[i] = input[i] * scale;
201:     }
202: 
203:     ctx.close();
204:     return normalized;
205:   }
206: 
207:   removeSilence(audioBuffer, threshDb = -50, minSilenceMs = 1000, keepMs = 500) {
208:     const samples = audioBuffer.getChannelData(0);
209:     const sampleRate = audioBuffer.sampleRate;
210:     const windowSize = Math.floor(sampleRate * 0.01);
211:     const linearThresh = Math.pow(10, threshDb / 20);
212:     const minSilenceSamples = (minSilenceMs / 1000) * sampleRate;
213: 
214:     const silenceSegments = [];
215:     let silenceStart = null;
216: 
217:     for (let i = 0; i < samples.length; i += windowSize) {
218:       let sum = 0;
219:       const end = Math.min(i + windowSize, samples.length);
220:       for (let j = i; j < end; j++) {
221:         sum += samples[j] * samples[j];
222:       }
223:       const rms = Math.sqrt(sum / (end - i));
224: 
225:       if (rms < linearThresh) {
226:         if (silenceStart === null) silenceStart = i;
227:       } else {
228:         if (silenceStart !== null) {
229:           const duration = i - silenceStart;
230:           if (duration >= minSilenceSamples) {
231:             silenceSegments.push([silenceStart, i]);
232:           }
233:           silenceStart = null;
234:         }
235:       }
236:     }
237: 
238:     if (silenceSegments.length === 0) return audioBuffer;
239: 
240:     const keepSamples = Math.floor((keepMs / 1000) * sampleRate);
241:     let newLength = audioBuffer.length;
242:     for (const [start, end] of silenceSegments) {
243:       newLength -= Math.max(0, end - start - keepSamples);
244:     }
245: 
246:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
247:     const processed = ctx.createBuffer(1, newLength, sampleRate);
248:     const input = audioBuffer.getChannelData(0);
249:     const output = processed.getChannelData(0);
250: 
251:     let outputIdx = 0;
252:     let inputIdx = 0;
253: 
254:     for (const [silenceStart, silenceEnd] of silenceSegments) {
255:       while (inputIdx < silenceStart) {
256:         output[outputIdx++] = input[inputIdx++];
257:       }
258: 
259:       const silenceLength = silenceEnd - silenceStart;
260:       const keepLength = Math.min(keepSamples, silenceLength);
261:       for (let i = 0; i < keepLength; i++) {
262:         output[outputIdx++] = input[inputIdx + i];
263:       }
264: 
265:       inputIdx = silenceEnd;
266:     }
267: 
268:     while (inputIdx < input.length) {
269:       output[outputIdx++] = input[inputIdx++];
270:     }
271: 
272:     ctx.close();
273:     return processed;
274:   }
275: 
276:   limitDuration(audioBuffer, maxSeconds = 15) {
277:     const maxSamples = Math.floor(maxSeconds * audioBuffer.sampleRate);
278:     if (audioBuffer.length <= maxSamples) return audioBuffer;
279: 
280:     const ctx = new (window.AudioContext || window.webkitAudioContext)();
281:     const limited = ctx.createBuffer(1, maxSamples, audioBuffer.sampleRate);
282:     const input = audioBuffer.getChannelData(0);
283:     const output = limited.getChannelData(0);
284: 
285:     for (let i = 0; i < maxSamples; i++) {
286:       output[i] = input[i];
287:     }
288: 
289:     ctx.close();
290:     return limited;
291:   }
292: 
293:   async processReferenceAudio(file) {
294:     let audio = await this.loadAudio(file);
295:     audio = this.toMono(audio);
296:     audio = this.limitDuration(audio, 15);
297:     audio = this.removeSilence(audio);
298:     audio = this.normalizeRMS(audio);
299:     audio = this.resample(audio);
300:     return audio;
301:   }
302: 
303:   // Text processing (simplified English-only)
304:   tokenizeText(text) {
305:     const chars = text.toLowerCase().split('');
306:     const tokens = chars.map(char => this.vocabMap[char] || 0);
307:     return new Int32Array(tokens);
308:   }
309: 
310:   splitTextIntoBatches(text, maxChars = 200) {
311:     if (new Blob([text]).size <= maxChars) return [text];
312: 
313:     if (!text.match(/[.!?]$/)) text += '.';
314: 
315:     const sentences = text.split(/([.!?])/).reduce((acc, curr, i) => {
316:       if (i % 2 === 0) acc.push(curr);
317:       else acc[acc.length - 1] += curr;
318:       return acc;
319:     }, []).filter(s => s.trim());
320: 
321:     const batches = [];
322:     let current = "";
323: 
324:     for (const sentence of sentences) {
325:       if (new Blob([current + sentence]).size <= maxChars) {
326:         current += sentence;
327:       } else {
328:         if (current) batches.push(current);
329:         current = sentence;
330:       }
331:     }
332: 
333:     if (current) batches.push(current);
334:     return batches;
335:   }
336: 
337:   // calculateDuration(refText, genText, refAudioLen, speed = 1.0) {
338:   //   const zhPausePunc = /[。，、；：？！]/g;
339:     
340:   //   const refTextLen = new TextEncoder().encode(refText).length + 
341:   //                     3 * (refText.match(zhPausePunc) || []).length;
342:   //   const genTextLen = new TextEncoder().encode(genText).length + 
343:   //                     3 * (genText.match(zhPausePunc) || []).length;
344:     
345:   //   return refAudioLen + Math.floor(refAudioLen / refTextLen * genTextLen / speed);
346:   // }
347: 
348:   normalizeToInt16(audio) {
349:     const maxVal = Math.max(...audio.map(Math.abs));
350:     const scale = maxVal > 0 ? 32767 / maxVal : 1;
351:     return new Int16Array(audio.map(x => Math.round(x * scale)));
352:   }
353: 
354:   async inference(refAudio, refText, genText, onProgress, speed = 1.0) {
355:     if (!this.sessionA || !this.sessionB || !this.sessionC) {
356:       throw new Error('Models not loaded');
357:     }
358: 
359:     // Prepare audio - exact Python implementation
360:     const audioSamples = Array.from(refAudio.getChannelData(0));
361:     const audioInt16 = this.normalizeToInt16(audioSamples);
362:     const audioTensor = new ort.Tensor('int16', new Int16Array(audioInt16), [1, 1, audioInt16.length]);
363: 
364:     // Prepare text
365:     const combinedText = refText + " " + genText;
366:     const textTokens = this.tokenizeText(combinedText);
367:     const textTensor = new ort.Tensor('int32', textTokens, [1, textTokens.length]);
368: 
369:     // Calculate duration - matching Python
370:     const refAudioLen = Math.floor(audioSamples.length / this.hopLength);
371:     const refTextLen = refText.length;
372:     const genTextLen = genText.length;
373:     const duration = refAudioLen + Math.trunc(refAudioLen / refTextLen * genTextLen / speed);
374:     const durationTensor = new ort.Tensor('int64', new BigInt64Array([BigInt(duration)]), [1]);
375: 
376:     this.saveDebugData(textTensor, "text_tensor");
377: 
378:     // Stage A: Preprocess - exact input names from Python
379:     const preprocessInputs = {
380:       [this.sessionA.inputNames[0]]: audioTensor,
381:       [this.sessionA.inputNames[1]]: textTensor,
382:       [this.sessionA.inputNames[2]]: durationTensor
383:     };
384: 
385:     const preprocessOutputs = await this.sessionA.run(preprocessInputs);
386: 
387:     let noise = preprocessOutputs[this.sessionA.outputNames[0]];
388:     const ropeCosQ = preprocessOutputs[this.sessionA.outputNames[1]];
389:     const ropeSinQ = preprocessOutputs[this.sessionA.outputNames[2]];
390:     const ropeCosK = preprocessOutputs[this.sessionA.outputNames[3]];
391:     const ropeSinK = preprocessOutputs[this.sessionA.outputNames[4]];
392:     const catMelText = preprocessOutputs[this.sessionA.outputNames[5]];
393:     const catMelTextDrop = preprocessOutputs[this.sessionA.outputNames[6]];
394:     const refSignalLen = preprocessOutputs[this.sessionA.outputNames[7]];
395:     this.saveDebugData(noise, "stage_a_noise");
396:     this.saveDebugData(refSignalLen, "stage_a_ref_signal_len");
397: 
398:     // Stage B: Transformer NFE steps - exact Python loop
399:     let timeStep = new ort.Tensor('int32', new Int32Array([0]), [1]);
400: 
401:     for (let step = 0; step < this.nfeStep - 1; step++) {
402:       const transformerInputs = {
403:         [this.sessionB.inputNames[0]]: noise,
404:         [this.sessionB.inputNames[1]]: ropeCosQ,
405:         [this.sessionB.inputNames[2]]: ropeSinQ,
406:         [this.sessionB.inputNames[3]]: ropeCosK,
407:         [this.sessionB.inputNames[4]]: ropeSinK,
408:         [this.sessionB.inputNames[5]]: catMelText,
409:         [this.sessionB.inputNames[6]]: catMelTextDrop,
410:         [this.sessionB.inputNames[7]]: timeStep
411:       };
412: 
413:       const transformerOutputs = await this.sessionB.run(transformerInputs);
414:       noise = transformerOutputs[this.sessionB.outputNames[0]];
415:       timeStep = transformerOutputs[this.sessionB.outputNames[1]];
416: 
417:       this.saveDebugData(noise, "stage_b_noise", step);
418:       this.saveDebugData(timeStep, "stage_b_timestep", step);
419: 
420:       if (onProgress) {
421:         onProgress(((step + 1) / this.nfeStep) * 100, `NFE Step ${step + 1}/${this.nfeStep}`);
422:       }
423:     }
424: 
425:     // Stage C: Decode
426:     const decodeInputs = {
427:       [this.sessionC.inputNames[0]]: noise,
428:       [this.sessionC.inputNames[1]]: refSignalLen
429:     };
430: 
431:     const decodeOutputs = await this.sessionC.run(decodeInputs);
432:     const generatedSignal = decodeOutputs[this.sessionC.outputNames[0]];
433: 
434:     this.saveDebugData(generatedSignal, "stage_c_output");
435: 
436:     // Convert int16 to float32 - exact Python conversion
437:     const int16Data = generatedSignal.data;
438:     const float32Data = new Float32Array(int16Data.length);
439:     for (let i = 0; i < int16Data.length; i++) {
440:       float32Data[i] = int16Data[i] / 32767.0;
441:     }
442: 
443:     const generatedFloat32 = new Float32Array(int16Data.length);
444:     for (let i = 0; i < int16Data.length; i++) {
445:       generatedFloat32[i] = int16Data[i] / 32767.0;
446:     }
447: 
448:     // Apply RMS adjustment if reference was quiet
449:     const refRMS = this.calculateRMS(refAudio);
450:     if (refRMS < this.targetRMS) {
451:       const currentRMS = Math.sqrt(generatedFloat32.reduce((sum, x) => sum + x * x, 0) / generatedFloat32.length);
452:       if (currentRMS > 0) {
453:         const scale = refRMS / this.targetRMS;
454:         for (let i = 0; i < generatedFloat32.length; i++) {
455:           generatedFloat32[i] *= scale;
456:         }
457:       }
458:     }
459: 
460:     this.saveDebugData(audioInt16, "audio_processed");
461:     this.saveDebugData(audioSamples.length, "audio_len");
462:     // this.saveDebugData(textIds, "text_ids");
463:     this.saveDebugData(duration, "duration");
464: 
465:     return generatedFloat32;
466:   }
467: 
468:   async generateSpeech(refAudio, refText, genText, onProgress, speed = 1.0) {
469:     // Calculate max chars based on reference audio
470:     const refDuration = refAudio.length / refAudio.sampleRate;
471:     const maxChars = Math.trunc(new TextEncoder().encode(refText).length / (refAudio.length / refAudio.sampleRate) * (30 - refAudio.length / refAudio.sampleRate));
472: 
473:     const textBatches = this.splitTextIntoBatches(genText, Math.max(maxChars, 100));
474: 
475:     if (textBatches.length === 1) {
476:       return await this.inference(refAudio, refText, genText, onProgress);
477:     }
478: 
479:     // Batch processing
480:     const results = [];
481:     const total = textBatches.length * this.nfeStep;
482:     let current = 0;
483: 
484:     for (let i = 0; i < textBatches.length; i++) {
485:       const result = await this.inference(
486:         refAudio,
487:         refText,
488:         textBatches[i],
489:         (progress, message) => {
490:           if (onProgress) {
491:             const overall = (current + progress) / total * 100;
492:             onProgress(overall, `Batch ${i + 1}/${textBatches.length}: ${message}`);
493:           }
494:         }
495:       );
496: 
497:       results.push(result);
498:       current += this.nfeStep;
499:     }
500: 
501:     // Concatenate results
502:     const totalLength = results.reduce((sum, arr) => sum + arr.length, 0);
503:     const combined = new Float32Array(totalLength);
504:     let offset = 0;
505: 
506:     for (const result of results) {
507:       combined.set(result, offset);
508:       offset += result.length;
509:     }
510: 
511:     return combined;
512:   }
513: }
</file>

<file path="src/index.css">
 1: @tailwind base;
 2: @tailwind components;
 3: @tailwind utilities;
 4: 
 5: :root {
 6:   font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
 7:   line-height: 1.5;
 8:   font-weight: 400;
 9: }
10: 
11: body {
12:   margin: 0;
13:   display: flex;
14:   place-items: center;
15:   min-width: 320px;
16:   min-height: 100vh;
17:   background-color: #f3f4f6;
18: }
19: 
20: #root {
21:   width: 100%;
22:   margin: 0 auto;
23:   text-align: left;
24: }
</file>

<file path="src/main.jsx">
 1: import React from 'react'
 2: import ReactDOM from 'react-dom/client'
 3: import App from './App.jsx'
 4: import './index.css'
 5: 
 6: ReactDOM.createRoot(document.getElementById('root')).render(
 7:   <React.StrictMode>
 8:     <App />
 9:   </React.StrictMode>,
10: )
</file>

<file path="index.html">
 1: <!DOCTYPE html>
 2: <html lang="en">
 3: <head>
 4:     <meta charset="UTF-8">
 5:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
 6:     <title>F5-TTS Web</title>
 7:     <script src="https://cdn.tailwindcss.com"></script>
 8: </head>
 9: <body>
10:     <div id="root"></div>
11:     <script type="module" src="/src/main.jsx"></script>
12: </body>
13: </html>
</file>

<file path="package.json">
 1: {
 2:   "name": "f5-tts-web",
 3:   "version": "1.0.0",
 4:   "type": "module",
 5:   "scripts": {
 6:     "dev": "vite",
 7:     "build": "vite build",
 8:     "preview": "vite preview"
 9:   },
10:   "dependencies": {
11:     "react": "^18.2.0",
12:     "react-dom": "^18.2.0",
13:     "onnxruntime-web": "^1.16.3"
14:   },
15:   "devDependencies": {
16:     "@vitejs/plugin-react": "^4.2.1",
17:     "vite": "^5.0.8"
18:   }
19: }
</file>

<file path="README.md">
  1: # F5-TTS Web
  2: 
  3: A web-based implementation of F5-TTS using ONNX Runtime with WebGPU acceleration for fast, browser-native text-to-speech synthesis.
  4: 
  5: ## Features
  6: 
  7: - **ONNX-optimized F5-TTS inference** with WebGPU acceleration
  8: - **Modular React architecture** with proper component separation
  9: - **Real-time audio processing** with silence detection and RMS normalization
 10: - **Batch text processing** for long-form generation
 11: - **WebGPU/CPU fallback** for maximum compatibility
 12: - **Zero backend required** - runs entirely in the browser
 13: 
 14: ## Project Structure
 15: 
 16: ```
 17: f5-tts-web/
 18: ├── package.json
 19: ├── vite.config.js
 20: ├── index.html
 21: ├── src/
 22: │   ├── main.jsx           # Entry point
 23: │   ├── App.jsx            # Main React component
 24: │   ├── index.css          # Tailwind CSS
 25: │   ├── f5-tts.js          # ONNX inference engine
 26: │   └── audio-utils.js     # Audio processing utilities
 27: └── public/
 28:     └── models/            # ONNX model files (you provide)
 29:         ├── F5_Preprocess.onnx
 30:         ├── F5_Transformer.onnx
 31:         ├── F5_Decode.onnx
 32:         └── vocab.txt
 33: ```
 34: 
 35: ## Setup
 36: 
 37: ### 1. Install Dependencies
 38: 
 39: ```bash
 40: npm install
 41: ```
 42: 
 43: ### 2. Add ONNX Models
 44: 
 45: You need to convert the F5-TTS PyTorch models to ONNX format and place them in `public/models/`:
 46: 
 47: - `F5_Preprocess.onnx` - Audio preprocessing model (CPU-optimized)
 48: - `F5_Transformer.onnx` - Main transformer model (WebGPU-accelerated)
 49: - `F5_Decode.onnx` - Audio generation model with integrated Vocos (CPU-optimized)
 50: - `vocab.txt` - Character vocabulary file
 51: 
 52: ### 3. Run Development Server
 53: 
 54: ```bash
 55: npm run dev
 56: ```
 57: 
 58: Open http://localhost:5173 in your browser.
 59: 
 60: ### 4. Build for Production
 61: 
 62: ```bash
 63: npm run build
 64: npm run preview
 65: ```
 66: 
 67: ## Usage
 68: 
 69: 1. **Load Models**: Click "Load Models" to initialize the ONNX runtime
 70: 2. **Upload Reference Audio**: Provide a clean audio sample (< 15 seconds)
 71: 3. **Enter Reference Text**: Type what the reference audio says
 72: 4. **Enter Generation Text**: Type what you want to synthesize
 73: 5. **Generate**: Click "Generate Speech" and wait for processing
 74: 
 75: ## Technical Details
 76: 
 77: ### Audio Processing Pipeline
 78: 
 79: 1. **Load & Convert**: Decode uploaded audio to AudioBuffer
 80: 2. **Mono Conversion**: Average multi-channel audio to mono
 81: 3. **Duration Limiting**: Clip to 15 seconds maximum
 82: 4. **Silence Removal**: Remove quiet segments while preserving speech
 83: 5. **RMS Normalization**: Adjust volume to target level (0.1)
 84: 6. **Resampling**: Convert to 24kHz using linear interpolation
 85: 
 86: ### ONNX Inference Pipeline
 87: 
 88: 1. **Stage A (Preprocess)**: Convert audio/text to model inputs
 89: 2. **Stage B (Transformer)**: Run NFE diffusion steps (32 iterations)
 90: 3. **Stage C (Decode)**: Generate final audio with integrated Vocos
 91: 
 92: ### Performance Optimizations
 93: 
 94: - **WebGPU acceleration** for transformer model
 95: - **CPU processing** for lighter preprocessing/decode models
 96: - **Batch processing** for long text inputs
 97: - **Memory-efficient** tensor operations
 98: - **Progressive loading** with real-time progress feedback
 99: 
100: ## Browser Compatibility
101: 
102: ### Supported Browsers
103: - **Chrome/Edge 113+** (WebGPU supported)
104: - **Firefox 110+** (CPU fallback)
105: - **Safari 16+** (CPU fallback)
106: 
107: ### Requirements
108: - Modern browser with ES6 modules support
109: - AudioContext API support
110: - ONNX Runtime Web compatibility
111: 
112: ## Performance Expectations
113: 
114: ### With WebGPU (Recommended)
115: - **Model Loading**: 5-10 seconds
116: - **Audio Processing**: 1-2 seconds
117: - **Generation**: 3-5 seconds per batch
118: 
119: ### CPU Fallback
120: - **Model Loading**: 10-15 seconds  
121: - **Audio Processing**: 2-3 seconds
122: - **Generation**: 15-30 seconds per batch
123: 
124: ## Troubleshooting
125: 
126: ### Model Loading Issues
127: - Ensure ONNX files are in `public/models/` directory
128: - Check browser console for detailed error messages
129: - Verify model files are not corrupted
130: - Try refreshing the page
131: 
132: ### WebGPU Issues
133: - Check if WebGPU is enabled in browser flags
134: - Update to latest browser version
135: - App will automatically fall back to CPU
136: 
137: ### Audio Quality Issues
138: - Use high-quality reference audio (clear speech, no background noise)
139: - Keep reference audio under 15 seconds
140: - Ensure reference text exactly matches the audio
141: - Try different reference audio if quality is poor
142: 
143: ### Memory Issues
144: - Close other browser tabs
145: - Use shorter text inputs
146: - Refresh page if app becomes unresponsive
147: 
148: ## Development
149: 
150: ### Adding New Features
151: 
152: The modular architecture makes it easy to extend:
153: 
154: - **Audio processing**: Modify `src/audio-utils.js`
155: - **Model inference**: Update `src/f5-tts.js`
156: - **UI components**: Edit `src/App.jsx`
157: 
158: ### Testing Audio Processing
159: 
160: Each utility function can be tested independently:
161: 
162: ```javascript
163: import { calculateRMS, resample } from './src/audio-utils.js';
164: 
165: // Test RMS calculation
166: const rms = calculateRMS(audioBuffer);
167: console.log('RMS:', rms);
168: 
169: // Test resampling
170: const resampled = resample(audioBuffer, 24000);
171: console.log('Resampled rate:', resampled.sampleRate);
172: ```
173: 
174: ## License
175: 
176: This project is based on the original F5-TTS implementation. Refer to the original repository for licensing details.
177: 
178: ## Credits
179: 
180: - [F5-TTS](https://github.com/SWivid/F5-TTS) - Original implementation
181: - [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript.html) - Browser inference
182: - [Vocos](https://github.com/charactr-platform/vocos) - Neural vocoder
</file>

<file path="vite.config.js">
 1: import { defineConfig } from 'vite'
 2: import react from '@vitejs/plugin-react'
 3: 
 4: export default defineConfig({
 5:   plugins: [react()],
 6:   server: {
 7:     headers: {
 8:       'Cross-Origin-Embedder-Policy': 'require-corp',
 9:       'Cross-Origin-Opener-Policy': 'same-origin'
10:     }
11:   },
12:   optimizeDeps: {
13:     exclude: ['onnxruntime-web']
14:   },
15:   build: {
16:     rollupOptions: {
17:       external: [],
18:     }
19:   }
20: })
</file>

</files>
